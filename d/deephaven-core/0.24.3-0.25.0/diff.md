# Comparing `tmp/deephaven_core-0.24.3-py3-none-any.whl.zip` & `tmp/deephaven_core-0.25.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,75 +1,76 @@
-Zip file size: 130354 bytes, number of entries: 73
--rw-r--r--  2.0 unx     1120 b- defN 23-May-26 22:37 deephaven/__init__.py
--rw-r--r--  2.0 unx      970 b- defN 23-May-26 22:37 deephaven/_gc.py
--rw-r--r--  2.0 unx      676 b- defN 23-May-26 22:37 deephaven/_jpy.py
--rw-r--r--  2.0 unx     4551 b- defN 23-May-26 22:37 deephaven/_wrapper.py
--rw-r--r--  2.0 unx    10813 b- defN 23-May-26 22:37 deephaven/agg.py
--rw-r--r--  2.0 unx     1987 b- defN 23-May-26 22:37 deephaven/appmode.py
--rw-r--r--  2.0 unx     4776 b- defN 23-May-26 22:37 deephaven/arrow.py
--rw-r--r--  2.0 unx    14670 b- defN 23-May-26 22:37 deephaven/calendar.py
--rw-r--r--  2.0 unx     7276 b- defN 23-May-26 22:37 deephaven/column.py
--rw-r--r--  2.0 unx     3123 b- defN 23-May-26 22:37 deephaven/constants.py
--rw-r--r--  2.0 unx     4515 b- defN 23-May-26 22:37 deephaven/csv.py
--rw-r--r--  2.0 unx     2890 b- defN 23-May-26 22:37 deephaven/dherror.py
--rw-r--r--  2.0 unx     9105 b- defN 23-May-26 22:37 deephaven/dtypes.py
--rw-r--r--  2.0 unx     3517 b- defN 23-May-26 22:37 deephaven/execution_context.py
--rw-r--r--  2.0 unx     5157 b- defN 23-May-26 22:37 deephaven/filters.py
--rw-r--r--  2.0 unx      650 b- defN 23-May-26 22:37 deephaven/html.py
--rw-r--r--  2.0 unx     4696 b- defN 23-May-26 22:37 deephaven/jcompat.py
--rw-r--r--  2.0 unx     2503 b- defN 23-May-26 22:37 deephaven/liveness_scope.py
--rw-r--r--  2.0 unx     6109 b- defN 23-May-26 22:37 deephaven/numpy.py
--rw-r--r--  2.0 unx     8745 b- defN 23-May-26 22:37 deephaven/pandas.py
--rw-r--r--  2.0 unx     9173 b- defN 23-May-26 22:37 deephaven/parquet.py
--rw-r--r--  2.0 unx     8081 b- defN 23-May-26 22:37 deephaven/perfmon.py
--rw-r--r--  2.0 unx     2690 b- defN 23-May-26 22:37 deephaven/query_library.py
--rw-r--r--  2.0 unx     2471 b- defN 23-May-26 22:37 deephaven/replay.py
--rw-r--r--  2.0 unx   154120 b- defN 23-May-26 22:37 deephaven/table.py
--rw-r--r--  2.0 unx    11181 b- defN 23-May-26 22:37 deephaven/table_factory.py
--rw-r--r--  2.0 unx    15976 b- defN 23-May-26 22:37 deephaven/table_listener.py
--rw-r--r--  2.0 unx    21914 b- defN 23-May-26 22:37 deephaven/time.py
--rw-r--r--  2.0 unx     4712 b- defN 23-May-26 22:37 deephaven/ugp.py
--rw-r--r--  2.0 unx    67886 b- defN 23-May-26 22:37 deephaven/updateby.py
--rw-r--r--  2.0 unx     1155 b- defN 23-May-26 22:37 deephaven/uri.py
--rw-r--r--  2.0 unx      842 b- defN 23-May-26 22:37 deephaven/config/__init__.py
--rw-r--r--  2.0 unx     3200 b- defN 23-May-26 22:37 deephaven/dbc/__init__.py
--rw-r--r--  2.0 unx     1794 b- defN 23-May-26 22:37 deephaven/dbc/adbc.py
--rw-r--r--  2.0 unx     1668 b- defN 23-May-26 22:37 deephaven/dbc/odbc.py
--rw-r--r--  2.0 unx     1316 b- defN 23-May-26 22:37 deephaven/experimental/__init__.py
--rw-r--r--  2.0 unx     4815 b- defN 23-May-26 22:37 deephaven/experimental/ema.py
--rw-r--r--  2.0 unx     4097 b- defN 23-May-26 22:37 deephaven/experimental/outer_joins.py
--rw-r--r--  2.0 unx     7350 b- defN 23-May-26 22:37 deephaven/learn/__init__.py
--rw-r--r--  2.0 unx     3120 b- defN 23-May-26 22:37 deephaven/learn/gather.py
--rw-r--r--  2.0 unx      347 b- defN 23-May-26 22:37 deephaven/pandasplugin/__init__.py
--rw-r--r--  2.0 unx      550 b- defN 23-May-26 22:37 deephaven/pandasplugin/pandas_as_table.py
--rw-r--r--  2.0 unx      574 b- defN 23-May-26 22:37 deephaven/plot/__init__.py
--rw-r--r--  2.0 unx     2285 b- defN 23-May-26 22:37 deephaven/plot/axisformat.py
--rw-r--r--  2.0 unx     1479 b- defN 23-May-26 22:37 deephaven/plot/axistransform.py
--rw-r--r--  2.0 unx    16541 b- defN 23-May-26 22:37 deephaven/plot/color.py
--rw-r--r--  2.0 unx   110233 b- defN 23-May-26 22:37 deephaven/plot/figure.py
--rw-r--r--  2.0 unx     1659 b- defN 23-May-26 22:37 deephaven/plot/font.py
--rw-r--r--  2.0 unx     3514 b- defN 23-May-26 22:37 deephaven/plot/linestyle.py
--rw-r--r--  2.0 unx     1285 b- defN 23-May-26 22:37 deephaven/plot/plotstyle.py
--rw-r--r--  2.0 unx     2493 b- defN 23-May-26 22:37 deephaven/plot/selectable_dataset.py
--rw-r--r--  2.0 unx     1213 b- defN 23-May-26 22:37 deephaven/plot/shape.py
--rw-r--r--  2.0 unx      248 b- defN 23-May-26 22:37 deephaven/server/__init__.py
--rw-r--r--  2.0 unx      584 b- defN 23-May-26 22:37 deephaven/server/plugin/__init__.py
--rw-r--r--  2.0 unx     1098 b- defN 23-May-26 22:37 deephaven/server/plugin/register.py
--rw-r--r--  2.0 unx     1800 b- defN 23-May-26 22:37 deephaven/server/plugin/object/__init__.py
--rw-r--r--  2.0 unx     1850 b- defN 23-May-26 22:37 deephaven/server/script_session/__init__.py
--rw-r--r--  2.0 unx     1074 b- defN 23-May-26 22:37 deephaven/stream/__init__.py
--rw-r--r--  2.0 unx     1019 b- defN 23-May-26 22:37 deephaven/stream/kafka/__init__.py
--rw-r--r--  2.0 unx     7082 b- defN 23-May-26 22:37 deephaven/stream/kafka/cdc.py
--rw-r--r--  2.0 unx    18025 b- defN 23-May-26 22:37 deephaven/stream/kafka/consumer.py
--rw-r--r--  2.0 unx    10818 b- defN 23-May-26 22:37 deephaven/stream/kafka/producer.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-26 22:37 deephaven_internal/__init__.py
--rw-r--r--  2.0 unx     1576 b- defN 23-May-26 22:37 deephaven_internal/java_threads.py
--rw-r--r--  2.0 unx     3426 b- defN 23-May-26 22:37 deephaven_internal/stream.py
--rw-r--r--  2.0 unx      952 b- defN 23-May-26 22:37 deephaven_internal/auto_completer/__init__.py
--rw-r--r--  2.0 unx     8476 b- defN 23-May-26 22:37 deephaven_internal/auto_completer/_completer.py
--rw-r--r--  2.0 unx     2585 b- defN 23-May-26 22:37 deephaven_internal/jvm/__init__.py
--rw-r--r--  2.0 unx     2647 b- defN 23-May-26 22:37 deephaven_core-0.24.3.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-May-26 22:37 deephaven_core-0.24.3.dist-info/WHEEL
--rw-r--r--  2.0 unx       87 b- defN 23-May-26 22:37 deephaven_core-0.24.3.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       29 b- defN 23-May-26 22:37 deephaven_core-0.24.3.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     6081 b- defN 23-May-26 22:37 deephaven_core-0.24.3.dist-info/RECORD
-73 files, 637132 bytes uncompressed, 120824 bytes compressed:  81.0%
+Zip file size: 135862 bytes, number of entries: 74
+-rw-r--r--  2.0 unx     1120 b- defN 23-Jun-03 19:18 deephaven/__init__.py
+-rw-r--r--  2.0 unx      970 b- defN 23-Jun-03 19:18 deephaven/_gc.py
+-rw-r--r--  2.0 unx      676 b- defN 23-Jun-03 19:18 deephaven/_jpy.py
+-rw-r--r--  2.0 unx     4551 b- defN 23-Jun-03 19:18 deephaven/_wrapper.py
+-rw-r--r--  2.0 unx    13848 b- defN 23-Jun-03 19:18 deephaven/agg.py
+-rw-r--r--  2.0 unx     1987 b- defN 23-Jun-03 19:18 deephaven/appmode.py
+-rw-r--r--  2.0 unx     4758 b- defN 23-Jun-03 19:18 deephaven/arrow.py
+-rw-r--r--  2.0 unx    14651 b- defN 23-Jun-03 19:18 deephaven/calendar.py
+-rw-r--r--  2.0 unx     7275 b- defN 23-Jun-03 19:18 deephaven/column.py
+-rw-r--r--  2.0 unx     3123 b- defN 23-Jun-03 19:18 deephaven/constants.py
+-rw-r--r--  2.0 unx     4937 b- defN 23-Jun-03 19:18 deephaven/csv.py
+-rw-r--r--  2.0 unx     2890 b- defN 23-Jun-03 19:18 deephaven/dherror.py
+-rw-r--r--  2.0 unx     9696 b- defN 23-Jun-03 19:18 deephaven/dtypes.py
+-rw-r--r--  2.0 unx     3749 b- defN 23-Jun-03 19:18 deephaven/execution_context.py
+-rw-r--r--  2.0 unx     5157 b- defN 23-Jun-03 19:18 deephaven/filters.py
+-rw-r--r--  2.0 unx      650 b- defN 23-Jun-03 19:18 deephaven/html.py
+-rw-r--r--  2.0 unx     4708 b- defN 23-Jun-03 19:18 deephaven/jcompat.py
+-rw-r--r--  2.0 unx     2503 b- defN 23-Jun-03 19:18 deephaven/liveness_scope.py
+-rw-r--r--  2.0 unx     6214 b- defN 23-Jun-03 19:18 deephaven/numpy.py
+-rw-r--r--  2.0 unx     9027 b- defN 23-Jun-03 19:18 deephaven/pandas.py
+-rw-r--r--  2.0 unx     9173 b- defN 23-Jun-03 19:18 deephaven/parquet.py
+-rw-r--r--  2.0 unx     8081 b- defN 23-Jun-03 19:18 deephaven/perfmon.py
+-rw-r--r--  2.0 unx     2690 b- defN 23-Jun-03 19:18 deephaven/query_library.py
+-rw-r--r--  2.0 unx     2711 b- defN 23-Jun-03 19:18 deephaven/replay.py
+-rw-r--r--  2.0 unx   154823 b- defN 23-Jun-03 19:18 deephaven/table.py
+-rw-r--r--  2.0 unx    11192 b- defN 23-Jun-03 19:18 deephaven/table_factory.py
+-rw-r--r--  2.0 unx    16259 b- defN 23-Jun-03 19:18 deephaven/table_listener.py
+-rw-r--r--  2.0 unx    57543 b- defN 23-Jun-03 19:18 deephaven/time.py
+-rw-r--r--  2.0 unx     7982 b- defN 23-Jun-03 19:18 deephaven/update_graph.py
+-rw-r--r--  2.0 unx    71286 b- defN 23-Jun-03 19:18 deephaven/updateby.py
+-rw-r--r--  2.0 unx     1155 b- defN 23-Jun-03 19:18 deephaven/uri.py
+-rw-r--r--  2.0 unx      385 b- defN 23-Jun-03 19:18 deephaven/config/__init__.py
+-rw-r--r--  2.0 unx     3200 b- defN 23-Jun-03 19:18 deephaven/dbc/__init__.py
+-rw-r--r--  2.0 unx     1794 b- defN 23-Jun-03 19:18 deephaven/dbc/adbc.py
+-rw-r--r--  2.0 unx     1668 b- defN 23-Jun-03 19:18 deephaven/dbc/odbc.py
+-rw-r--r--  2.0 unx     1316 b- defN 23-Jun-03 19:18 deephaven/experimental/__init__.py
+-rw-r--r--  2.0 unx     4815 b- defN 23-Jun-03 19:18 deephaven/experimental/ema.py
+-rw-r--r--  2.0 unx     4106 b- defN 23-Jun-03 19:18 deephaven/experimental/outer_joins.py
+-rw-r--r--  2.0 unx     1570 b- defN 23-Jun-03 19:18 deephaven/experimental/sql.py
+-rw-r--r--  2.0 unx     7350 b- defN 23-Jun-03 19:18 deephaven/learn/__init__.py
+-rw-r--r--  2.0 unx     3120 b- defN 23-Jun-03 19:18 deephaven/learn/gather.py
+-rw-r--r--  2.0 unx      347 b- defN 23-Jun-03 19:18 deephaven/pandasplugin/__init__.py
+-rw-r--r--  2.0 unx      550 b- defN 23-Jun-03 19:18 deephaven/pandasplugin/pandas_as_table.py
+-rw-r--r--  2.0 unx      574 b- defN 23-Jun-03 19:18 deephaven/plot/__init__.py
+-rw-r--r--  2.0 unx     2279 b- defN 23-Jun-03 19:18 deephaven/plot/axisformat.py
+-rw-r--r--  2.0 unx     1479 b- defN 23-Jun-03 19:18 deephaven/plot/axistransform.py
+-rw-r--r--  2.0 unx    16541 b- defN 23-Jun-03 19:18 deephaven/plot/color.py
+-rw-r--r--  2.0 unx   110184 b- defN 23-Jun-03 19:18 deephaven/plot/figure.py
+-rw-r--r--  2.0 unx     1659 b- defN 23-Jun-03 19:18 deephaven/plot/font.py
+-rw-r--r--  2.0 unx     3514 b- defN 23-Jun-03 19:18 deephaven/plot/linestyle.py
+-rw-r--r--  2.0 unx     1285 b- defN 23-Jun-03 19:18 deephaven/plot/plotstyle.py
+-rw-r--r--  2.0 unx     2493 b- defN 23-Jun-03 19:18 deephaven/plot/selectable_dataset.py
+-rw-r--r--  2.0 unx     1213 b- defN 23-Jun-03 19:18 deephaven/plot/shape.py
+-rw-r--r--  2.0 unx      248 b- defN 23-Jun-03 19:18 deephaven/server/__init__.py
+-rw-r--r--  2.0 unx      584 b- defN 23-Jun-03 19:18 deephaven/server/plugin/__init__.py
+-rw-r--r--  2.0 unx     1098 b- defN 23-Jun-03 19:18 deephaven/server/plugin/register.py
+-rw-r--r--  2.0 unx     1800 b- defN 23-Jun-03 19:18 deephaven/server/plugin/object/__init__.py
+-rw-r--r--  2.0 unx     1850 b- defN 23-Jun-03 19:18 deephaven/server/script_session/__init__.py
+-rw-r--r--  2.0 unx     1074 b- defN 23-Jun-03 19:18 deephaven/stream/__init__.py
+-rw-r--r--  2.0 unx     1019 b- defN 23-Jun-03 19:18 deephaven/stream/kafka/__init__.py
+-rw-r--r--  2.0 unx     7082 b- defN 23-Jun-03 19:18 deephaven/stream/kafka/cdc.py
+-rw-r--r--  2.0 unx    18025 b- defN 23-Jun-03 19:18 deephaven/stream/kafka/consumer.py
+-rw-r--r--  2.0 unx    10818 b- defN 23-Jun-03 19:18 deephaven/stream/kafka/producer.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jun-03 19:18 deephaven_internal/__init__.py
+-rw-r--r--  2.0 unx     1576 b- defN 23-Jun-03 19:18 deephaven_internal/java_threads.py
+-rw-r--r--  2.0 unx     3426 b- defN 23-Jun-03 19:18 deephaven_internal/stream.py
+-rw-r--r--  2.0 unx      952 b- defN 23-Jun-03 19:18 deephaven_internal/auto_completer/__init__.py
+-rw-r--r--  2.0 unx     8476 b- defN 23-Jun-03 19:18 deephaven_internal/auto_completer/_completer.py
+-rw-r--r--  2.0 unx     2585 b- defN 23-Jun-03 19:18 deephaven_internal/jvm/__init__.py
+-rw-r--r--  2.0 unx     2647 b- defN 23-Jun-03 19:18 deephaven_core-0.25.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jun-03 19:18 deephaven_core-0.25.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       87 b- defN 23-Jun-03 19:18 deephaven_core-0.25.0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       29 b- defN 23-Jun-03 19:18 deephaven_core-0.25.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     6176 b- defN 23-Jun-03 19:18 deephaven_core-0.25.0.dist-info/RECORD
+74 files, 686471 bytes uncompressed, 126180 bytes compressed:  81.6%
```

## zipnote {}

```diff
@@ -78,15 +78,15 @@
 
 Filename: deephaven/table_listener.py
 Comment: 
 
 Filename: deephaven/time.py
 Comment: 
 
-Filename: deephaven/ugp.py
+Filename: deephaven/update_graph.py
 Comment: 
 
 Filename: deephaven/updateby.py
 Comment: 
 
 Filename: deephaven/uri.py
 Comment: 
@@ -108,14 +108,17 @@
 
 Filename: deephaven/experimental/ema.py
 Comment: 
 
 Filename: deephaven/experimental/outer_joins.py
 Comment: 
 
+Filename: deephaven/experimental/sql.py
+Comment: 
+
 Filename: deephaven/learn/__init__.py
 Comment: 
 
 Filename: deephaven/learn/gather.py
 Comment: 
 
 Filename: deephaven/pandasplugin/__init__.py
@@ -198,23 +201,23 @@
 
 Filename: deephaven_internal/auto_completer/_completer.py
 Comment: 
 
 Filename: deephaven_internal/jvm/__init__.py
 Comment: 
 
-Filename: deephaven_core-0.24.3.dist-info/METADATA
+Filename: deephaven_core-0.25.0.dist-info/METADATA
 Comment: 
 
-Filename: deephaven_core-0.24.3.dist-info/WHEEL
+Filename: deephaven_core-0.25.0.dist-info/WHEEL
 Comment: 
 
-Filename: deephaven_core-0.24.3.dist-info/entry_points.txt
+Filename: deephaven_core-0.25.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: deephaven_core-0.24.3.dist-info/top_level.txt
+Filename: deephaven_core-0.25.0.dist-info/top_level.txt
 Comment: 
 
-Filename: deephaven_core-0.24.3.dist-info/RECORD
+Filename: deephaven_core-0.25.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## deephaven/__init__.py

```diff
@@ -3,15 +3,15 @@
 #
 
 """Deephaven Python Integration Package provides the ability to access the Deephaven's query engine natively and thus
 unlocks the unique power of Deephaven to the Python community.
 
 """
 
-__version__ = "0.24.3"
+__version__ = "0.25.0"
 
 from deephaven_internal import jvm
 
 try:
     jvm.check_ready()
 finally:
     del jvm
```

## deephaven/agg.py

```diff
@@ -1,23 +1,24 @@
 #
 # Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
 #
 
 from __future__ import annotations
 
-from typing import List, Union
+from typing import List, Union, Any
 
 import jpy
 
 from deephaven import DHError
 from deephaven.jcompat import to_sequence
 
 _JAggregation = jpy.get_type("io.deephaven.api.agg.Aggregation")
 _JAggSpec = jpy.get_type("io.deephaven.api.agg.spec.AggSpec")
 _JPair = jpy.get_type("io.deephaven.api.Pair")
+_JUnionObject = jpy.get_type("io.deephaven.api.object.UnionObject")
 
 
 class Aggregation:
     """An Aggregation object represents an aggregation operation.
 
     Note: It should not be instantiated directly by user code but rather through the static methods on the class.
     """
@@ -41,283 +42,322 @@
     def j_agg_spec(self):
         if self._j_aggregation:
             raise DHError(message="unsupported aggregation operation.")
         return self._j_agg_spec
 
 
 def sum_(cols: Union[str, List[str]] = None) -> Aggregation:
-    """Create a Sum aggregation.
+    """Creates a Sum aggregation.
 
     Args:
         cols (Union[str, List[str]]): the column(s) to aggregate on, can be renaming expressions, i.e. "new_col = col";
             default is None, only valid when used in Table agg_all_by operation
 
     Returns:
         an aggregation
     """
     return Aggregation(j_agg_spec=_JAggSpec.sum(), cols=cols)
 
 
 def abs_sum(cols: Union[str, List[str]] = None) -> Aggregation:
-    """Create an Absolute-sum aggregation.
+    """Creates an Absolute-sum aggregation.
 
     Args:
         cols (Union[str, List[str]]): the column(s) to aggregate on, can be renaming expressions, i.e. "new_col = col";
             default is None, only valid when used in Table agg_all_by operation
 
     Returns:
         an aggregation
     """
     return Aggregation(j_agg_spec=_JAggSpec.absSum(), cols=cols)
 
 
 def group(cols: Union[str, List[str]] = None) -> Aggregation:
-    """Create a Group aggregation.
+    """Creates a Group aggregation.
 
     Args:
         cols (Union[str, List[str]]): the column(s) to aggregate on, can be renaming expressions, i.e. "new_col = col";
             default is None, only valid when used in Table agg_all_by operation
 
     Returns:
         an aggregation
     """
     return Aggregation(j_agg_spec=_JAggSpec.group(), cols=cols)
 
 
 def avg(cols: Union[str, List[str]] = None) -> Aggregation:
-    """Create an Average aggregation.
+    """Creates an Average aggregation.
 
     Args:
         cols (Union[str, List[str]]): the column(s) to aggregate on, can be renaming expressions, i.e. "new_col = col";
             default is None, only valid when used in Table agg_all_by operation
 
     Returns:
         an aggregation
     """
     return Aggregation(j_agg_spec=_JAggSpec.avg(), cols=cols)
 
 
 def count_(col: str) -> Aggregation:
-    """Create a Count aggregation. This is not supported in 'Table.agg_all_by'.
+    """Creates a Count aggregation. This is not supported in 'Table.agg_all_by'.
 
     Args:
         col (str): the column to hold the counts of each distinct group
 
     Returns:
         an aggregation
     """
     return Aggregation(j_aggregation=_JAggregation.AggCount(col))
 
 
 def partition(col: str, include_by_columns: bool = True) -> Aggregation:
-    """Create a Partition aggregation. This is not supported in 'Table.agg_all_by'.
+    """Creates a Partition aggregation. This is not supported in 'Table.agg_all_by'.
 
     Args:
         col (str): the column to hold the sub tables
         include_by_columns (bool): whether to include the group by columns in the result, default is True
 
     Returns:
         an aggregation
     """
     return Aggregation(j_aggregation=_JAggregation.AggPartition(col, include_by_columns))
 
 
-def count_distinct(cols: Union[str, List[str]] = None) -> Aggregation:
-    """Create a Count Distinct aggregation.
+def count_distinct(cols: Union[str, List[str]] = None, count_nulls: bool = False) -> Aggregation:
+    """Creates a Count Distinct aggregation which computes the count of distinct values within an aggregation group for
+    each of the given columns.
 
     Args:
         cols (Union[str, List[str]]): the column(s) to aggregate on, can be renaming expressions, i.e. "new_col = col";
             default is None, only valid when used in Table agg_all_by operation
+        count_nulls (bool): whether null values should be counted, default is False
 
     Returns:
         an aggregation
     """
-    return Aggregation(j_agg_spec=_JAggSpec.countDistinct(), cols=cols)
+    return Aggregation(j_agg_spec=_JAggSpec.countDistinct(count_nulls), cols=cols)
 
 
 def first(cols: Union[str, List[str]] = None) -> Aggregation:
-    """Create a First aggregation.
+    """Creates a First aggregation.
 
     Args:
         cols (Union[str, List[str]]): the column(s) to aggregate on, can be renaming expressions, i.e. "new_col = col";
             default is None, only valid when used in Table agg_all_by operation
 
     Returns:
         an aggregation
     """
     return Aggregation(j_agg_spec=_JAggSpec.first(), cols=cols)
 
 
 def formula(formula: str, formula_param: str, cols: Union[str, List[str]] = None) -> Aggregation:
-    """Create a user defined formula aggregation.
+    """Creates a user defined formula aggregation.
 
     Args:
         formula (str): the user defined formula to apply to each group
         formula_param (str): the parameter name within the formula
         cols (Union[str, List[str]]): the column(s) to aggregate on, can be renaming expressions, i.e. "new_col = col";
             default is None, only valid when used in Table agg_all_by operation
 
     Returns:
         an aggregation
     """
     return Aggregation(j_agg_spec=_JAggSpec.formula(formula, formula_param), cols=cols)
 
 
 def last(cols: Union[str, List[str]] = None) -> Aggregation:
-    """Create Last aggregation.
+    """Creates Last aggregation.
 
     Args:
         cols (Union[str, List[str]]): the column(s) to aggregate on, can be renaming expressions, i.e. "new_col = col";
             default is None, only valid when used in Table agg_all_by operation
 
     Returns:
         an aggregation
     """
     return Aggregation(j_agg_spec=_JAggSpec.last(), cols=cols)
 
 
 def min_(cols: Union[str, List[str]] = None) -> Aggregation:
-    """Create a Min aggregation.
+    """Creates a Min aggregation.
 
     Args:
         cols (Union[str, List[str]]): the column(s) to aggregate on, can be renaming expressions, i.e. "new_col = col";
             default is None, only valid when used in Table agg_all_by operation
 
     Returns:
         an aggregation
     """
     return Aggregation(j_agg_spec=_JAggSpec.min(), cols=cols)
 
 
 def max_(cols: Union[str, List[str]] = None) -> Aggregation:
-    """Create a Max aggregation to the ComboAggregation object.
+    """Creates a Max aggregation to the ComboAggregation object.
 
     Args:
         cols (Union[str, List[str]]): the column(s) to aggregate on, can be renaming expressions, i.e. "new_col = col";
             default is None, only valid when used in Table agg_all_by operation
 
     Returns:
         an aggregation
     """
     return Aggregation(j_agg_spec=_JAggSpec.max(), cols=cols)
 
 
-def median(cols: Union[str, List[str]] = None) -> Aggregation:
-    """Create a Median aggregation.
+def median(cols: Union[str, List[str]] = None, average_evenly_divided: bool = True) -> Aggregation:
+    """Creates a Median aggregation which computes the median value within an aggregation group for each of the
+    given columns.
 
     Args:
         cols (Union[str, List[str]]): the column(s) to aggregate on, can be renaming expressions, i.e. "new_col = col";
             default is None, only valid when used in Table agg_all_by operation
+        average_evenly_divided (bool): when the group size is an even number, whether to average the two middle values
+            for the output value. When set to True, average the two middle values. When set to False, use the smaller
+            value. The default is True. This flag is only valid for numeric types.
 
     Returns:
         an aggregation
     """
-    return Aggregation(j_agg_spec=_JAggSpec.median(), cols=cols)
+    return Aggregation(j_agg_spec=_JAggSpec.median(average_evenly_divided), cols=cols)
 
 
-def pct(percentile: float, cols: Union[str, List[str]] = None) -> Aggregation:
-    """Create a Percentile aggregation.
+def pct(percentile: float, cols: Union[str, List[str]] = None, average_evenly_divided: bool = False) -> Aggregation:
+    """Creates a Percentile aggregation which computes the percentile value within an aggregation group for each of
+    the given columns.
 
     Args:
         percentile (float): the percentile used for calculation
         cols (Union[str, List[str]]): the column(s) to aggregate on, can be renaming expressions, i.e. "new_col = col";
             default is None, only valid when used in Table agg_all_by operation
+        average_evenly_divided (bool): when the percentile splits the group into two halves, whether to average the two
+            middle values for the output value. When set to True, average the two middle values. When set to False, use
+            the smaller value. The default is False. This flag is only valid for numeric types.
 
     Returns:
         an aggregation
     """
-    return Aggregation(j_agg_spec=_JAggSpec.percentile(percentile), cols=cols)
+    return Aggregation(j_agg_spec=_JAggSpec.percentile(percentile, average_evenly_divided), cols=cols)
 
 
 def sorted_first(order_by: str, cols: Union[str, List[str]] = None) -> Aggregation:
-    """Create a SortedFirst aggregation.
+    """Creates a SortedFirst aggregation.
 
     Args:
         order_by (str): the column to sort by
         cols (Union[str, List[str]]): the column(s) to aggregate on, can be renaming expressions, i.e. "new_col = col";
             default is None, only valid when used in Table agg_all_by operation
 
     Returns:
         an aggregation
     """
     return Aggregation(j_agg_spec=_JAggSpec.sortedFirst(order_by), cols=cols)
 
 
 def sorted_last(order_by: str, cols: Union[str, List[str]] = None) -> Aggregation:
-    """Create a SortedLast aggregation.
+    """Creates a SortedLast aggregation.
 
     Args:
         order_by (str): the column to sort by
         cols (Union[str, List[str]]): the column(s) to aggregate on, can be renaming expressions, i.e. "new_col = col";
             default is None, only valid when used in Table agg_all_by operation
 
     Returns:
         an aggregation
     """
     return Aggregation(j_agg_spec=_JAggSpec.sortedLast(order_by), cols=cols)
 
 
 def std(cols: Union[str, List[str]] = None) -> Aggregation:
-    """Create a Std aggregation.
+    """Creates a Std aggregation.
 
     Args:
         cols (Union[str, List[str]]): the column(s) to aggregate on, can be renaming expressions, i.e. "new_col = col";
             default is None, only valid when used in Table agg_all_by operation
 
     Returns:
         an aggregation
     """
     return Aggregation(j_agg_spec=_JAggSpec.std(), cols=cols)
 
 
-def unique(cols: Union[str, List[str]] = None) -> Aggregation:
-    """Create a Unique aggregation.
+def unique(cols: Union[str, List[str]] = None, include_nulls: bool = False, non_unique_sentinel: Any = None) -> Aggregation:
+    """Creates a Unique aggregation which computes the single unique value within an aggregation group for each of
+    the given columns. If all values in a column are null, or if there is more than one distinct value in a column, the
+    result is null or the specified non_unique_sentinel value.
 
     Args:
         cols (Union[str, List[str]]): the column(s) to aggregate on, can be renaming expressions, i.e. "new_col = col";
             default is None, only valid when used in Table agg_all_by operation
+        include_nulls (bool): whether null is treated as a value for the purpose of determining if the values in the
+            aggregation group are unique, default is False.
+        non_unique_sentinel (Any): the non-null sentinel value when no unique value exists, default is None. Must be
+            a non-None value when include_nulls is True.
 
     Returns:
         an aggregation
     """
-    return Aggregation(j_agg_spec=_JAggSpec.unique(), cols=cols)
+
+    if non_unique_sentinel is None:
+        if include_nulls:
+            raise DHError(message="when include_nulls is True, a non-null sentinel value must be provided.")
+        else:
+            return Aggregation(j_agg_spec=_JAggSpec.unique(), cols=cols)
+    else:
+        non_unique_sentinel = _JUnionObject.of(non_unique_sentinel)
+        return Aggregation(j_agg_spec=_JAggSpec.unique(include_nulls, non_unique_sentinel), cols=cols)
 
 
 def var(cols: Union[str, List[str]] = None) -> Aggregation:
-    """Create a Var aggregation.
+    """Creates a Var aggregation.
 
     Args:
         cols (Union[str, List[str]]): the column(s) to aggregate on, can be renaming expressions, i.e. "new_col = col";
             default is None, only valid when used in Table agg_all_by operation
 
     Returns:
         an aggregation
     """
     return Aggregation(j_agg_spec=_JAggSpec.var(), cols=cols)
 
 
 def weighted_avg(wcol: str, cols: Union[str, List[str]] = None) -> Aggregation:
-    """Create a Weighted-avg aggregation.
+    """Creates a Weighted-avg aggregation.
 
     Args:
         wcol (str): the name of the weight column
         cols (Union[str, List[str]]): the column(s) to aggregate on, can be renaming expressions, i.e. "new_col = col";
             default is None, only valid when used in Table agg_all_by operation
 
     Returns:
         an aggregation
     """
     return Aggregation(j_agg_spec=_JAggSpec.wavg(wcol), cols=cols)
 
 
 def weighted_sum(wcol: str, cols: Union[str, List[str]] = None) -> Aggregation:
-    """Create a Weighted-sum aggregation.
+    """Creates a Weighted-sum aggregation.
 
     Args:
         wcol (str): the name of the weight column
         cols (Union[str, List[str]]): the column(s) to aggregate on, can be renaming expressions, i.e. "new_col = col";
             default is None, only valid when used in Table agg_all_by operation
 
     Returns:
         an aggregation
     """
     return Aggregation(j_agg_spec=_JAggSpec.wsum(wcol), cols=cols)
+
+
+def distinct(cols: Union[str, List[str]] = None, include_nulls: bool = False) -> Aggregation:
+    """Creates a Distinct aggregation which computes the distinct values within an aggregation group for each of the
+    given columns and stores them as vectors.
+
+    Args:
+        cols (Union[str, List[str]]): the column(s) to aggregate on, can be renaming expressions, i.e. "new_col = col";
+            default is None, only valid when used in Table agg_all_by operation
+        include_nulls (bool): whether nulls should be included as distinct values, default is False
+
+    Returns:
+        an aggregation
+    """
+    return Aggregation(j_agg_spec=_JAggSpec.distinct(include_nulls), cols=cols)
```

## deephaven/arrow.py

```diff
@@ -31,15 +31,15 @@
     pa.time32('s'): '',
     pa.time32('ms'): '',
     pa.time64('us'): '',
     pa.time64('ns'): '',
     pa.timestamp('s'): '',
     pa.timestamp('ms'): '',
     pa.timestamp('us'): '',
-    pa.timestamp('ns'): 'io.deephaven.time.DateTime',
+    pa.timestamp('ns'): 'java.time.Instant',
     pa.date32(): '',
     pa.date64(): '',
     pa.duration('s'): '',
     pa.duration('ms'): '',
     pa.duration('us'): '',
     pa.duration('ns'): '',
     pa.month_day_nano_interval(): '',
@@ -62,15 +62,15 @@
 
 def _map_arrow_type(arrow_type) -> Dict[str, str]:
     """Maps a pyarrow type to the corresponding Deephaven column data type."""
     dh_type = _ARROW_DH_DATA_TYPE_MAPPING.get(arrow_type)
     if not dh_type:
         # if this is a case of timestamp with tz specified
         if isinstance(arrow_type, pa.TimestampType):
-            dh_type = "io.deephaven.time.DateTime"
+            dh_type = "java.time.Instant"
 
     if not dh_type:
         raise DHError(message=f'unsupported arrow data type : {arrow_type}, refer to '
                               f'deephaven.arrow.SUPPORTED_ARROW_TYPES for the list of supported pyarrow types.')
 
     return {"deephaven:type": dh_type}
```

## deephaven/calendar.py

```diff
@@ -6,15 +6,15 @@
 
 from enum import Enum
 from typing import List
 
 import jpy
 
 from deephaven._wrapper import JObjectWrapper
-from deephaven.time import TimeZone, DateTime
+from deephaven.time import TimeZone, Instant
 
 from deephaven import DHError
 
 _JCalendars = jpy.get_type("io.deephaven.time.calendar.Calendars")
 _JCalendar = jpy.get_type("io.deephaven.time.calendar.Calendar")
 _JDayOfWeek = jpy.get_type("java.time.DayOfWeek")
 _JBusinessPeriod = jpy.get_type("io.deephaven.time.calendar.BusinessPeriod")
@@ -79,20 +79,20 @@
         return repr(self)
 
     @property
     def j_object(self) -> jpy.JType:
         return self.j_business_period
 
     @property
-    def start_time(self) -> DateTime:
+    def start_time(self) -> Instant:
         """ The start of the period. """
         return self.j_business_period.getStartTime()
 
     @property
-    def end_time(self) -> DateTime:
+    def end_time(self) -> Instant:
         """ The end of the period. """
         return self.j_business_period.getEndTime()
 
     @property
     def length(self) -> int:
         """ The length of the period in nanoseconds. """
         return self.j_business_period.getLength()
@@ -119,48 +119,48 @@
         j_periods = self.j_business_schedule.getBusinessPeriods()
         periods = []
         for j_period in j_periods:
             periods.append(BusinessPeriod(j_period))
         return periods
 
     @property
-    def start_of_day(self) -> DateTime:
+    def start_of_day(self) -> Instant:
         """ The start of the business day. """
         return self.j_business_schedule.getStartOfBusinessDay()
 
     @property
-    def end_of_day(self) -> DateTime:
+    def end_of_day(self) -> Instant:
         """ The end of the business day. """
         return self.j_business_schedule.getEndOfBusinessDay()
 
     def is_business_day(self) -> bool:
         """ Whether it is a business day.
 
         Returns:
             bool
         """
         return self.j_business_schedule.isBusinessDay()
 
-    def is_business_time(self, time: DateTime) -> bool:
+    def is_business_time(self, time: Instant) -> bool:
         """ Whether the specified time is a business time for the day.
 
         Args:
-             time (DateTime): the time during the day
+             time (Instant): the time during the day
 
         Return:
             bool
         """
         return self.j_business_schedule.isBusinessTime(time)
 
-    def business_time_elapsed(self, time: DateTime) -> int:
+    def business_time_elapsed(self, time: Instant) -> int:
         """ Returns the amount of business time in nanoseconds that has elapsed on the given day by the specified
         time.
 
         Args:
-            time (DateTime): the time during the day
+            time (Instant): the time during the day
 
         Returns:
             int
         """
         return self.j_business_schedule.businessTimeElapsed(time)
 
 
@@ -175,15 +175,15 @@
     def current_day(self) -> str:
         """ The current day. """
         return self.j_calendar.currentDay()
 
     @property
     def time_zone(self) -> TimeZone:
         """ Returns the timezone of the calendar. """
-        return TimeZone(self.j_calendar.timeZone())
+        return self.j_calendar.timeZone()
 
     def previous_day(self, date: str) -> str:
         """ Gets the day prior to the given date.
 
         Args:
             date (str): the date of interest
```

## deephaven/column.py

```diff
@@ -197,15 +197,15 @@
     Args:
         name (str): the column name
         data (Any): a sequence of Datetime instances
 
     Returns:
         a new input column
     """
-    return InputColumn(name=name, data_type=dtypes.DateTime, input_data=data)
+    return InputColumn(name=name, data_type=dtypes.Instant, input_data=data)
 
 
 def pyobj_col(name: str, data: Sequence) -> InputColumn:
     """ Creates an input column containing complex, non-primitive-like Python objects.
 
     Args:
         name (str): the column name
```

## deephaven/csv.py

```diff
@@ -7,26 +7,27 @@
 """
 from typing import Dict, List
 
 import jpy
 
 import deephaven.dtypes as dht
 from deephaven import DHError
-from deephaven.table import Table
 from deephaven.constants import MAX_LONG
+from deephaven.table import Table
 
 _JCsvTools = jpy.get_type("io.deephaven.csv.CsvTools")
 _JParsers = jpy.get_type("io.deephaven.csv.parsers.Parsers")
 _JArrays = jpy.get_type("java.util.Arrays")
 
 
 def read(
     path: str,
     header: Dict[str, dht.DType] = None,
     headless: bool = False,
+    header_row: int = 0,
     skip_rows: int = 0,
     num_rows: int = MAX_LONG,
     ignore_empty_lines: bool = False,
     allow_missing_columns: bool = False,
     ignore_excess_columns: bool = False,
     delimiter: str = ",",
     quote: str = '"',
@@ -34,22 +35,32 @@
     trim: bool = False,
 ) -> Table:
     """Read the CSV data specified by the path parameter as a table.
 
     Args:
         path (str): a file path or a URL string
         header (Dict[str, DType]): a dict to define the table columns with key being the name, value being the data type
-        skip_rows (long): number of data rows to skip before processing data. This is useful when you want to parse data in chunks. Defaults to 0
-        num_rows (long): max number of rows to process. This is useful when you want to parse data in chunks. Defaults to {@link Long#MAX_VALUE} 
-        allow_missing_columns (bool): whether the library should allow missing columns in the input. If this flag is set, then rows that are too short (that have fewer columns than the header row) will be interpreted as if the missing columns contained the empty string. Defaults to false.
-        ignore_excess_columns (bool): whether the library should allow excess columns in the input. If this flag is set, then rows that are too long (that have more columns than the header row) will have those excess columns dropped. Defaults to false.
-        headless (bool): indicates if the CSV data is headless, default is False
+        headless (bool): whether the csv file doesn't have a header row, default is False
+        header_row (int): the header row number, all the rows before it will be skipped, default is 0. Must be 0 if
+            headless is True, otherwise an exception will be raised
+        skip_rows (long): number of data rows to skip before processing data. This is useful when you want to parse
+            data in chunks. Defaults to 0
+        num_rows (long): max number of rows to process. This is useful when you want to parse data in chunks.
+            Defaults to {@link Long#MAX_VALUE}
+        ignore_empty_lines (bool): whether to ignore empty lines, default is False
+        allow_missing_columns (bool): whether the library should allow missing columns in the input. If this flag is
+            set, then rows that are too short (that have fewer columns than the header row) will be interpreted as if
+            the missing columns contained the empty string. Defaults to false.
+        ignore_excess_columns (bool): whether the library should allow excess columns in the input. If this flag is
+            set, then rows that are too long (that have more columns than the header row) will have those excess columns
+            dropped. Defaults to false.
         delimiter (str): the delimiter used by the CSV, default is the comma
         quote (str): the quote character for the CSV, default is double quote
-        ignore_surrounding_spaces (bool): Indicates whether surrounding white space should be ignored for unquoted text fields, default is True
+        ignore_surrounding_spaces (bool): Indicates whether surrounding white space should be ignored for unquoted
+            text fields, default is True
         trim (bool): indicates whether to trim white space inside a quoted string, default is False
 
     Returns:
         a table
 
     Raises:
         DHError
@@ -65,21 +76,22 @@
                 dht.char: _JParsers.CHAR,
                 dht.short: _JParsers.SHORT,
                 dht.int_: _JParsers.INT,
                 dht.long: _JParsers.LONG,
                 dht.float_: _JParsers.FLOAT_FAST,
                 dht.double: _JParsers.DOUBLE,
                 dht.string: _JParsers.STRING,
-                dht.DateTime: _JParsers.DATETIME,
+                dht.Instant: _JParsers.DATETIME,
             }
             for column_name, column_type in header.items():
                 csv_specs_builder.putParserForName(column_name, parser_map[column_type])
 
         csv_specs = (
             csv_specs_builder.hasHeaderRow(not headless)
+            .skipHeaderRows(header_row)
             .skipRows(skip_rows)
             .numRows(num_rows)
             .ignoreEmptyLines(ignore_empty_lines)
             .allowMissingColumns(allow_missing_columns)
             .ignoreExcessColumns(ignore_excess_columns)
             .delimiter(ord(delimiter))
             .quote(ord(quote))
@@ -106,8 +118,7 @@
     Raises:
         DHError
     """
     try:
         _JCsvTools.writeCsv(table.j_table, False, path, *cols)
     except Exception as e:
         raise DHError(message="write csv failed.") from e
-
```

## deephaven/dtypes.py

```diff
@@ -98,18 +98,28 @@
 """Double-precision floating-point number type"""
 string = DType(j_name="java.lang.String", qst_type=_JQstType.stringType(), np_type=np.str_)
 """String type"""
 BigDecimal = DType(j_name="java.math.BigDecimal")
 """Java BigDecimal type"""
 StringSet = DType(j_name="io.deephaven.stringset.StringSet")
 """Deephaven StringSet type"""
-DateTime = DType(j_name="io.deephaven.time.DateTime", np_type=np.dtype("datetime64[ns]"))
-"""Deephaven DateTime type"""
-Period = DType(j_name="io.deephaven.time.Period")
-"""Deephaven time period type"""
+Instant = DType(j_name="java.time.Instant", np_type=np.dtype("datetime64[ns]"))
+"""Instant date time type"""
+LocalDate = DType(j_name="java.time.LocalDate")
+"""Local date type"""
+LocalTime = DType(j_name="java.time.LocalTime")
+"""Local time type"""
+ZonedDateTime = DType(j_name="java.time.ZonedDateTime")
+"""Zoned date time type"""
+Duration = DType(j_name="java.time.Duration")
+"""Time period type, which is a unit of time in terms of clock time (24-hour days, hours, minutes, seconds, and nanoseconds)."""
+Period = DType(j_name="java.time.Period")
+"""Time period type, which is a unit of time in terms of calendar time (days, weeks, months, years, etc.)."""
+TimeZone = DType(j_name="java.time.ZoneId")
+"""Time zone type."""
 PyObject = DType(j_name="org.jpy.PyObject")
 """Python object type"""
 JObject = DType(j_name="java.lang.Object")
 """Java Object type"""
 byte_array = DType(j_name='[B')
 """Byte array type"""
 int8_array = byte_array
@@ -134,16 +144,18 @@
 """Double-precision floating-point array type"""
 float64_array = double_array
 """Double-precision floating-point array type"""
 float_array = double_array
 """Double-precision floating-point array type"""
 string_array = DType(j_name='[Ljava.lang.String;')
 """Java String array type"""
-datetime_array = DType(j_name='[Lio.deephaven.time.DateTime;')
-"""Deephaven DateTime array type"""
+instant_array = DType(j_name='[Ljava.time.Instant;')
+"""Java Instant array type"""
+zdt_array = DType(j_name='[Ljava.time.ZonedDateTime;')
+"""Zoned date time array type"""
 
 _PRIMITIVE_DTYPE_NULL_MAP = {
     bool_: NULL_BYTE,
     byte: NULL_BYTE,
     char: NULL_CHAR,
     int16: NULL_SHORT,
     int32: NULL_INT,
@@ -201,17 +213,17 @@
             seq = [remap(v) for v in seq]
 
         if isinstance(seq, np.ndarray):
             if dtype == bool_:
                 bytes_ = seq.astype(dtype=np.int8)
                 j_bytes = array(byte, bytes_)
                 seq = _JPrimitiveArrayConversionUtility.translateArrayByteToBoolean(j_bytes)
-            elif dtype == DateTime:
+            elif dtype == Instant:
                 longs = jpy.array('long', seq.astype('datetime64[ns]').astype('int64'))
-                seq = _JPrimitiveArrayConversionUtility.translateArrayLongToDateTime(longs)
+                seq = _JPrimitiveArrayConversionUtility.translateArrayLongToInstant(longs)
 
         return jpy.array(dtype.j_type, seq)
     except Exception as e:
         raise DHError(e, f"failed to create a Java {dtype.j_name} array.") from e
 
 
 def from_jtype(j_class: Any) -> DType:
@@ -240,14 +252,14 @@
         else:
             return PyObject
 
     if np_dtype.kind in {'U', 'S'}:
         return string
 
     if np_dtype.kind in {'M'}:
-        return DateTime
+        return Instant
 
     for _, dtype in _j_name_type_map.items():
         if np.dtype(dtype.np_type) == np_dtype and dtype.np_type != np.object_:
             return dtype
 
     return PyObject
```

## deephaven/execution_context.py

```diff
@@ -12,14 +12,15 @@
 import jpy
 
 from deephaven import DHError
 from deephaven._wrapper import JObjectWrapper
 from deephaven.jcompat import to_sequence
 
 _JExecutionContext = jpy.get_type("io.deephaven.engine.context.ExecutionContext")
+_JUpdateGraph = jpy.get_type("io.deephaven.engine.updategraph.UpdateGraph")
 
 
 class ExecutionContext(JObjectWrapper, ContextDecorator):
     """An ExecutionContext represents a specific combination of query library, query compiler, and query scope under
     which a query is evaluated.
 
     A default, systemic ExecutionContext is created when a Deephaven script session is initialized, and it is used to
@@ -32,14 +33,18 @@
     """
     j_object_type = _JExecutionContext
 
     @property
     def j_object(self) -> jpy.JType:
         return self.j_exec_ctx
 
+    @property
+    def update_graph(self) -> _JUpdateGraph:
+        return self.j_exec_ctx.getUpdateGraph()
+
     def __init__(self, j_exec_ctx):
         self.j_exec_ctx = j_exec_ctx
 
     def __enter__(self):
         self._j_safe_closable = self.j_exec_ctx.open()
         return self
 
@@ -69,14 +74,15 @@
         return ExecutionContext(j_exec_ctx=_JExecutionContext.makeExecutionContext(False))
     else:
         try:
             j_exec_ctx = (_JExecutionContext.newBuilder()
                           .captureQueryCompiler()
                           .captureQueryLibrary()
                           .captureQueryScopeVars(*freeze_vars)
+                          .captureUpdateGraph()
                           .build())
             return ExecutionContext(j_exec_ctx=j_exec_ctx)
         except Exception as e:
             raise DHError(message="failed to make a new ExecutionContext") from e
 
 
 def get_exec_ctx() -> ExecutionContext:
```

## deephaven/jcompat.py

```diff
@@ -29,28 +29,28 @@
 
 
 def j_hashmap(d: Dict = None) -> jpy.JType:
     """Creates a Java HashMap from a dict."""
     if d is None:
         return None
 
-    r = jpy.get_type("java.util.HashMap")()
+    r = jpy.get_type("java.util.HashMap")(len(d))
     for k, v in d.items():
         k = unwrap(k)
         v = unwrap(v)
         r.put(k, v)
     return r
 
 
 def j_hashset(s: Set = None) -> jpy.JType:
     """Creates a Java HashSet from a set."""
     if s is None:
         return None
 
-    r = jpy.get_type("java.util.HashSet")()
+    r = jpy.get_type("java.util.HashSet")(len(s))
     for v in s:
         r.add(unwrap(v))
     return r
 
 
 def j_properties(d: Dict = None) -> jpy.JType:
     """Creates a Java Properties from a dict."""
```

## deephaven/numpy.py

```diff
@@ -11,29 +11,29 @@
 from deephaven.dtypes import DType
 
 from deephaven import DHError, dtypes, empty_table, new_table
 from deephaven.column import Column, InputColumn
 from deephaven.table import Table
 
 _JPrimitiveArrayConversionUtility = jpy.get_type("io.deephaven.integrations.common.PrimitiveArrayConversionUtility")
-
+_JDataAccessHelpers = jpy.get_type("io.deephaven.engine.table.impl.DataAccessHelpers")
 
 def _to_column_name(name: str) -> str:
     """ Transforms the given name string into a valid table column name. """
     tmp_name = re.sub(r"\W+", " ", str(name)).strip()
     return re.sub(r"\s+", "_", tmp_name)
 
 
 def column_to_numpy_array(col_def: Column, j_array: jpy.JType) -> np.ndarray:
     """ Produces a numpy array from the given Java array and the Table column definition. """
     try:
         if col_def.data_type.is_primitive:
             np_array = np.frombuffer(j_array, col_def.data_type.np_type)
-        elif col_def.data_type == dtypes.DateTime:
-            longs = _JPrimitiveArrayConversionUtility.translateArrayDateTimeToLong(j_array)
+        elif col_def.data_type == dtypes.Instant:
+            longs = _JPrimitiveArrayConversionUtility.translateArrayInstantToLong(j_array)
             np_long_array = np.frombuffer(longs, np.int64)
             np_array = np_long_array.view(col_def.data_type.np_type)
         elif col_def.data_type == dtypes.bool_:
             bytes_ = _JPrimitiveArrayConversionUtility.translateArrayBooleanToByte(j_array)
             np_array = np.frombuffer(bytes_, col_def.data_type.np_type)
         elif col_def.data_type == dtypes.string:
             np_array = np.array([s for s in j_array], dtypes.string.np_type)
@@ -109,15 +109,15 @@
 
         col_defs = [col_def_dict[col] for col in cols]
         if len(set([col_def.data_type for col_def in col_defs])) != 1:
             raise DHError(message="columns must be of the same data type.")
 
         j_arrays = []
         for col_def in col_defs:
-            data_col = table.j_table.getColumn(col_def.name)
+            data_col = _JDataAccessHelpers.getColumn(table.j_table, col_def.name)
             j_arrays.append(data_col.getDirect())
         return _columns_to_2d_numpy_array(col_defs[0], j_arrays)
     except DHError:
         raise
     except Exception as e:
         raise DHError(e, "failed to create a numpy array from the table column.") from e
```

## deephaven/pandas.py

```diff
@@ -14,14 +14,15 @@
 from deephaven.arrow import SUPPORTED_ARROW_TYPES
 from deephaven.column import Column
 from deephaven.constants import NULL_BYTE, NULL_SHORT, NULL_INT, NULL_LONG, NULL_FLOAT, NULL_DOUBLE
 from deephaven.numpy import column_to_numpy_array, _make_input_column
 from deephaven.table import Table
 
 _JPrimitiveArrayConversionUtility = jpy.get_type("io.deephaven.integrations.common.PrimitiveArrayConversionUtility")
+_JDataAccessHelpers = jpy.get_type("io.deephaven.engine.table.impl.DataAccessHelpers")
 _is_dtype_backend_supported = pd.__version__ >= "2.0.0"
 
 
 def _column_to_series(table: Table, col_def: Column) -> pd.Series:
     """Produce a copy of the specified column as a pandas.Series object.
 
     Args:
@@ -31,15 +32,15 @@
     Returns:
         a pandas Series
 
     Raises:
         DHError
     """
     try:
-        data_col = table.j_table.getColumn(col_def.name)
+        data_col = _JDataAccessHelpers.getColumn(table.j_table, col_def.name)
         np_array = column_to_numpy_array(col_def, data_col.getDirect())
 
         return pd.Series(data=np_array, copy=False)
     except DHError:
         raise
     except Exception as e:
         raise DHError(e, message="failed to create a pandas Series for {col}") from e
@@ -217,16 +218,20 @@
         except:
             pass
 
     try:
         input_cols = []
         for col in cols:
             np_array = df.get(col).values
-            dtype = dtypes.from_np_dtype(np_array.dtype)
+            if isinstance(df.dtypes[col], pd.CategoricalDtype):
+                dtype = df.dtypes[col].categories.dtype
+            else:
+                dtype = np_array.dtype
+            dh_dtype = dtypes.from_np_dtype(dtype)
             np_array = _map_na(np_array)
-            input_cols.append(_make_input_column(col, np_array, dtype))
+            input_cols.append(_make_input_column(col, np_array, dh_dtype))
 
         return new_table(cols=input_cols)
     except DHError:
         raise
     except Exception as e:
         raise DHError(e, "failed to create a Deephaven Table from a pandas DataFrame.") from e
```

## deephaven/replay.py

```diff
@@ -1,16 +1,17 @@
 #
 # Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
 #
 
 """ This module provides support for replaying historical data. """
 
+from typing import Union
 import jpy
 
-from deephaven import dtypes, DHError
+from deephaven import dtypes, DHError, time
 from deephaven._wrapper import JObjectWrapper
 from deephaven.table import Table
 
 _JReplayer = jpy.get_type("io.deephaven.engine.table.impl.replay.Replayer")
 
 
 class TableReplayer(JObjectWrapper):
@@ -18,24 +19,31 @@
 
     Tables to be replayed are registered with the replayer.  The resulting dynamic replay tables all update in sync,
     using the same simulated clock.  Each registered table must contain a timestamp column.
     """
 
     j_object_type = _JReplayer
 
-    def __init__(self, start_time: dtypes.DateTime, end_time: dtypes.DateTime):
+    def __init__(self, start_time: Union[dtypes.Instant,str], end_time: Union[dtypes.Instant,str]):
         """Initializes the replayer.
 
         Args:
              start_time (DateTime): replay start time
              end_time (DateTime): replay end time
 
         Raises:
             DHError
         """
+
+        if isinstance(start_time, str):
+            start_time = time.parse_instant(start_time)
+
+        if isinstance(end_time, str):
+            end_time = time.parse_instant(end_time)
+
         self.start_time = start_time
         self.end_time = end_time
         try:
             self._j_replayer = _JReplayer(start_time, end_time)
         except Exception as e:
             raise DHError(e, "failed to create a replayer.") from e
```

## deephaven/table.py

```diff
@@ -23,39 +23,39 @@
 from deephaven._wrapper import JObjectWrapper
 from deephaven._wrapper import unwrap
 from deephaven.agg import Aggregation
 from deephaven.column import Column, ColumnType
 from deephaven.filters import Filter, and_, or_
 from deephaven.jcompat import j_unary_operator, j_binary_operator, j_map_to_dict, j_hashmap
 from deephaven.jcompat import to_sequence, j_array_list
-from deephaven.ugp import auto_locking_ctx
+from deephaven.update_graph import auto_locking_ctx
 from deephaven.updateby import UpdateByOperation
 
 # Table
 _J_Table = jpy.get_type("io.deephaven.engine.table.Table")
 _JLiveAttributeMap = jpy.get_type("io.deephaven.engine.table.impl.LiveAttributeMap")
 _JTableTools = jpy.get_type("io.deephaven.engine.util.TableTools")
 _JColumnName = jpy.get_type("io.deephaven.api.ColumnName")
 _JSortColumn = jpy.get_type("io.deephaven.api.SortColumn")
 _JFilter = jpy.get_type("io.deephaven.api.filter.Filter")
 _JFilterOr = jpy.get_type("io.deephaven.api.filter.FilterOr")
 _JPair = jpy.get_type("io.deephaven.api.Pair")
 _JLayoutHintBuilder = jpy.get_type("io.deephaven.engine.util.LayoutHintBuilder")
 _JSearchDisplayMode = jpy.get_type("io.deephaven.engine.util.LayoutHintBuilder$SearchDisplayModes")
 _JSnapshotWhenOptions = jpy.get_type("io.deephaven.api.snapshot.SnapshotWhenOptions")
+_JUpdateGraph = jpy.get_type("io.deephaven.engine.updategraph.UpdateGraph")
 
 # PartitionedTable
 _JPartitionedTable = jpy.get_type("io.deephaven.engine.table.PartitionedTable")
 _JPartitionedTableFactory = jpy.get_type("io.deephaven.engine.table.PartitionedTableFactory")
 _JTableDefinition = jpy.get_type("io.deephaven.engine.table.TableDefinition")
 _JPartitionedTableProxy = jpy.get_type("io.deephaven.engine.table.PartitionedTable$Proxy")
 _JJoinMatch = jpy.get_type("io.deephaven.api.JoinMatch")
 _JJoinAddition = jpy.get_type("io.deephaven.api.JoinAddition")
 _JAsOfJoinRule = jpy.get_type("io.deephaven.api.AsOfJoinRule")
-_JReverseAsOfJoinRule = jpy.get_type("io.deephaven.api.ReverseAsOfJoinRule")
 _JTableOperations = jpy.get_type("io.deephaven.api.TableOperations")
 
 # Dynamic Query Scope
 _JExecutionContext = jpy.get_type("io.deephaven.engine.context.ExecutionContext")
 _JScriptSessionQueryScope = jpy.get_type("io.deephaven.engine.util.AbstractScriptSession$ScriptSessionQueryScope")
 _JPythonScriptSession = jpy.get_type("io.deephaven.integrations.python.PythonDeephavenSession")
 
@@ -450,16 +450,16 @@
     # combine the immediate caller's globals and locals into a single dict and use it as the query scope
     caller_frame = outer_frames[i + 1].frame
     function = outer_frames[i + 1].function
     j_py_script_session = _j_py_script_session()
     if j_py_script_session and (len(outer_frames) > i + 2 or function != "<module>"):
         scope_dict = caller_frame.f_globals.copy()
         scope_dict.update(caller_frame.f_locals)
+        j_py_script_session.pushScope(scope_dict)
         try:
-            j_py_script_session.pushScope(scope_dict)
             yield
         finally:
             j_py_script_session.popScope()
     else:
         # in the __main__ module, use the default main global scope
         yield
 
@@ -504,14 +504,15 @@
     def __init__(self, j_table: jpy.JType):
         self.j_table = jpy.cast(j_table, self.j_object_type)
         if self.j_table is None:
             raise DHError("j_table type is not io.deephaven.engine.table.Table")
         self._definition = self.j_table.getDefinition()
         self._schema = None
         self._is_refreshing = None
+        self._update_graph = None
         self._is_flat = None
 
     def __repr__(self):
         default_repr = super().__repr__()
         # default_repr is in a format like so:
         # deephaven.table.Table(io.deephaven.engine.table.Table(objectRef=0x7f07e4890518))
         # We take the last two brackets off, add a few more details about the table, then add the necessary brackets back
@@ -535,14 +536,23 @@
     def is_refreshing(self) -> bool:
         """Whether this table is refreshing."""
         if self._is_refreshing is None:
             self._is_refreshing = self.j_table.isRefreshing()
         return self._is_refreshing
 
     @property
+    def update_graph(self) -> _JUpdateGraph:
+        """None if not refreshing otherwise is this table's update graph."""
+        if self.is_refreshing:
+            if self._update_graph is None:
+                self._update_graph = self.j_table.getUpdateGraph()
+            return self._update_graph
+        return None
+
+    @property
     def is_flat(self) -> bool:
         """Whether this table is guaranteed to be flat, i.e. its row set will be from 0 to number of rows - 1."""
         if self._is_flat is None:
             self._is_flat = self.j_table.isFlat()
         return self._is_flat
 
     @property
@@ -553,15 +563,15 @@
 
         self._schema = _td_to_columns(self._definition)
         return self._schema
 
     @property
     def meta_table(self) -> Table:
         """The column definitions of the table in a Table form. """
-        return Table(j_table=self.j_table.getMeta())
+        return Table(j_table=self.j_table.meta())
 
     @property
     def j_object(self) -> jpy.JType:
         return self.j_table
 
     def attributes(self) -> Dict[str, Any]:
         """Returns all the attributes defined on the table."""
@@ -1255,16 +1265,16 @@
         the keys from the right table without going over. If there is no matching key in the right table, appended row
         values are NULL.
 
         Args:
             table (Table): the right-table of the join
             on (Union[str, Sequence[str]]): the column(s) to match, can be a common name or a match condition of two
                 columns, e.g. 'col_a = col_b'. The first 'N-1' matches are exact matches.  The final match is an inexact
-                match.  The inexact match can use either '<' or '<='.  If a common name is used for the inexact match,
-                '<=' is used for the comparison.
+                match.  The inexact match can use either '>' or '>='.  If a common name is used for the inexact match,
+                '>=' is used for the comparison.
             joins (Union[str, Sequence[str]], optional): the column(s) to be added from the right table to the result
                 table, can be renaming expressions, i.e. "new_col = col"; default is None
         Returns:
             a new table
 
         Raises:
             DHError
@@ -1285,16 +1295,16 @@
         the keys from the right table without going under. If there is no matching key in the right table, appended row
         values are NULL.
 
         Args:
             table (Table): the right-table of the join
             on (Union[str, Sequence[str]]): the column(s) to match, can be a common name or a match condition of two
                 columns, e.g. 'col_a = col_b'. The first 'N-1' matches are exact matches.  The final match is an inexact
-                match.  The inexact match can use either '>' or '>='.  If a common name is used for the inexact match,
-                '>=' is used for the comparison.
+                match.  The inexact match can use either '<' or '<='.  If a common name is used for the inexact match,
+                '<=' is used for the comparison.
             joins (Union[str, Sequence[str]], optional): the column(s) to be added from the right table to the result
                 table, can be renaming expressions, i.e. "new_col = col"; default is None
 
         Returns:
             a new table
 
         Raises:
@@ -1456,15 +1466,15 @@
             with auto_locking_ctx(self):
                 return Table(j_table=self.j_table.tailBy(num_rows, *by))
         except Exception as e:
             raise DHError(e, "table tail_by operation failed.") from e
 
     def group_by(self, by: Union[str, Sequence[str]] = None) -> Table:
         """The group_by method creates a new table containing grouping columns and grouped data, column content is
-        grouped into arrays.
+        grouped into vectors.
 
         Args:
             by (Union[str, Sequence[str]], optional): the group-by column name(s), default is None
 
         Returns:
             a new table
 
@@ -2296,14 +2306,19 @@
     def table(self) -> Table:
         """The underlying partitioned table."""
         if self._table is None:
             self._table = Table(j_table=self.j_partitioned_table.table())
         return self._table
 
     @property
+    def update_graph(self) -> _JUpdateGraph:
+        """The underlying partitioned table's update graph."""
+        return self.table.update_graph
+
+    @property
     def is_refreshing(self) -> bool:
         """Whether the underlying partitioned table is refreshing."""
         if self._is_refreshing is None:
             self._is_refreshing = self.table.is_refreshing
         return self._is_refreshing
 
     @property
@@ -2551,14 +2566,19 @@
         return self.j_pt_proxy
 
     @property
     def is_refreshing(self) -> bool:
         """Whether this proxy represents a refreshing partitioned table."""
         return self.target.is_refreshing
 
+    @property
+    def update_graph(self) -> _JUpdateGraph:
+        """The underlying partitioned table proxy's update graph."""
+        return self.target.update_graph
+
     def __init__(self, j_pt_proxy):
         self.j_pt_proxy = jpy.cast(j_pt_proxy, _JPartitionedTableProxy)
         self.require_matching_keys = self.j_pt_proxy.requiresMatchingKeys()
         self.sanity_check_joins = self.j_pt_proxy.sanityChecksJoins()
         self.target = PartitionedTable(j_partitioned_table=self.j_pt_proxy.target())
 
     def head(self, num_rows: int) -> PartitionedTableProxy:
@@ -3012,16 +3032,16 @@
         In the case of the right table being another PartitionedTableProxy, the :meth:`~Table.aj` table operation
         is applied to the matching pairs of the constituent tables from both underlying partitioned tables.
 
         Args:
             table (Union[Table, PartitionedTableProxy]): the right table or PartitionedTableProxy of the join
             on (Union[str, Sequence[str]]): the column(s) to match, can be a common name or a match condition of two
                 columns, e.g. 'col_a = col_b'. The first 'N-1' matches are exact matches.  The final match is an inexact
-                match.  The inexact match can use either '<' or '<='.  If a common name is used for the inexact match,
-                '<=' is used for the comparison.
+                match.  The inexact match can use either '>' or '>='.  If a common name is used for the inexact match,
+                '>=' is used for the comparison.
             joins (Union[str, Sequence[str]], optional): the column(s) to be added from the right table to the result
                 table, can be renaming expressions, i.e. "new_col = col"; default is None
         Returns:
             a new PartitionedTableProxy
 
         Raises:
             DHError
@@ -3046,16 +3066,16 @@
         In the case of the right table being another PartitionedTableProxy, the :meth:`~Table.raj` table operation
         is applied to the matching pairs of the constituent tables from both underlying partitioned tables.
 
         Args:
             table (Union[Table, PartitionedTableProxy]): the right table or PartitionedTableProxy of the join
             on (Union[str, Sequence[str]]): the column(s) to match, can be a common name or a match condition of two
                 columns, e.g. 'col_a = col_b'. The first 'N-1' matches are exact matches.  The final match is an inexact
-                match.  The inexact match can use either '>' or '>='.  If a common name is used for the inexact match,
-                '>=' is used for the comparison.
+                match.  The inexact match can use either '<' or '<='.  If a common name is used for the inexact match,
+                '<=' is used for the comparison.
             joins (Union[str, Sequence[str]], optional): the column(s) to be added from the right table to the result
                 table, can be renaming expressions, i.e. "new_col = col"; default is None
         Returns:
             a new PartitionedTableProxy
 
         Raises:
             DHError
```

## deephaven/table_factory.py

```diff
@@ -10,15 +10,15 @@
 
 from deephaven import DHError
 from deephaven._wrapper import JObjectWrapper
 from deephaven.column import InputColumn, Column
 from deephaven.dtypes import DType
 from deephaven.jcompat import to_sequence
 from deephaven.table import Table
-from deephaven.ugp import auto_locking_ctx
+from deephaven.update_graph import auto_locking_ctx
 
 _JTableFactory = jpy.get_type("io.deephaven.engine.table.TableFactory")
 _JTableTools = jpy.get_type("io.deephaven.engine.util.TableTools")
 _JDynamicTableWriter = jpy.get_type("io.deephaven.engine.table.impl.util.DynamicTableWriter")
 _JMutableInputTable = jpy.get_type("io.deephaven.engine.util.config.MutableInputTable")
 _JAppendOnlyArrayBackedMutableTable = jpy.get_type(
     "io.deephaven.engine.table.impl.util.AppendOnlyArrayBackedMutableTable")
@@ -48,15 +48,15 @@
 
 
 def time_table(period: Union[str, int], start_time: str = None) -> Table:
     """Creates a table that adds a new row on a regular interval.
 
     Args:
         period (Union[str, int]): time interval between new row additions, can be expressed as an integer in
-            nanoseconds or a time interval string, e.g. "00:00:00.001"
+            nanoseconds or a time interval string, e.g. "PT00:00:00.001"
         start_time (str): start time for adding new rows
 
     Returns:
         a Table
 
     Raises:
         DHError
```

## deephaven/table_listener.py

```diff
@@ -10,25 +10,26 @@
 from inspect import signature
 from typing import Callable, Union, List, Generator, Dict, Optional
 
 import jpy
 import numpy
 
 from deephaven import DHError
-from deephaven import ugp
+from deephaven import update_graph
 from deephaven._wrapper import JObjectWrapper
 from deephaven.column import Column
 from deephaven.jcompat import to_sequence
 from deephaven.numpy import column_to_numpy_array
 from deephaven.table import Table
 
 _JPythonListenerAdapter = jpy.get_type("io.deephaven.integrations.python.PythonListenerAdapter")
 _JPythonReplayListenerAdapter = jpy.get_type("io.deephaven.integrations.python.PythonReplayListenerAdapter")
 _JTableUpdate = jpy.get_type("io.deephaven.engine.table.TableUpdate")
 _JTableUpdateDataReader = jpy.get_type("io.deephaven.integrations.python.PythonListenerTableUpdateDataReader")
+_JUpdateGraph = jpy.get_type("io.deephaven.engine.updategraph.UpdateGraph")
 
 
 def _col_defs(table: Table, cols: Union[str, List[str]]) -> List[Column]:
     if not cols:
         col_defs = table.columns
     else:
         cols = to_sequence(cols)
@@ -233,33 +234,37 @@
     def modified_columns(self) -> List[str]:
         """The list of modified columns in this update."""
         cols = self.j_table_update.modifiedColumnSet.dirtyColumnNames()
 
         return list(cols) if cols else []
 
 
-def _do_locked(f: Callable, lock_type="shared") -> None:
-    """Executes a function while holding the UpdateGraphProcessor (UGP) lock.  Holding the UGP lock
+def _do_locked(ug: Union[_JUpdateGraph, Table], f: Callable, lock_type="shared") -> None:
+    """Executes a function while holding the UpdateGraph (UG) lock.  Holding the UG lock
     ensures that the contents of a table will not change during a computation, but holding
     the lock also prevents table updates from happening.  The lock should be held for as little
     time as possible.
 
     Args:
-        f (Callable): callable to execute while holding the UGP lock, could be function or an object with an 'apply'
+        ug (Union[_JUpdateGraph, Table]): The Update Graph (UG) or a table-like object.
+        f (Callable): callable to execute while holding the UG lock, could be function or an object with an 'apply'
             attribute which is callable
-        lock_type (str): UGP lock type, valid values are "exclusive" and "shared".  "exclusive" allows only a single
+        lock_type (str): UG lock type, valid values are "exclusive" and "shared".  "exclusive" allows only a single
             reader or writer to hold the lock.  "shared" allows multiple readers or a single writer to hold the lock.
     Raises:
         ValueError
     """
+    if isinstance(ug, Table):
+        ug = ug.update_graph
+
     if lock_type == "exclusive":
-        with ugp.exclusive_lock():
+        with update_graph.exclusive_lock(ug):
             f()
     elif lock_type == "shared":
-        with ugp.shared_lock():
+        with update_graph.shared_lock(ug):
             f()
     else:
         raise ValueError(f"Unsupported lock type: lock_type={lock_type}")
 
 
 class TableListener(ABC):
     """An abstract table listener class that should be subclassed by any user table listener class."""
@@ -384,15 +389,15 @@
             def _start():
                 if do_replay:
                     self.listener.replay()
 
                 self.t.j_table.addUpdateListener(self.listener)
 
             if do_replay:
-                _do_locked(_start, lock_type=replay_lock)
+                _do_locked(self.t, _start, lock_type=replay_lock)
             else:
                 _start()
         except Exception as e:
             raise DHError(e, "failed to listen to the table changes.") from e
 
         self.started = True
```

## deephaven/time.py

```diff
@@ -1,849 +1,2105 @@
 #
 # Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
 #
 
 """ This module defines functions for handling Deephaven date/time data. """
 
 from __future__ import annotations
-from enum import Enum
+from typing import Union, Optional
 
 import jpy
 
 from deephaven import DHError
-from deephaven.dtypes import DateTime, Period
+from deephaven.dtypes import Instant, LocalDate, LocalTime, ZonedDateTime, Duration, Period, TimeZone, from_jtype
+from deephaven.constants import NULL_INT, NULL_LONG, NULL_DOUBLE
 
+_JDateTimeUtils = jpy.get_type("io.deephaven.time.DateTimeUtils")
+
+MICRO = 1000  #: One microsecond in nanoseconds.
+MILLI = 1000000  #: One millisecond in nanosecondsl
 SECOND = 1000000000  #: One second in nanoseconds.
 MINUTE = 60 * SECOND  #: One minute in nanoseconds.
 HOUR = 60 * MINUTE  #: One hour in nanoseconds.
-DAY = 24 * HOUR  #: One day in nanoseconds.
-WEEK = 7 * DAY  #: One week in nanoseconds.
-YEAR = 52 * WEEK  #: One year in nanoseconds.
+DAY = 24 * HOUR  #: One day in nanoseconds.  This is one hour of wall time and does not take into account calendar adjustments.
+WEEK = 7 * DAY  #: One week in nanoseconds.  This is 7 days of wall time and does not take into account calendar adjustments.
+YEAR_365 = 365 * DAY  #: One 365 day year in nanoseconds.  This is 365 days of wall time and does not take into account calendar adjustments.
+YEAR_AVG = 31556952000000000  #: One average year in nanoseconds.  This is 365.2425 days of wall time and does not take into account calendar adjustments.
 
-_JDateTimeUtils = jpy.get_type("io.deephaven.time.DateTimeUtils")
-_JTimeZone = jpy.get_type("io.deephaven.time.TimeZone")
+SECONDS_PER_NANO = 1 / SECOND  #: Number of seconds per nanosecond.
+MINUTES_PER_NANO = 1 / MINUTE  #: Number of minutes per nanosecond.
+HOURS_PER_NANO = 1 / HOUR  #: Number of hours per nanosecond.
+DAYS_PER_NANO = 1 / DAY  #: Number of days per nanosecond.
+YEARS_PER_NANO_365 = 1 / YEAR_365  #: Number of 365 day years per nanosecond.
+YEARS_PER_NANO_AVG = 1 / YEAR_AVG  #: Number of average (365.2425 day) years per nanosecond.
 
 
-class TimeZone(Enum):
-    """ A Enum for known time zones. """
-    NY = _JTimeZone.TZ_NY
-    """ America/New_York """
-    ET = _JTimeZone.TZ_ET
-    """ America/New_York """
-    MN = _JTimeZone.TZ_MN
-    """ America/Chicago """
-    CT = _JTimeZone.TZ_CT
-    """ America/Chicago """
-    MT = _JTimeZone.TZ_MT
-    """ America/Denver """
-    PT = _JTimeZone.TZ_PT
-    """ America/Los_Angeles """
-    HI = _JTimeZone.TZ_HI
-    """ Pacific/Honolulu """
-    BT = _JTimeZone.TZ_BT
-    """ America/Sao_Paulo """
-    KR = _JTimeZone.TZ_KR
-    """ Asia/Seoul """
-    HK = _JTimeZone.TZ_HK
-    """ Asia/Hong_Kong """
-    JP = _JTimeZone.TZ_JP
-    """ Asia/Tokyo """
-    AT = _JTimeZone.TZ_AT
-    """ Canada/Atlantic """
-    NF = _JTimeZone.TZ_NF
-    """ Canada/Newfoundland """
-    AL = _JTimeZone.TZ_AL
-    """ America/Anchorage """
-    IN = _JTimeZone.TZ_IN
-    """ Asia/Kolkata """
-    CE = _JTimeZone.TZ_CE
-    """ Europe/Berlin """
-    SG = _JTimeZone.TZ_SG
-    """ Asia/Singapore """
-    LON = _JTimeZone.TZ_LON
-    """ Europe/London """
-    MOS = _JTimeZone.TZ_MOS
-    """ Europe/Moscow """
-    SHG = _JTimeZone.TZ_SHG
-    """ Asia/Shanghai """
-    CH = _JTimeZone.TZ_CH
-    """ Europe/Zurich """
-    NL = _JTimeZone.TZ_NL
-    """ Europe/Amsterdam """
-    TW = _JTimeZone.TZ_TW
-    """ Asia/Taipei """
-    SYD = _JTimeZone.TZ_SYD
-    """ Australia/Sydney """
-    UTC = _JTimeZone.TZ_UTC
-    """ UTC """
+# region Clock
 
-    @staticmethod
-    def get_default_timezone() -> TimeZone:
-        """ Gets the default time zone. """
-        return TimeZone(_JTimeZone.getTzDefault())
 
-    @staticmethod
-    def set_default_timezone(tz: TimeZone) -> None:
-        """ Sets the default time zone.
+def now(system: bool = False, resolution: str = 'ns') -> Instant:
+    """ Provides the current datetime according to a clock.
 
-        Args:
-             tz (TimeZone): the TimeZone to use as default
-        """
-        _JTimeZone.setTzDefault(tz.value)
+    Args:
+        system (bool): True to use the system clock; False to use the default clock.  Under most circumstances,
+            the default clock will return the current system time, but during replay simulations, the default
+            clock can return the replay time.
+
+        resolution (str): The resolution of the returned time.  The default 'ns' will return nanosecond resolution times
+            if possible. 'ms' will return millisecond resolution times.
+
+    Returns:
+        Instant
 
+    Raises:
+        DHError
+    """
+    try:
+        if resolution == "ns":
+            if system:
+                return _JDateTimeUtils.nowSystem()
+            else:
+                return _JDateTimeUtils.now()
+        elif resolution == "ms":
+            if system:
+                return _JDateTimeUtils.nowSystemMillisResolution()
+            else:
+                return _JDateTimeUtils.nowMillisResolution()
+        else:
+            raise ValueError("Unsupported time resolution: " + resolution)
+    except Exception as e:
+        raise DHError(e) from e
 
-def to_datetime(s: str, quiet: bool = False) -> DateTime:
-    """ Converts a datetime string to a DateTime object.
 
-    Supports ISO 8601 format and others.
+def today(tz: TimeZone) -> str:
+    """ Provides the current date string according to the current clock.
+    Under most circumstances, this method will return the date according to current system time,
+    but during replay simulations, this method can return the date according to replay time.
 
     Args:
-        s (str): in the form of ISO 8601 or "yyyy-MM-ddThh:mm:ss[.SSSSSSSSS] TZ"
-        quiet (bool): when True, if the datetime string can't be parsed, this function returns None, otherwise
-            it raises an exception. The default is False
+        tz (TimeZone): Time zone to use when determining the date.
 
     Returns:
-        a DateTime
+        Date string
 
     Raises:
         DHError
     """
-    if quiet:
-        return _JDateTimeUtils.convertDateTimeQuiet(s)
+    try:
+        return _JDateTimeUtils.today(tz)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+# endregion
+
+# region Time Zone
+
+
+def time_zone(tz: Optional[str]) -> TimeZone:
+    """ Gets the time zone for a time zone name.
+
+    Args:
+        tz (Optional[str]): Time zone name.  If None is provided, the system default time zone is returned.
+
+    Returns:
+        TimeZone
 
+    Raises:
+        DHError
+    """
     try:
-        return _JDateTimeUtils.convertDateTime(s)
+        if tz is None:
+            return _JDateTimeUtils.timeZone()
+        else:
+            return _JDateTimeUtils.timeZone(tz)
     except Exception as e:
         raise DHError(e) from e
 
 
-def to_period(s: str, quiet: bool = False) -> Period:
-    """ Converts a period string into a Period object.
+def time_zone_alias_add(alias: str, tz: str) -> None:
+    """ Adds a new time zone alias.
 
     Args:
-        s (str): a string in the form of nYnMnWnDTnHnMnS, with n being numeric values, e.g. 1W for one week, T1M for
-            one minute, 1WT1H for one week plus one hour
-        quiet (bool): when True, if the period string can't be parsed, this function returns None, otherwise
-            it raises an exception. The default is False
+        alias (str): Alias name.
+        tz (str): Time zone name.
 
     Returns:
-        a Period
+        None
 
     Raises:
         DHError
     """
-    if quiet:
-        return _JDateTimeUtils.convertPeriodQuiet(s)
+    try:
+        return _JDateTimeUtils.timeZoneAliasAdd(alias, tz)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def time_zone_alias_rm(alias: str) -> bool:
+    """ Removes a time zone alias.
+
+    Args:
+        alias (str): Alias name.
+
+    Returns:
+        True if the alias was present; False if the alias was not present.
 
+    Raises:
+        DHError
+    """
     try:
-        return _JDateTimeUtils.convertPeriod(s)
+        return _JDateTimeUtils.timeZoneAliasRm(alias)
     except Exception as e:
         raise DHError(e) from e
 
 
-def to_nanos(s, quiet: bool = False) -> int:
-    """ Converts a time string to nanoseconds.
+# endregion
+
+# region Conversions: Time Units
+
+
+def micros_to_nanos(micros: int) -> int:
+    """ Converts microseconds to nanoseconds.
 
     Args:
-        s (str): in the format of: hh:mm:ss[.SSSSSSSSS]
-        quiet (bool): to return None or raise an exception if the string can't be parsed, default is False
+        micros (int): Microseconds to convert.
 
     Returns:
-        int
+        NULL_LONG if the input is NULL_LONG; otherwise the input microseconds converted to nanoseconds.
 
     Raises:
         DHError
     """
-    if quiet:
-        return _JDateTimeUtils.convertTimeQuiet(s)
+    try:
+        return _JDateTimeUtils.microsToNanos(micros)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def millis_to_nanos(millis: int) -> int:
+    """ Converts milliseconds to nanoseconds.
 
+    Args:
+        millis (int): Milliseconds to convert.
+
+    Returns:
+        NULL_LONG if the input is NULL_LONG; otherwise the input milliseconds converted to nanoseconds.
+
+    Raises:
+        DHError
+    """
     try:
-        return _JDateTimeUtils.convertTime(s)
+        return _JDateTimeUtils.millisToNanos(millis)
     except Exception as e:
         raise DHError(e) from e
 
 
-def now() -> DateTime:
-    """ Provides the current datetime.
+def seconds_to_nanos(seconds: int) -> int:
+    """ Converts seconds to nanoseconds.
+
+    Args:
+        seconds (int): Seconds to convert.
 
     Returns:
-        DateTime
+        NULL_LONG if the input is NULL_LONG; otherwise the input seconds converted to nanoseconds.
 
     Raises:
         DHError
     """
     try:
-        return _JDateTimeUtils.currentTime()
+        return _JDateTimeUtils.secondsToNanos(seconds)
     except Exception as e:
         raise DHError(e) from e
 
 
-def datetime_at_midnight(dt: DateTime, tz: TimeZone) -> DateTime:
-    """ Returns a DateTime for the requested DateTime at midnight in the specified time zone.
+def nanos_to_micros(nanos: int) -> int:
+    """ Converts nanoseconds to microseconds.
 
     Args:
-        dt (DateTime): the DateTime for which the new value at midnight should be calculated
-        tz (TimeZone): the TimeZone to use when interpreting the DateTime
+        nanos (int): nanoseconds to convert.
 
     Returns:
-        DateTime
+        NULL_LONG if the input is NULL_LONG; otherwise the input nanoseconds converted to microseconds, rounded down.
 
     Raises:
         DHError
     """
     try:
+        return _JDateTimeUtils.nanosToMicros(nanos)
+    except Exception as e:
+        raise DHError(e) from e
 
-        return _JDateTimeUtils.dateAtMidnight(dt, tz.value)
+
+def millis_to_micros(millis: int) -> int:
+    """ Converts milliseconds to microseconds.
+
+    Args:
+        millis (int): milliseconds to convert.
+
+    Returns:
+        NULL_LONG if the input is NULL_LONG; otherwise the input milliseconds converted to microseconds.
+
+    Raises:
+        DHError
+    """
+    try:
+        return _JDateTimeUtils.millisToMicros(millis)
     except Exception as e:
         raise DHError(e) from e
 
 
-def day_of_month(dt: DateTime, tz: TimeZone) -> int:
-    """ Returns an 1-based int value of the day of the month for a DateTime and specified time zone.
+def seconds_to_micros(seconds: int) -> int:
+    """ Converts seconds to microseconds.
 
     Args:
-        dt (DateTime): the DateTime for which to find the day of the month
-        tz (TimeZone): the TimeZone to use when interpreting the DateTime
+        seconds (int): Seconds to convert.
 
     Returns:
-        int: NULL_INT if dt is None
+        NULL_LONG if the input is NULL_LONG; otherwise the input seconds converted to microseconds.
 
     Raises:
         DHError
     """
     try:
-        return _JDateTimeUtils.dayOfMonth(dt, tz.value)
+        return _JDateTimeUtils.secondsToMicros(seconds)
     except Exception as e:
         raise DHError(e) from e
 
 
-def day_of_week(dt: DateTime, tz: TimeZone) -> int:
-    """ Returns an 1-based int value of the day of the week for a DateTime in the specified time zone, with 1 being
-     Monday and 7 being Sunday.
+def nanos_to_millis(nanos: int) -> int:
+    """ Converts nanoseconds to milliseconds.
 
     Args:
-        dt (DateTime): the DateTime for which to find the day of the week.
-        tz (TimeZone): the TimeZone to use when interpreting the DateTime.
+        nanos (int): Nanoseconds to convert.
 
     Returns:
-        int: NULL_INT if dt is None
+        NULL_LONG if the input is NULL_LONG; otherwise the input nanoseconds converted to milliseconds, rounded down.
 
     Raises:
         DHError
     """
     try:
-        return _JDateTimeUtils.dayOfWeek(dt, tz.value)
+        return _JDateTimeUtils.nanosToMillis(nanos)
     except Exception as e:
         raise DHError(e) from e
 
 
-def day_of_year(dt: DateTime, tz: TimeZone) -> int:
-    """ Returns an 1-based int value of the day of the year (Julian date) for a DateTime in the specified time zone.
+def micros_to_millis(micros: int) -> int:
+    """ Converts microseconds to milliseconds.
 
     Args:
-        dt (DateTime): the DateTime for which to find the day of the year
-        tz (TimeZone): the TimeZone to use when interpreting the DateTime
+        micros (int): Microseconds to convert.
 
     Returns:
-        int: NULL_INT if dt is None
+        NULL_LONG if the input is NULL_LONG; otherwise the input microseconds converted to milliseconds, rounded down.
 
     Raises:
         DHError
     """
     try:
-        return _JDateTimeUtils.dayOfYear(dt, tz.value)
+        return _JDateTimeUtils.microsToMillis(micros)
     except Exception as e:
         raise DHError(e) from e
 
 
-def diff_nanos(dt1: DateTime, dt2: DateTime) -> int:
-    """ Returns the difference in nanoseconds between two DateTime values.
+def seconds_to_millis(seconds: int) -> int:
+    """ Converts seconds to milliseconds.
 
     Args:
-        dt1 (DateTime): the 1st DateTime
-        dt2 (DateTime): the 2nd DateTime
+        seconds (int): Seconds to convert.
 
     Returns:
-        int: NULL_LONG if either dt1 or dt2 is None
+        NULL_LONG if the input is NULL_LONG; otherwise the input seconds converted to milliseconds.
 
     Raises:
         DHError
     """
     try:
-        return _JDateTimeUtils.diffNanos(dt1, dt2)
+        return _JDateTimeUtils.secondsToMillis(seconds)
     except Exception as e:
         raise DHError(e) from e
 
 
-def format_datetime(dt: DateTime, tz: TimeZone) -> str:
-    """ Returns a string DateTime representation formatted as "yyyy-MM-ddThh:mm:ss.SSSSSSSSS TZ".
+def nanos_to_seconds(nanos: int) -> int:
+    """ Converts nanoseconds to seconds.
 
     Args:
-        dt (DateTime): the DateTime to format as a string
-        tz (TimeZone): the TimeZone to use when interpreting the DateTime
+        nanos (int): Nanoseconds to convert.
 
     Returns:
-        str
+        NULL_LONG if the input is NULL_LONG; otherwise the input nanoseconds converted to seconds.
 
     Raises:
         DHError
     """
     try:
-        return _JDateTimeUtils.format(dt, tz.value)
+        return _JDateTimeUtils.nanosToSeconds(nanos)
     except Exception as e:
         raise DHError(e) from e
 
 
-def format_nanos(ns: int) -> str:
-    """ Returns a string DateTime representation formatted as "yyyy-MM-ddThh:mm:ss.SSSSSSSSS".
+def micros_to_seconds(micros: int) -> int:
+    """ Converts microseconds to seconds.
 
     Args:
-        ns (int): the number of nanoseconds
+        micros (int): Microseconds to convert.
 
     Returns:
-        str
+        NULL_LONG if the input is NULL_LONG; otherwise the input microseconds converted to seconds.
 
     Raises:
         DHError
     """
     try:
-        return _JDateTimeUtils.format(ns)
+        return _JDateTimeUtils.microsToSeconds(micros)
     except Exception as e:
         raise DHError(e) from e
 
 
-def format_date(dt: DateTime, tz: TimeZone) -> str:
-    """ Returns a string date representation of a DateTime interpreted for a specified time zone formatted as
-    "yyy-MM-dd".
+def millis_to_seconds(millis: int) -> int:
+    """ Converts milliseconds to seconds.
 
     Args:
-        dt (DateTime): the DateTime to format
-        tz (TimeZone): the TimeZone to use when interpreting the DateTime
+        millis (int): Milliseconds to convert.
 
     Returns:
-        str
+        NULL_LONG if the input is NULL_LONG; otherwise the input milliseconds converted to seconds.
 
     Raises:
         DHError
     """
     try:
-        return _JDateTimeUtils.formatDate(dt, tz.value)
+        return _JDateTimeUtils.millisToSeconds(millis)
     except Exception as e:
         raise DHError(e) from e
 
 
-def hour_of_day(dt: DateTime, tz: TimeZone) -> int:
-    """ Returns the hour of the day for a DateTime in the specified time zone. The hour is on a 24 hour clock (0 - 23).
+# endregion
+
+# region Conversions: Date Time Types
+
+def to_instant(dt: ZonedDateTime) -> Instant:
+    """ Converts a date time to an Instant.
 
     Args:
-        dt (DateTime): the DateTime for which to find the hour of the day
-        tz (TimeZone): the TimeZone to use when interpreting the DateTime
+        dt (ZonedDateTime): Date time to convert.
 
     Returns:
-        int: NULL_INT if dt is None
+        Instant or None if dt is None.
 
     Raises:
         DHError
     """
+    if not dt:
+        return None
+
     try:
-        return _JDateTimeUtils.hourOfDay(dt, tz.value)
+        return _JDateTimeUtils.toInstant(dt)
     except Exception as e:
         raise DHError(e) from e
 
 
-def is_after(dt1: DateTime, dt2: DateTime) -> bool:
-    """ Evaluates whether one DateTime value is later than a second DateTime value.
+def to_zdt(dt: Instant, tz: TimeZone) -> ZonedDateTime:
+    """ Converts a date time to a ZonedDateTime.
 
     Args:
-        dt1 (DateTime): the 1st DateTime
-        dt2 (DateTime): the 2nd DateTime
+        dt (Instant): Date time to convert.
+        tz (TimeZone): Time zone.
 
     Returns:
-        bool
+        ZonedDateTime or None if any input is None.
 
     Raises:
         DHError
     """
+    if not dt or not tz:
+        return None
+
     try:
-        dt1 = dt1 if dt1 else None
-        dt2 = dt2 if dt2 else None
-        return _JDateTimeUtils.isAfter(dt1, dt2)
+        return _JDateTimeUtils.toZonedDateTime(dt, tz)
     except Exception as e:
         raise DHError(e) from e
 
 
-def is_before(dt1: DateTime, dt2: DateTime) -> bool:
-    """ Evaluates whether one DateTime value is before a second DateTime value.
+def make_instant(date: LocalDate, time: LocalTime, tz: TimeZone) -> Instant:
+    """ Makes an Instant.
 
     Args:
-        dt1 (DateTime): the 1st DateTime
-        dt2 (DateTime): the 2nd DateTime
+        date (LocalDate): Local date.
+        time (LocalTime): Local time.
+        tz (TimeZone): Time zone.
 
     Returns:
-        bool
+        Instant or None if any input is None.
 
     Raises:
         DHError
     """
+    if not date or not time or not tz:
+        return None
+
     try:
-        return _JDateTimeUtils.isBefore(dt1, dt2)
+        return _JDateTimeUtils.toInstant(date, time, tz)
     except Exception as e:
         raise DHError(e) from e
 
 
-def lower_bin(dt: DateTime, interval: int, offset: int = 0) -> DateTime:
-    """ Returns a DateTime value, which is at the starting (lower) end of a time range defined by the interval
-     nanoseconds. For example, a 5*MINUTE intervalNanos value would return the DateTime value for the start of the
-     five minute window that contains the input date time.
+def make_zdt(date: LocalDate, time: LocalTime, tz: TimeZone) -> ZonedDateTime:
+    """ Makes a ZonedDateTime.
 
     Args:
-        dt (DateTime): the DateTime for which to evaluate the start of the containing window
-        interval (int): the size of the window in nanoseconds
-        offset (int): the window start offset in nanoseconds. For example, a value of MINUTE would offset all windows by
-              one minute. Default is 0
+        date (LocalDate): Local date.
+        time (LocalTime): Local time.
+        tz (TimeZone): Time zone.
 
     Returns:
-        DateTime
+        ZonedDateTime or None if any input is None.
 
     Raises:
         DHError
     """
+    if not date or not time or not tz:
+        return None
+
     try:
-        return _JDateTimeUtils.lowerBin(dt, interval, offset)
+        return _JDateTimeUtils.toZonedDateTime(date, time, tz)
     except Exception as e:
         raise DHError(e) from e
 
 
-def millis(dt: DateTime) -> int:
-    """ Returns milliseconds since Epoch for a DateTime value.
+def to_local_date(dt: Union[Instant, ZonedDateTime], tz: TimeZone) -> LocalDate:
+    """ Converts a date time to a LocalDate.
 
     Args:
-        dt (DateTime): the DateTime for which the milliseconds offset should be returned
+        dt (Instant): Date time to convert.
+        tz (TimeZone): Time zone.
 
     Returns:
-        int: NULL_LONG if dt is None
+        LocalDate or None if any input is None.
 
     Raises:
         DHError
     """
     try:
-        return _JDateTimeUtils.millis(dt)
+        if not dt or not tz:
+            return None
+
+        if from_jtype(dt.getClass()) == ZonedDateTime:
+            dt = to_instant(dt)
+
+        return _JDateTimeUtils.toLocalDate(dt, tz)
     except Exception as e:
         raise DHError(e) from e
 
 
-def millis_of_day(dt: DateTime, tz: TimeZone) -> int:
-    """ Returns the number of milliseconds since midnight for a DateTime in the specified time zone.
+def to_local_time(dt: Union[Instant, ZonedDateTime], tz: TimeZone) -> LocalTime:
+    """ Converts a date time to a LocalTime.
 
     Args:
-        dt (DateTime): the DateTime for which to find the milliseconds since midnight
-        tz (TimeZone): the TimeZone to use when interpreting the DateTime
+        dt (Instant): Date time to convert.
+        tz (TimeZone): Time zone.
 
     Returns:
-        int: NULL_INT if dt is None
+        LocalTime or None if any input is None.
 
     Raises:
         DHError
     """
     try:
-        return _JDateTimeUtils.millisOfDay(dt, tz.value)
+        if not dt or not tz:
+            return None
+
+        if from_jtype(dt.getClass()) == ZonedDateTime:
+            dt = to_instant(dt)
+
+        return _JDateTimeUtils.toLocalTime(dt, tz)
     except Exception as e:
         raise DHError(e) from e
 
 
-def millis_of_second(dt: DateTime, tz: TimeZone) -> int:
-    """ Returns the number of milliseconds since the top of the second for a DateTime in the specified time zone.
+# endregion
+
+# region Conversions: Epoch
+
+def epoch_nanos(dt: Union[Instant, ZonedDateTime]) -> int:
+    """ Returns nanoseconds from the Epoch for a date time value.
 
     Args:
-        dt (DateTime): the DateTime for which to find the milliseconds
-        tz (TimeZone): the TimeZone to use when interpreting the DateTime
+        dt (Union[Instant,ZonedDateTime]): Date time.
 
     Returns:
-        int: NULL_INT if dt is None
+        nanoseconds since Epoch, or a NULL_LONG value if the date time is null.
 
     Raises:
         DHError
     """
+    if not dt:
+        return NULL_LONG
+
     try:
-        return _JDateTimeUtils.millisOfSecond(dt, tz.value)
+        return _JDateTimeUtils.epochNanos(dt)
     except Exception as e:
         raise DHError(e) from e
 
 
-def millis_to_nanos(ms: int) -> int:
-    """ Converts milliseconds to nanoseconds.
+def epoch_micros(dt: Union[Instant, ZonedDateTime]) -> int:
+    """ Returns microseconds from the Epoch for a date time value.
 
     Args:
-        ms (int): the milliseconds value to convert
+        dt (Union[Instant,ZonedDateTime]): Date time.
 
     Returns:
-        int: NULL_LONG if ms is NULL_LONG
+        microseconds since Epoch, or a NULL_LONG value if the date time is null.
 
     Raises:
         DHError
     """
+    if not dt:
+        return NULL_LONG
+
     try:
-        return _JDateTimeUtils.millisToNanos(ms)
+        return _JDateTimeUtils.epochMicros(dt)
     except Exception as e:
         raise DHError(e) from e
 
 
-def millis_to_datetime(ms: int) -> DateTime:
-    """ Converts a value of milliseconds from Epoch in the UTC time zone to a DateTime.
+def epoch_millis(dt: Union[Instant, ZonedDateTime]) -> int:
+    """ Returns milliseconds from the Epoch for a date time value.
 
     Args:
-        ms (int): the milliseconds value to convert
+        dt (Union[Instant,ZonedDateTime]): Date time.
 
-    returns:
-        DateTime
+    Returns:
+        milliseconds since Epoch, or a NULL_LONG value if the date time is null.
 
     Raises:
         DHError
     """
+    if not dt:
+        return NULL_LONG
+
     try:
-        return _JDateTimeUtils.millisToTime(ms)
+        return _JDateTimeUtils.epochMillis(dt)
     except Exception as e:
         raise DHError(e) from e
 
 
-def minus(dt1: DateTime, dt2: DateTime) -> int:
-    """ Subtracts one time from another, returns the difference in nanos.
+def epoch_seconds(dt: Union[Instant, ZonedDateTime]) -> int:
+    """ Returns seconds from the Epoch for a date time value.
 
     Args:
-        dt1 (DateTime): the 1st DateTime
-        dt2 (DateTiem): the 2nd DateTime
+        dt (Union[Instant,ZonedDateTime]): Date time.
 
     Returns:
-        int: NULL_LONG if either dt1 or dt2 is None
+        seconds since Epoch, or a NULL_LONG value if the date time is null.
 
     Raises:
         DHError
     """
+    if not dt:
+        return NULL_LONG
+
     try:
-        return _JDateTimeUtils.minus(dt1, dt2)
+        return _JDateTimeUtils.epochSeconds(dt)
     except Exception as e:
         raise DHError(e) from e
 
 
-def minus_nanos(dt: DateTime, ns: int) -> DateTime:
-    """ Subtracts nanoseconds from a DateTime.
+def epoch_nanos_to_instant(nanos: int) -> Instant:
+    """ Converts nanoseconds from the Epoch to an Instant.
 
     Args:
-        dt (DateTime): the starting DateTime value
-        ns (int): the number of nanoseconds to subtract from dateTime
+        nanos (int): Nanoseconds since Epoch.
 
     Returns:
-        DateTime
+        Instant or None if the input is NULL_LONG.
 
     Raises:
         DHError
     """
     try:
-        return _JDateTimeUtils.minus(dt, ns)
+        return _JDateTimeUtils.epochNanosToInstant(nanos)
     except Exception as e:
         raise DHError(e) from e
 
 
-def minus_period(dt: DateTime, period) -> DateTime:
-    """ Subtracts a period from a DateTime.
+def epoch_micros_to_instant(micros: int) -> Instant:
+    """ Converts microseconds from the Epoch to an Instant.
 
     Args:
-        dt (DateTime): the starting DateTime value
-        period (Period): the Period to subtract from dateTime
+        micros (int): Microseconds since Epoch.
 
     Returns:
-        DateTime
+        Instant or None if the input is NULL_LONG.
 
     Raises:
         DHError
     """
     try:
-        return _JDateTimeUtils.minus(dt, period)
+        return _JDateTimeUtils.epochMicrosToInstant(micros)
     except Exception as e:
         raise DHError(e) from e
 
 
-def minute_of_day(dt: DateTime, tz: TimeZone) -> int:
-    """ Returns the number of minutes since midnight for a DateTime in the specified time zone.
+def epoch_millis_to_instant(millis: int) -> Instant:
+    """ Converts milliseconds from the Epoch to an Instant.
 
     Args:
-        dt (DateTime): the DateTime for which to find the minutes
-        tz (TimeZone): the TimeZone to use when interpreting the DateTime
+        millis (int): Milliseconds since Epoch.
 
     Returns:
-        int: NULL_INT if dt is None
+        Instant or None if the input is NULL_LONG.
 
     Raises:
         DHError
     """
     try:
-        return _JDateTimeUtils.minuteOfDay(dt, tz.value)
+        return _JDateTimeUtils.epochMillisToInstant(millis)
     except Exception as e:
         raise DHError(e) from e
 
 
-def minute_of_hour(dt: DateTime, tz: TimeZone) -> int:
-    """ Returns the number of minutes since the top of the hour for a DateTime in the specified time zone.
+def epoch_seconds_to_instant(seconds: int) -> Instant:
+    """ Converts seconds from the Epoch to an Instant.
 
     Args:
-        dt (DateTime): the DateTime for which to find the minutes
-        tz (TimeZone): the TimeZone to use when interpreting the DateTime
+        seconds (int): Seconds since Epoch.
 
     Returns:
-        int: NULL_INT if dt is None
+        Instant or None if the input is NULL_LONG.
 
     Raises:
         DHError
     """
     try:
+        return _JDateTimeUtils.epochSecondsToInstant(seconds)
+    except Exception as e:
+        raise DHError(e) from e
 
-        return _JDateTimeUtils.minuteOfHour(dt, tz.value)
+
+def epoch_nanos_to_zdt(nanos: int, tz: TimeZone) -> ZonedDateTime:
+    """ Converts nanoseconds from the Epoch to a ZonedDateTime.
+
+    Args:
+        nanos (int): Nanoseconds since Epoch.
+        tz (TimeZone): Time zone.
+
+    Returns:
+        ZonedDateTime or None if the input is NULL_LONG or None.
+
+    Raises:
+        DHError
+    """
+    try:
+        return _JDateTimeUtils.epochNanosToZonedDateTime(nanos, tz)
     except Exception as e:
         raise DHError(e) from e
 
 
-def month_of_year(dt: DateTime, tz: TimeZone) -> int:
-    """ Returns an 1-based int value for the month of a DateTime in the specified time zone. January is 1,
-    and December is 12.
+def epoch_micros_to_zdt(micros: int, tz: TimeZone) -> ZonedDateTime:
+    """ Converts microseconds from the Epoch to a ZonedDateTime.
 
     Args:
-        dt (DateTime): the DateTime for which to find the month
-        tz (TimeZone): the TimeZone to use when interpreting the DateTime
+        micros (int): Microseconds since Epoch.
+        tz (TimeZone): Time zone.
 
     Returns:
-        int: NULL_INT if dt is None
+        ZonedDateTime or None if the input is NULL_LONG or None.
 
     Raises:
         DHError
     """
     try:
-        return _JDateTimeUtils.monthOfYear(dt, tz.value)
+        return _JDateTimeUtils.epochMicrosToZonedDateTime(micros, tz)
     except Exception as e:
         raise DHError(e) from e
 
 
-def nanos(dt: DateTime) -> int:
-    """ Returns nanoseconds since Epoch for a DateTime value.
+def epoch_millis_to_zdt(millis: int, tz: TimeZone) -> ZonedDateTime:
+    """ Converts milliseconds from the Epoch to a ZonedDateTime.
 
     Args:
-        dt (DateTime): the DateTime for which the nanoseconds offset should be returned
+        millis (int): Milliseconds since Epoch.
+        tz (TimeZone): Time zone.
 
     Returns:
-        int: NULL_LONG if dt is None
+        ZonedDateTime or None if the input is NULL_LONG or None.
 
     Raises:
         DHError
     """
     try:
+        return _JDateTimeUtils.epochMillisToZonedDateTime(millis, tz)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def epoch_seconds_to_zdt(seconds: int, tz: TimeZone) -> ZonedDateTime:
+    """ Converts seconds from the Epoch to a ZonedDateTime.
+
+    Args:
+        seconds (int): Seconds since Epoch.
+        tz (TimeZone): Time zone.
+
+    Returns:
+        ZonedDateTime or None if the input is NULL_LONG or None.
 
-        return _JDateTimeUtils.nanos(dt)
+    Raises:
+        DHError
+    """
+    try:
+        return _JDateTimeUtils.epochSecondsToZonedDateTime(seconds, tz)
     except Exception as e:
         raise DHError(e) from e
 
 
-def nanos_of_day(dt: DateTime, tz: TimeZone) -> int:
-    """ Returns the number of nanoseconds since midnight for a DateTime in the specified time zone.
+def epoch_auto_to_epoch_nanos(epoch_offset: int) -> int:
+    """ Converts an offset from the Epoch to a nanoseconds from the Epoch.
+    The offset can be in milliseconds, microseconds, or nanoseconds.
+    Expected date ranges are used to infer the units for the offset.
 
     Args:
-        dt (DateTime): the DateTime for which to find the nanoseconds since midnight
-        tz (TimeZone): the TimeZone to use when interpreting the DateTime
+        epoch_offset (int): Time offset from the Epoch.
 
     Returns:
-        int: NULL_LONG if dt is None
+        the input offset from the Epoch converted to nanoseconds from the Epoch, or NULL_LONG if the input is NULL_LONG.
 
     Raises:
         DHError
     """
     try:
-        return _JDateTimeUtils.nanosOfDay(dt, tz.value)
+        return _JDateTimeUtils.epochAutoToEpochNanos(epoch_offset)
     except Exception as e:
         raise DHError(e) from e
 
 
-def nanos_of_second(dt: DateTime, tz: TimeZone) -> int:
-    """ Returns the number of nanoseconds since the top of the second for a DateTime in the specified time zone.
+def epoch_auto_to_instant(epoch_offset: int) -> Instant:
+    """ Converts an offset from the Epoch to an Instant.
+    The offset can be in milliseconds, microseconds, or nanoseconds.
+    Expected date ranges are used to infer the units for the offset.
 
     Args:
-        dt (DateTime): the DateTime for which to find the nanoseconds
-        tz (TimeZone): the TimeZone to use when interpreting the DateTime
+        epoch_offset (int): Time offset from the Epoch.
 
     Returns:
-        int: NULL_LONG if dt is None
+        Instant or None if the input is NULL_LONG
 
     Raises:
         DHError
     """
     try:
+        return _JDateTimeUtils.epochAutoToInstant(epoch_offset)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def epoch_auto_to_zdt(epoch_offset: int, tz: TimeZone) -> ZonedDateTime:
+    """ Converts an offset from the Epoch to a ZonedDateTime.
+    The offset can be in milliseconds, microseconds, or nanoseconds.
+    Expected date ranges are used to infer the units for the offset.
+
+    Args:
+        epoch_offset (int): Time offset from the Epoch.
+        tz (TimeZone): Time zone.
+
+    Returns:
+        ZonedDateTime or None if the input is NULL_LONG
 
-        return _JDateTimeUtils.nanosOfSecond(dt, tz.value)
+    Raises:
+        DHError
+    """
+    try:
+        return _JDateTimeUtils.epochAutoToZonedDateTime(epoch_offset, tz)
     except Exception as e:
         raise DHError(e) from e
 
 
-def nanos_to_millis(ns: int) -> int:
-    """ Converts nanoseconds to milliseconds.
+# endregion
+
+# region Conversions: Excel
+
+def to_excel_time(dt: Union[Instant, ZonedDateTime], tz: TimeZone) -> float:
+    """ Converts a date time to an Excel time represented as a double.
 
     Args:
-        ns (int): the value of nanoseconds to convert
+        dt (Instant): Date time.
+        tz (TimeZone): Time zone.
 
     Returns:
-        int: NULL_LONG if ns is NULL_LONG
+        Excel time as a double or NULL_DOUBLE if any input is None
 
     Raises:
         DHError
     """
+    if not dt or not tz:
+        return NULL_DOUBLE
+
     try:
-        return _JDateTimeUtils.nanosToMillis(ns)
+        if not dt or not tz:
+            return NULL_DOUBLE
+
+        if from_jtype(dt.getClass()) == ZonedDateTime:
+            dt = to_instant(dt)
+
+        return _JDateTimeUtils.toExcelTime(dt, tz)
     except Exception as e:
         raise DHError(e) from e
 
 
-def nanos_to_datetime(ns: int) -> DateTime:
-    """ Converts a value of nanoseconds from Epoch to a DateTime.
+def excel_to_instant(excel: float, tz: TimeZone) -> Instant:
+    """ Converts an Excel time represented as a double to an Instant.
 
     Args:
-        ns (long): the long nanoseconds since Epoch value to convert
+        excel (float): Excel time.
+        tz (TimeZone): Time zone.
 
     Returns:
-        DateTime
+        Instant or None if any input is None or NULL_DOUBLE.
+
+    Raises:
+        DHError
+    """
+    try:
+        return _JDateTimeUtils.excelToInstant(excel, tz)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def excel_to_zdt(excel: float, tz: TimeZone) -> ZonedDateTime:
+    """ Converts an Excel time represented as a double to a ZonedDateTime.
+
+    Args:
+        excel (float): Excel time.
+        tz (TimeZone): Time zone.
+
+    Returns:
+        ZonedDateTime or None if any input is None or NULL_DOUBLE.
+
+    Raises:
+        DHError
     """
     try:
-        return _JDateTimeUtils.nanosToTime(ns)
+        return _JDateTimeUtils.excelToZonedDateTime(excel, tz)
     except Exception as e:
         raise DHError(e) from e
 
 
-def plus_period(dt: DateTime, period: Period) -> DateTime:
-    """ Adds a period to a DateTime.
+# endregion
+
+# region Arithmetic
+
+def plus_period(dt: Union[Instant, ZonedDateTime], period: Union[int, Duration, Period]) -> \
+        Union[Instant, ZonedDateTime]:
+    """ Adds a time period to a date time.
 
     Args:
-        dt (DateTime): the starting DateTime value
-        period (Period): the Period to add to the DateTime
+        dt (Union[Instant,ZonedDateTime]): Date time.
+        period (Union[int, Duration, Period]): Time period to add.  Integer inputs are nanoseconds.
 
     Returns:
-        DateTime: None if either dt or period is None
+        Date time, or None if any input is None or NULL_LONG.
 
     Raises:
         DHError
     """
+    if not dt or not period:
+        return None
+
     try:
         return _JDateTimeUtils.plus(dt, period)
     except Exception as e:
         raise DHError(e) from e
 
 
-def plus_nanos(dt: DateTime, ns: int) -> DateTime:
-    """ Adds nanoseconds to a DateTime.
+def minus_period(dt: Union[Instant, ZonedDateTime], period: Union[int, Duration, Period]) -> \
+        Union[Instant, ZonedDateTime]:
+    """ Subtracts a time period from a date time.
 
     Args:
-        dt (DateTime): the starting DateTime value
-        ns (int): the number of nanoseconds to add to DateTime
+        dt (Union[Instant,ZonedDateTime]): Date time.
+        period (Union[int, Duration, Period]): Time period to subtract.  Integer inputs are nanoseconds.
 
     Returns:
-        DateTime: None if dt is None or ns is NULL_LONG
+        Date time, or None if any input is None or NULL_LONG.
 
     Raises:
         DHError
     """
+    if not dt or not period:
+        return None
+
     try:
-        return _JDateTimeUtils.plus(dt, ns)
+        return _JDateTimeUtils.minus(dt, period)
     except Exception as e:
         raise DHError(e) from e
 
 
-def second_of_day(dt: DateTime, tz: TimeZone) -> int:
-    """ Returns the number of seconds since midnight for a DateTime in the specified time zone.
+def diff_nanos(start: Union[Instant, ZonedDateTime], end: Union[Instant, ZonedDateTime]) -> int:
+    """ Returns the difference in nanoseconds between two date time values.  Both values must be of the same type.
 
     Args:
-        dt (DateTime): the DateTime for which to find the seconds
-        tz (TimeZone): the TimeZone to use when interpreting the DateTime
+        start (Union[Instant,ZonedDateTime]): Start time.
+        end (Union[Instant,ZonedDateTime]): End time.
 
     Returns:
-        int: NULL_INT if dt is None
+        the difference in start and end in nanoseconds or NULL_LONG if any input is None.
 
     Raises:
         DHError
     """
+    if not start or not end:
+        return NULL_LONG
+
     try:
-        return _JDateTimeUtils.secondOfDay(dt, tz.value)
+        return _JDateTimeUtils.diffNanos(start, end)
     except Exception as e:
         raise DHError(e) from e
 
 
-def second_of_minute(dt: DateTime, tz: TimeZone) -> int:
-    """ Returns the number of seconds since the top of the minute for a DateTime in the specified time zone.
+def diff_micros(start: Union[Instant, ZonedDateTime], end: Union[Instant, ZonedDateTime]) -> int:
+    """ Returns the difference in microseconds between two date time values.  Both values must be of the same type.
 
     Args:
-        dt (DateTime): the DateTime for which to find the seconds
-        tz (TimeZone): the TimeZone to use when interpreting the DateTime
+        start (Union[Instant,ZonedDateTime]): Start time.
+        end (Union[Instant,ZonedDateTime]): End time.
 
     Returns:
-        int: NULL_INT if dt is None
+        the difference in start and end in microseconds or NULL_LONG if any input is None.
 
     Raises:
         DHError
     """
+    if not start or not end:
+        return NULL_LONG
+
     try:
-        return _JDateTimeUtils.secondOfMinute(dt, tz.value)
+        return _JDateTimeUtils.diffMicros(start, end)
     except Exception as e:
         raise DHError(e) from e
 
 
-def upper_bin(dt, interval: int, offset: int = 0):
-    """ Returns a DateTime value, which is at the ending (upper) end of a time range defined by the interval
-     nanoseconds. For example, a 5*MINUTE intervalNanos value would return the DateTime value for the end of the five
+def diff_millis(start: Union[Instant, ZonedDateTime], end: Union[Instant, ZonedDateTime]) -> int:
+    """ Returns the difference in milliseconds between two date time values.  Both values must be of the same type.
+
+    Args:
+        start (Union[Instant,ZonedDateTime]): Start time.
+        end (Union[Instant,ZonedDateTime]): End time.
+
+    Returns:
+        the difference in start and end in milliseconds or NULL_LONG if any input is None.
+
+    Raises:
+        DHError
+    """
+    if not start or not end:
+        return NULL_LONG
+
+    try:
+        return _JDateTimeUtils.diffMillis(start, end)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def diff_seconds(start: Union[Instant, ZonedDateTime], end: Union[Instant, ZonedDateTime]) -> float:
+    """ Returns the difference in seconds between two date time values.  Both values must be of the same type.
+
+    Args:
+        start (Union[Instant,ZonedDateTime]): Start time.
+        end (Union[Instant,ZonedDateTime]): End time.
+
+    Returns:
+        the difference in start and end in seconds or NULL_DOUBLE if any input is None.
+
+    Raises:
+        DHError
+    """
+    if not start or not end:
+        return NULL_DOUBLE
+
+    try:
+        return _JDateTimeUtils.diffSeconds(start, end)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def diff_minutes(start: Union[Instant, ZonedDateTime], end: Union[Instant, ZonedDateTime]) -> float:
+    """ Returns the difference in minutes between two date time values.  Both values must be of the same type.
+
+    Args:
+        start (Union[Instant,ZonedDateTime]): Start time.
+        end (Union[Instant,ZonedDateTime]): End time.
+
+    Returns:
+        the difference in start and end in minutes or NULL_DOUBLE if any input is None.
+
+    Raises:
+        DHError
+    """
+    if not start or not end:
+        return NULL_DOUBLE
+
+    try:
+        return _JDateTimeUtils.diffMinutes(start, end)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def diff_days(start: Union[Instant, ZonedDateTime], end: Union[Instant, ZonedDateTime]) -> float:
+    """ Returns the difference in days between two date time values.  Both values must be of the same type.
+
+    Args:
+        start (Union[Instant,ZonedDateTime]): Start time.
+        end (Union[Instant,ZonedDateTime]): End time.
+
+    Returns:
+        the difference in start and end in days or NULL_DOUBLE if any input is None.
+
+    Raises:
+        DHError
+    """
+    if not start or not end:
+        return NULL_DOUBLE
+
+    try:
+        return _JDateTimeUtils.diffDays(start, end)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def diff_years_365(start: Union[Instant, ZonedDateTime], end: Union[Instant, ZonedDateTime]) -> float:
+    """ Returns the difference in years between two date time values.  Both values must be of the same type.
+
+    Years are defined in terms of 365 day years.
+
+    Args:
+        start (Union[Instant,ZonedDateTime]): Start time.
+        end (Union[Instant,ZonedDateTime]): End time.
+
+    Returns:
+        the difference in start and end in years or NULL_DOUBLE if any input is None.
+
+    Raises:
+        DHError
+    """
+    if not start or not end:
+        return NULL_DOUBLE
+
+    try:
+        return _JDateTimeUtils.diffYears365(start, end)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def diff_years_avg(start: Union[Instant, ZonedDateTime], end: Union[Instant, ZonedDateTime]) -> float:
+    """ Returns the difference in years between two date time values.  Both values must be of the same type.
+
+    Years are defined in terms of 365.2425 day years.
+
+    Args:
+        start (Union[Instant,ZonedDateTime]): Start time.
+        end (Union[Instant,ZonedDateTime]): End time.
+
+    Returns:
+        the difference in start and end in years or NULL_DOUBLE if any input is None.
+
+    Raises:
+        DHError
+    """
+    if not start or not end:
+        return NULL_DOUBLE
+
+    try:
+        return _JDateTimeUtils.diffYearsAvg(start, end)
+    except Exception as e:
+        raise DHError(e) from e
+
+# endregion
+
+# region Comparisons
+
+def is_before(dt1: Union[Instant, ZonedDateTime], dt2: Union[Instant, ZonedDateTime]) -> bool:
+    """ Evaluates whether one date time value is before a second date time value.
+    Both values must be of the same type.
+
+    Args:
+        dt1 (Union[Instant,ZonedDateTime]): First date time.
+        dt2 (Union[Instant,ZonedDateTime]): Second date time.
+
+    Returns:
+        True if dt1 is before dt2; otherwise, False if either value is null or if dt2 is equal to or before dt1.
+
+    Raises:
+        DHError
+    """
+    if not dt1 or not dt2:
+        return False
+
+    try:
+        return _JDateTimeUtils.isBefore(dt1, dt2)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def is_before_or_equal(dt1: Union[Instant, ZonedDateTime], dt2: Union[Instant, ZonedDateTime]) -> bool:
+    """ Evaluates whether one date time value is before or equal to a second date time value.
+    Both values must be of the same type.
+
+    Args:
+        dt1 (Union[Instant,ZonedDateTime]): First date time.
+        dt2 (Union[Instant,ZonedDateTime]): Second date time.
+
+    Returns:
+        True if dt1 is before or equal to dt2; otherwise, False if either value is null or if dt2 is before dt1.
+
+    Raises:
+        DHError
+    """
+    if not dt1 or not dt2:
+        return False
+
+    try:
+        return _JDateTimeUtils.isBeforeOrEqual(dt1, dt2)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def is_after(dt1: Union[Instant, ZonedDateTime], dt2: Union[Instant, ZonedDateTime]) -> bool:
+    """ Evaluates whether one date time value is after a second date time value.
+    Both values must be of the same type.
+
+    Args:
+        dt1 (Union[Instant,ZonedDateTime]): First date time.
+        dt2 (Union[Instant,ZonedDateTime]): Second date time.
+
+    Returns:
+        True if dt1 is after dt2; otherwise, False if either value is null or if dt2 is equal to or after dt1.
+
+    Raises:
+        DHError
+    """
+    if not dt1 or not dt2:
+        return False
+
+    try:
+        return _JDateTimeUtils.isAfter(dt1, dt2)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def is_after_or_equal(dt1: Union[Instant, ZonedDateTime], dt2: Union[Instant, ZonedDateTime]) -> bool:
+    """ Evaluates whether one date time value is after or equal to a second date time value.
+    Both values must be of the same type.
+
+    Args:
+        dt1 (Union[Instant,ZonedDateTime]): First date time.
+        dt2 (Union[Instant,ZonedDateTime]): Second date time.
+
+    Returns:
+        True if dt1 is after or equal to dt2; otherwise, False if either value is null or if dt2 is after dt1.
+
+    Raises:
+        DHError
+    """
+    if not dt1 or not dt2:
+        return False
+
+    try:
+        return _JDateTimeUtils.isAfterOrEqual(dt1, dt2)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+# endregion
+
+# region Chronology
+
+def nanos_of_milli(dt: Union[Instant, ZonedDateTime]) -> int:
+    """ Returns the number of nanoseconds that have elapsed since the top of the millisecond.
+
+    Args:
+        dt (Union[Instant,ZonedDateTime]): Date time.
+
+    Returns:
+        Number of nanoseconds that have elapsed since the top of the millisecond, or NULL_INT if the input is None.
+
+    Raises:
+        DHError
+    """
+    if not dt:
+        return NULL_INT
+
+    try:
+        return _JDateTimeUtils.nanosOfMilli(dt)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def micros_of_milli(dt: Union[Instant, ZonedDateTime]) -> int:
+    """ Returns the number of microseconds that have elapsed since the top of the millisecond.
+
+    Args:
+        dt (Union[Instant,ZonedDateTime]): Date time.
+
+    Returns:
+        Number of microseconds that have elapsed since the top of the millisecond, or NULL_INT if the input is None.
+
+    Raises:
+        DHError
+    """
+    if not dt:
+        return NULL_INT
+
+    try:
+        return _JDateTimeUtils.microsOfMilli(dt)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def nanos_of_second(dt: Union[Instant, ZonedDateTime], tz: TimeZone) -> int:
+    """ Returns the number of nanoseconds that have elapsed since the top of the second.
+
+    Args:
+        dt (Union[Instant,ZonedDateTime]): Date time.
+        tz (TimeZone): Time zone.
+
+    Returns:
+        Number of nanoseconds that have elapsed since the top of the second, or NULL_LONG if any input is None.
+
+    Raises:
+        DHError
+    """
+    try:
+        if not dt or not tz:
+            return NULL_LONG
+
+        if from_jtype(dt.getClass()) == ZonedDateTime:
+            dt = to_instant(dt)
+
+        return _JDateTimeUtils.nanosOfSecond(dt, tz)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def micros_of_second(dt: Union[Instant, ZonedDateTime], tz: TimeZone) -> int:
+    """ Returns the number of microseconds that have elapsed since the top of the second.
+
+    Args:
+        dt (Union[Instant,ZonedDateTime]): Date time.
+        tz (TimeZone): Time zone.
+
+    Returns:
+        Number of microseconds that have elapsed since the top of the second, or NULL_LONG if any input is None.
+
+    Raises:
+        DHError
+    """
+    try:
+        if not dt or not tz:
+            return NULL_LONG
+
+        if from_jtype(dt.getClass()) == ZonedDateTime:
+            dt = to_instant(dt)
+
+        return _JDateTimeUtils.microsOfSecond(dt, tz)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def millis_of_second(dt: Union[Instant, ZonedDateTime], tz: TimeZone) -> int:
+    """ Returns the number of milliseconds that have elapsed since the top of the second.
+
+    Args:
+        dt (Union[Instant,ZonedDateTime]): Date time.
+        tz (TimeZone): Time zone.
+
+    Returns:
+        Number of milliseconds that have elapsed since the top of the second, or NULL_INT if any input is None.
+
+    Raises:
+        DHError
+    """
+    try:
+        if not dt or not tz:
+            return NULL_INT
+
+        if from_jtype(dt.getClass()) == ZonedDateTime:
+            dt = to_instant(dt)
+
+        return _JDateTimeUtils.millisOfSecond(dt, tz)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def second_of_minute(dt: Union[Instant, ZonedDateTime], tz: TimeZone) -> int:
+    """ Returns the number of seconds that have elapsed since the top of the minute.
+
+    Args:
+        dt (Union[Instant,ZonedDateTime]): Date time.
+        tz (TimeZone): Time zone.
+
+    Returns:
+        Number of seconds that have elapsed since the top of the minute, or NULL_INT if any input is None.
+
+    Raises:
+        DHError
+    """
+    try:
+        if not dt or not tz:
+            return NULL_INT
+
+        if from_jtype(dt.getClass()) == ZonedDateTime:
+            dt = to_instant(dt)
+
+        return _JDateTimeUtils.secondOfMinute(dt, tz)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def minute_of_hour(dt: Union[Instant, ZonedDateTime], tz: TimeZone) -> int:
+    """ Returns the number of minutes that have elapsed since the top of the hour.
+
+    Args:
+        dt (Union[Instant,ZonedDateTime]): Date time.
+        tz (TimeZone): Time zone.
+
+    Returns:
+        Number of minutes that have elapsed since the top of the hour, or NULL_INT if any input is None.
+
+    Raises:
+        DHError
+    """
+    try:
+        if not dt or not tz:
+            return NULL_INT
+
+        if from_jtype(dt.getClass()) == ZonedDateTime:
+            dt = to_instant(dt)
+
+        return _JDateTimeUtils.minuteOfHour(dt, tz)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def nanos_of_day(dt: Union[Instant, ZonedDateTime], tz: TimeZone) -> int:
+    """ Returns the number of nanoseconds that have elapsed since the top of the day.
+
+    Args:
+        dt (Union[Instant,ZonedDateTime]): Date time.
+        tz (TimeZone): Time zone.
+
+    Returns:
+        Number of nanoseconds that have elapsed since the top of the day, or NULL_LONG if any input is None.
+
+    Raises:
+        DHError
+    """
+    try:
+        if not dt or not tz:
+            return NULL_LONG
+
+        if from_jtype(dt.getClass()) == ZonedDateTime:
+            dt = to_instant(dt)
+
+        return _JDateTimeUtils.nanosOfDay(dt, tz)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def millis_of_day(dt: Union[Instant, ZonedDateTime], tz: TimeZone) -> int:
+    """ Returns the number of milliseconds that have elapsed since the top of the day.
+
+    Args:
+        dt (Union[Instant,ZonedDateTime]): Date time.
+        tz (TimeZone): Time zone.
+
+    Returns:
+        Number of milliseconds that have elapsed since the top of the day, or NULL_INT if any input is None.
+
+    Raises:
+        DHError
+    """
+    try:
+        if not dt or not tz:
+            return NULL_INT
+
+        if from_jtype(dt.getClass()) == ZonedDateTime:
+            dt = to_instant(dt)
+
+        return _JDateTimeUtils.millisOfDay(dt, tz)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def second_of_day(dt: Union[Instant, ZonedDateTime], tz: TimeZone) -> int:
+    """ Returns the number of seconds that have elapsed since the top of the day.
+
+    Args:
+        dt (Union[Instant,ZonedDateTime]): Date time.
+        tz (TimeZone): Time zone.
+
+    Returns:
+        Number of seconds that have elapsed since the top of the day, or NULL_INT if any input is None.
+
+    Raises:
+        DHError
+    """
+    try:
+        if not dt or not tz:
+            return NULL_INT
+
+        if from_jtype(dt.getClass()) == ZonedDateTime:
+            dt = to_instant(dt)
+
+        return _JDateTimeUtils.secondOfDay(dt, tz)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def minute_of_day(dt: Union[Instant, ZonedDateTime], tz: TimeZone) -> int:
+    """ Returns the number of minutes that have elapsed since the top of the day.
+
+    Args:
+        dt (Union[Instant,ZonedDateTime]): Date time.
+        tz (TimeZone): Time zone.
+
+    Returns:
+        Number of minutes that have elapsed since the top of the day, or NULL_INT if any input is None.
+
+    Raises:
+        DHError
+    """
+    try:
+        if not dt or not tz:
+            return NULL_INT
+
+        if from_jtype(dt.getClass()) == ZonedDateTime:
+            dt = to_instant(dt)
+
+        return _JDateTimeUtils.minuteOfDay(dt, tz)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def hour_of_day(dt: Union[Instant, ZonedDateTime], tz: TimeZone) -> int:
+    """ Returns the number of hours that have elapsed since the top of the day.
+
+    Args:
+        dt (Union[Instant,ZonedDateTime]): Date time.
+        tz (TimeZone): Time zone.
+
+    Returns:
+        Number of hours that have elapsed since the top of the day, or NULL_INT if any input is None.
+
+    Raises:
+        DHError
+    """
+    try:
+        if not dt or not tz:
+            return NULL_INT
+
+        if from_jtype(dt.getClass()) == ZonedDateTime:
+            dt = to_instant(dt)
+
+        return _JDateTimeUtils.hourOfDay(dt, tz)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def day_of_week(dt: Union[Instant, ZonedDateTime], tz: TimeZone) -> int:
+    """ Returns a 1-based int value of the day of the week for a date time in the specified time zone, with 1 being
+    Monday and 7 being Sunday.
+
+    Args:
+        dt (Union[Instant,ZonedDateTime]): Date time.
+        tz (TimeZone): Time zone.
+
+    Returns:
+        Day of the week, or NULL_INT if any input is None.
+
+    Raises:
+        DHError
+    """
+    try:
+        if not dt or not tz:
+            return NULL_INT
+
+        if from_jtype(dt.getClass()) == ZonedDateTime:
+            dt = to_instant(dt)
+
+        return _JDateTimeUtils.dayOfWeek(dt, tz)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def day_of_month(dt: Union[Instant, ZonedDateTime], tz: TimeZone) -> int:
+    """ Returns a 1-based int value of the day of the month for a date time and specified time zone.
+    The first day of the month returns 1, the second day returns 2, etc.
+
+    Args:
+        dt (Union[Instant,ZonedDateTime]): Date time.
+        tz (TimeZone): Time zone.
+
+    Returns:
+        Day of the month, or NULL_INT if any input is None.
+
+    Raises:
+        DHError
+    """
+    try:
+        if not dt or not tz:
+            return NULL_INT
+
+        if from_jtype(dt.getClass()) == ZonedDateTime:
+            dt = to_instant(dt)
+
+        return _JDateTimeUtils.dayOfMonth(dt, tz)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def day_of_year(dt: Union[Instant, ZonedDateTime], tz: TimeZone) -> int:
+    """ Returns a 1-based int value of the day of the year (Julian date) for a date time in the specified time zone.
+    The first day of the year returns 1, the second day returns 2, etc.
+
+    Args:
+        dt (Union[Instant,ZonedDateTime]): Date time.
+        tz (TimeZone): Time zone.
+
+    Returns:
+        Day of the year, or NULL_INT if any input is None.
+
+    Raises:
+        DHError
+    """
+    try:
+        if not dt or not tz:
+            return NULL_INT
+
+        if from_jtype(dt.getClass()) == ZonedDateTime:
+            dt = to_instant(dt)
+
+        return _JDateTimeUtils.dayOfYear(dt, tz)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def month_of_year(dt: Union[Instant, ZonedDateTime], tz: TimeZone) -> int:
+    """ Returns a 1-based int value of the month of the year (Julian date) for a date time in the specified time zone.
+    January is 1, February is 2, etc.
+
+    Args:
+        dt (Union[Instant,ZonedDateTime]): Date time.
+        tz (TimeZone): Time zone.
+
+    Returns:
+        Month of the year, or NULL_INT if any input is None.
+
+    Raises:
+        DHError
+    """
+    try:
+        if not dt or not tz:
+            return NULL_INT
+
+        if from_jtype(dt.getClass()) == ZonedDateTime:
+            dt = to_instant(dt)
+
+        return _JDateTimeUtils.monthOfYear(dt, tz)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def year(dt: Union[Instant, ZonedDateTime], tz: TimeZone) -> int:
+    """ Returns the year for a date time in the specified time zone.
+
+    Args:
+        dt (Union[Instant,ZonedDateTime]): Date time.
+        tz (TimeZone): Time zone.
+
+    Returns:
+        Year, or NULL_INT if any input is None.
+
+    Raises:
+        DHError
+    """
+    try:
+        if not dt or not tz:
+            return NULL_INT
+
+        if from_jtype(dt.getClass()) == ZonedDateTime:
+            dt = to_instant(dt)
+
+        return _JDateTimeUtils.year(dt, tz)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def year_of_century(dt: Union[Instant, ZonedDateTime], tz: TimeZone) -> int:
+    """ Returns the year of the century (two-digit year) for a date time in the specified time zone.
+
+    Args:
+        dt (Union[Instant,ZonedDateTime]): Date time.
+        tz (TimeZone): Time zone.
+
+    Returns:
+        Year of the century, or NULL_INT if any input is None.
+
+    Raises:
+        DHError
+    """
+    try:
+        if not dt or not tz:
+            return NULL_INT
+
+        if from_jtype(dt.getClass()) == ZonedDateTime:
+            dt = to_instant(dt)
+
+        return _JDateTimeUtils.yearOfCentury(dt, tz)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def at_midnight(dt: Union[Instant, ZonedDateTime], tz: TimeZone) -> Union[Instant, ZonedDateTime]:
+    """ Returns a date time for the prior midnight in the specified time zone.
+
+    Args:
+        dt (Union[Instant,ZonedDateTime]): Date time.
+        tz (TimeZone): Time zone.
+
+    Returns:
+        date time for the prior midnight in the specified time zone, or None if any input is None.
+
+    Raises:
+        DHError
+    """
+    try:
+        if not dt or not tz:
+            return None
+
+        if from_jtype(dt.getClass()) == ZonedDateTime:
+            dt = to_instant(dt)
+
+        return _JDateTimeUtils.atMidnight(dt, tz)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+# endregion
+
+# region Binning
+
+def lower_bin(dt: Union[Instant, ZonedDateTime], interval: Union[int, str], offset: Union[int, str] = 0) -> \
+        Union[Instant, ZonedDateTime]:
+    """ Returns a date time value, which is at the starting (lower) end of a time range defined by the interval
+     nanoseconds. For example, a 5*MINUTE interval value would return the date time value for the start of the
+     five minute window that contains the input date time.
+
+    Args:
+        dt (DateTime): the date time for which to evaluate the start of the containing window.
+        interval (Union[int,str]): the size of the window.  Integer values are in nanoseconds,
+            and strings are parsed as Durations.
+        offset (Union[int,str]): the window start offset.  Integer values are in nanoseconds,
+            and strings are parsed as Durations. For example, a value of MINUTE would offset all windows by one minute.
+            Default is 0.
+
+    Returns:
+        DateTime
+
+    Raises:
+        DHError
+    """
+    try:
+        if isinstance(interval, str):
+            interval = parse_duration_nanos(interval)
+
+        if isinstance(offset, str):
+            offset = parse_duration_nanos(offset)
+
+        return _JDateTimeUtils.lowerBin(dt, interval, offset)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def upper_bin(dt: Union[Instant, ZonedDateTime], interval: int, offset: int = 0) -> Union[Instant, ZonedDateTime]:
+    """ Returns a date time value, which is at the ending (upper) end of a time range defined by the interval
+     nanoseconds. For example, a 5*MINUTE interval value would return the date time value for the end of the five
      minute window that contains the input date time.
 
     Args:
-        dt (DateTime): the DateTime for which to evaluate the end of the containing window
-        interval (int): the size of the window in nanoseconds
-        offset (int): the window start offset in nanoseconds. For example, a value of MINUTE would offset all windows by
-              one minute. Default is 0
+        dt (DateTime): the date time for which to evaluate the end of the containing window.
+        interval (Union[int,str]): the size of the window.  Integer values are in nanoseconds,
+            and strings are parsed as Durations.
+        offset (Union[int,str]): the window start offset.  Integer values are in nanoseconds,
+            and strings are parsed as Durations. For example, a value of MINUTE would offset all windows by one minute.
+            Default is 0.
 
     Returns:
         DateTime
 
     Raises:
         DHError
     """
     try:
+        if isinstance(interval, str):
+            interval = parse_duration_nanos(interval)
+
+        if isinstance(offset, str):
+            offset = parse_duration_nanos(offset)
+
         return _JDateTimeUtils.upperBin(dt, interval, offset)
     except Exception as e:
         raise DHError(e) from e
 
 
-def year(dt: DateTime, tz: TimeZone) -> int:
-    """ Returns an int value of the year for a DateTime in the specified time zone.
+# endregion
+
+# region Format
+
+def format_duration_nanos(nanos: int) -> str:
+    """ Returns a nanosecond duration formatted as a "[-]PThhh:mm:ss.nnnnnnnnn" string.
+
+    Args:
+        nanos (int): Nanosecond.
+
+    Returns:
+        Formatted string, or None if the input is NULL_INT.
+
+    Raises:
+        DHError
+    """
+    try:
+        return _JDateTimeUtils.formatDurationNanos(nanos)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def format_datetime(dt: Union[Instant, ZonedDateTime], tz: TimeZone) -> str:
+    """ Returns a date time formatted as a "yyyy-MM-ddThh:mm:ss.SSSSSSSSS TZ" string.
+
+    Args:
+        dt (Union[Instant,ZonedDateTime]): Date time.
+        tz (TimeZone): Time zone.
+
+    Returns:
+        Formatted string, or None if the any input is None.
+
+    Raises:
+        DHError
+    """
+    try:
+        if not dt or not tz:
+            return None
+
+        if from_jtype(dt.getClass()) == ZonedDateTime:
+            dt = to_instant(dt)
+
+        return _JDateTimeUtils.formatDateTime(dt, tz)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def format_date(dt: Union[Instant, ZonedDateTime], tz: TimeZone) -> str:
+    """ Returns a date time formatted as a "yyyy-MM-dd" string.
 
     Args:
-        dt (DateTime): the DateTime for which to find the year
-        tz (TimeZone): the TimeZone to use when interpreting the DateTime
+        dt (Union[Instant,ZonedDateTime]): Date time.
+        tz (TimeZone): Time zone.
 
     Returns:
-        int: NULL_INT if dt is None
+        Formatted string, or None if the any input is None.
 
     Raises:
         DHError
     """
     try:
-        return _JDateTimeUtils.year(dt, tz.value)
+        if not dt or not tz:
+            return None
+
+        if from_jtype(dt.getClass()) == ZonedDateTime:
+            dt = to_instant(dt)
+
+        return _JDateTimeUtils.formatDate(dt, tz)
     except Exception as e:
         raise DHError(e) from e
 
 
-def year_of_century(dt: DateTime, tz: TimeZone) -> int:
-    """ Returns the two-digit year for a DateTime in the specified time zone.
+# endregion
+
+# region Parse
+
+def parse_time_zone(s: str, quiet: bool = False) -> Optional[TimeZone]:
+    """ Parses the string argument as a time zone.
 
     Args:
-        dt (DateTime): the DateTime for which to find the year
-        tz (TimeZone): the TimeZone to use when interpreting the DateTime
+        s (str): String to be converted.
+        quiet (bool): False will cause exceptions when strings can not be parsed.  False will cause None to be returned.
 
     Returns:
-        int
+        Time Zone
 
     Raises:
         DHError
     """
     try:
-        return _JDateTimeUtils.yearOfCentury(dt, tz.value)
+        if quiet:
+            return _JDateTimeUtils.parseTimeZoneQuiet(s)
+        else:
+            return _JDateTimeUtils.parseTimeZone(s)
     except Exception as e:
         raise DHError(e) from e
+
+
+def parse_duration_nanos(s: str, quiet: bool = False) -> int:
+    """ Parses the string argument as a time duration in nanoseconds.
+
+    Time duration strings can be formatted as '[-]PT[-]hh:mm:[ss.nnnnnnnnn]' or as a duration string
+    formatted as '[-]PnDTnHnMn.nS}'.
+
+    Args:
+        s (str): String to be converted.
+        quiet (bool): False will cause exceptions when strings can not be parsed.
+            False will cause NULL_LONG to be returned.
+
+    Returns:
+        number of nanoseconds represented by the string.
+
+    Raises:
+        DHError
+    """
+    try:
+        if quiet:
+            return _JDateTimeUtils.parseDurationNanosQuiet(s)
+        else:
+            return _JDateTimeUtils.parseDurationNanos(s)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def parse_period(s: str, quiet: bool = False) -> Optional[Period]:
+    """ Parses the string argument as a period, which is a unit of time in terms of calendar time
+    (days, weeks, months, years, etc.).
+
+    Period strings are formatted according to the ISO-8601 duration format as 'PnYnMnD' and 'PnW', where the
+    coefficients can be positive or negative.  Zero coefficients can be omitted.  Optionally, the string can
+    begin with a negative sign.
+
+    Examples:
+      "P2Y"             -- 2 Years
+      "P3M"             -- 3 Months
+      "P4W"             -- 4 Weeks
+      "P5D"             -- 5 Days
+      "P1Y2M3D"         -- 1 Year, 2 Months, 3 Days
+      "P-1Y2M"          -- -1 Year, 2 Months
+      "-P1Y2M"          -- -1 Year, -2 Months
+
+    Args:
+        s (str): String to be converted.
+        quiet (bool): False will cause exceptions when strings can not be parsed.  False will cause None to be returned.
+
+    Returns:
+        Period represented by the string.
+
+    Raises:
+        DHError
+    """
+    try:
+        if quiet:
+            return _JDateTimeUtils.parsePeriodQuiet(s)
+        else:
+            return _JDateTimeUtils.parsePeriod(s)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def parse_duration(s: str, quiet: bool = False) -> Optional[Duration]:
+    """ Parses the string argument as a duration, which is a unit of time in terms of clock time
+    (24-hour days, hours, minutes, seconds, and nanoseconds).
+
+    Duration strings are formatted according to the ISO-8601 duration format as '[-]PnDTnHnMn.nS', where the
+    coefficients can be positive or negative.  Zero coefficients can be omitted.  Optionally, the string can
+    begin with a negative sign.
+
+    Examples:
+       "PT20.345S" -- parses as "20.345 seconds"
+       "PT15M"     -- parses as "15 minutes" (where a minute is 60 seconds)
+       "PT10H"     -- parses as "10 hours" (where an hour is 3600 seconds)
+       "P2D"       -- parses as "2 days" (where a day is 24 hours or 86400 seconds)
+       "P2DT3H4M"  -- parses as "2 days, 3 hours and 4 minutes"
+       "PT-6H3M"    -- parses as "-6 hours and +3 minutes"
+       "-PT6H3M"    -- parses as "-6 hours and -3 minutes"
+       "-PT-6H+3M"  -- parses as "+6 hours and -3 minutes"
+
+    Args:
+        s (str): String to be converted.
+        quiet (bool): False will cause exceptions when strings can not be parsed.  False will cause None to be returned.
+
+    Returns:
+        Period represented by the string.
+
+    Raises:
+        DHError
+    """
+    try:
+        if quiet:
+            return _JDateTimeUtils.parseDurationQuiet(s)
+        else:
+            return _JDateTimeUtils.parseDuration(s)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def parse_epoch_nanos(s: str, quiet: bool = False) -> int:
+    """ Parses the string argument as nanoseconds since the Epoch.
+
+    Date time strings are formatted according to the ISO 8601 date time format
+    'yyyy-MM-ddThh:mm:ss[.SSSSSSSSS] TZ' and others.
+    Additionally, date time strings can be integer values that are nanoseconds, milliseconds, or seconds
+    from the Epoch.  Expected date ranges are used to infer the units.
+
+    Args:
+        s (str): String to be converted.
+        quiet (bool): False will cause exceptions when strings can not be parsed.  False will cause NULL_LONG to be returned.
+
+    Returns:
+        Instant represented by the string.
+
+    Raises:
+        DHError
+    """
+    try:
+        if quiet:
+            return _JDateTimeUtils.parseEpochNanosQuiet(s)
+        else:
+            return _JDateTimeUtils.parseEpochNanos(s)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def parse_instant(s: str, quiet: bool = False) -> Optional[Instant]:
+    """ Parses the string argument as an Instant.
+
+    Date time strings are formatted according to the ISO 8601 date time format
+    'yyyy-MM-ddThh:mm:ss[.SSSSSSSSS] TZ' and others.
+    Additionally, date time strings can be integer values that are nanoseconds, milliseconds, or seconds
+    from the Epoch.  Expected date ranges are used to infer the units.
+
+    Args:
+        s (str): String to be converted.
+        quiet (bool): False will cause exceptions when strings can not be parsed.  False will cause None to be returned.
+
+    Returns:
+        Instant represented by the string.
+
+    Raises:
+        DHError
+    """
+    try:
+        if quiet:
+            return _JDateTimeUtils.parseInstantQuiet(s)
+        else:
+            return _JDateTimeUtils.parseInstant(s)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def parse_zdt(s: str, quiet: bool = False) -> Optional[ZonedDateTime]:
+    """ Parses the string argument as a ZonedDateTime.
+
+    Date time strings are formatted according to the ISO 8601 date time format
+    '{@code 'yyyy-MM-ddThh:mm:ss[.SSSSSSSSS] TZ' and others.
+
+    Args:
+        s (str): String to be converted.
+        quiet (bool): False will cause exceptions when strings can not be parsed.  False will cause None to be returned.
+
+    Returns:
+        Instant represented by the string.
+
+    Raises:
+        DHError
+    """
+    try:
+        if quiet:
+            return _JDateTimeUtils.parseZonedDateTimeQuiet(s)
+        else:
+            return _JDateTimeUtils.parseZonedDateTime(s)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def parse_time_precision(s: str, quiet: bool = False) -> Optional[str]:
+    """ Returns a string indicating the level of precision in a time, datetime, or period nanos string. (e.g. 'SecondOfMinute').
+
+    Args:
+        s (str): Time string.
+        quiet (bool): False will cause exceptions when strings can not be parsed.  False will cause None to be returned.
+
+    Returns:
+        String indicating the level of precision in a time or datetime string (e.g. 'SecondOfMinute').
+
+    Raises:
+        DHError
+    """
+    try:
+        if quiet:
+            p = _JDateTimeUtils.parseTimePrecisionQuiet(s)
+
+            if p:
+                return p.toString()
+            else:
+                return None
+        else:
+            return _JDateTimeUtils.parseTimePrecision(s).toString()
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def parse_local_date(s: str, quiet: bool = False) -> Optional[LocalTime]:
+    """ Parses the string argument as a local date, which is a date without a time or time zone.
+
+    Date strings are formatted according to the ISO 8601 date time format as 'YYYY-MM-DD}'.
+
+    Args:
+        s (str): String to be converted.
+        quiet (bool): False will cause exceptions when strings can not be parsed.  True will cause None to be returned.
+
+    Returns:
+        LocalDate represented by the string.
+
+    Raises:
+        DHError
+    """
+    try:
+        if quiet:
+            return _JDateTimeUtils.parseLocalDateQuiet(s)
+        else:
+            return _JDateTimeUtils.parseLocalDate(s)
+    except Exception as e:
+        raise DHError(e) from e
+
+
+def parse_local_time(s: str, quiet: bool = False) -> Optional[LocalTime]:
+    """ Parses the string argument as a local time, which is the time that would be read from a clock and
+    does not have a date or timezone.
+
+    Local time strings can be formatted as 'hh:mm:ss[.nnnnnnnnn]'.
+
+    Args:
+        s (str): String to be converted.
+        quiet (bool): False will cause exceptions when strings can not be parsed.  True will cause None to be returned.
+
+    Returns:
+        LocalTime represented by the string.
+
+    Raises:
+        DHError
+    """
+    try:
+        if quiet:
+            return _JDateTimeUtils.parseLocalTimeQuiet(s)
+        else:
+            return _JDateTimeUtils.parseLocalTime(s)
+    except Exception as e:
+        raise DHError(e) from e
+
+# endregion
```

## deephaven/updateby.py

```diff
@@ -104,290 +104,362 @@
         self.j_updateby_op = j_updateby_op
 
     @property
     def j_object(self) -> jpy.JType:
         return self.j_updateby_op
 
 
-def ema_tick(time_scale_ticks: float, cols: Union[str, List[str]],
+def ema_tick(decay_ticks: float, cols: Union[str, List[str]],
                    op_control: OperationControl = None) -> UpdateByOperation:
     """Creates an EMA (exponential moving average) UpdateByOperation for the supplied column names, using ticks as
     the decay unit.
 
     The formula used is
-        a = e^(-1 / time_scale_ticks)
+        a = e^(-1 / decay_ticks)
         ema_next = a * ema_last + (1 - a) * value
 
     Args:
-        time_scale_ticks (int): the decay rate in ticks
+        decay_ticks (float): the decay rate in ticks
         cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
             i.e. "new_col = col"; when empty, update_by perform the ema operation on all columns.
         op_control (OperationControl): defines how special cases should behave, when None, the default OperationControl
             settings as specified in :meth:`~OperationControl.__init__` will be used
 
     Returns:
         an UpdateByOperation
 
     Raises:
         DHError
     """
     try:
         cols = to_sequence(cols)
         if op_control is None:
-            return UpdateByOperation(j_updateby_op=_JUpdateByOperation.Ema(time_scale_ticks, *cols))
+            return UpdateByOperation(j_updateby_op=_JUpdateByOperation.Ema(decay_ticks, *cols))
         else:
             return UpdateByOperation(
-                j_updateby_op=_JUpdateByOperation.Ema(op_control.j_op_control, time_scale_ticks, *cols))
+                j_updateby_op=_JUpdateByOperation.Ema(op_control.j_op_control, decay_ticks, *cols))
     except Exception as e:
         raise DHError(e, "failed to create a tick-decay EMA UpdateByOperation.") from e
 
 
-def ema_time(ts_col: str, time_scale: Union[int, str], cols: Union[str, List[str]],
+def ema_time(ts_col: str, decay_time: Union[int, str], cols: Union[str, List[str]],
                    op_control: OperationControl = None) -> UpdateByOperation:
     """Creates an EMA(exponential moving average) UpdateByOperation for the supplied column names, using time as the
     decay unit.
 
     The formula used is
-        a = e^(-dt / time_scale)
+        a = e^(-dt / decay_time)
         ema_next = a * ema_last + (1 - a) * value
 
      Args:
         ts_col (str): the column in the source table to use for timestamps
-        time_scale (Union[int, str]): the decay rate, can be expressed as an integer in nanoseconds or a time
-            interval string, e.g. "00:00:00.001"
+        decay_time (Union[int, str]): the decay rate, can be expressed as an integer in nanoseconds or a time
+            interval string, e.g. "PT00:00:00.001"
         cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
             i.e. "new_col = col"; when empty, update_by perform the ema operation on all columns.
         op_control (OperationControl): defines how special cases should behave,  when None, the default OperationControl
             settings as specified in :meth:`~OperationControl.__init__` will be used
 
     Returns:
         an UpdateByOperation
 
     Raises:
         DHError
      """
     try:
-        time_scale = _JDateTimeUtils.expressionToNanos(time_scale) if isinstance(time_scale, str) else time_scale
+        decay_time = _JDateTimeUtils.parseDurationNanos(decay_time) if isinstance(decay_time, str) else decay_time
         cols = to_sequence(cols)
         if op_control is None:
-            return UpdateByOperation(j_updateby_op=_JUpdateByOperation.Ema(ts_col, time_scale, *cols))
+            return UpdateByOperation(j_updateby_op=_JUpdateByOperation.Ema(ts_col, decay_time, *cols))
         else:
             return UpdateByOperation(
-                j_updateby_op=_JUpdateByOperation.Ema(op_control.j_op_control, ts_col, time_scale, *cols))
+                j_updateby_op=_JUpdateByOperation.Ema(op_control.j_op_control, ts_col, decay_time, *cols))
     except Exception as e:
         raise DHError(e, "failed to create a time-decay EMA UpdateByOperation.") from e
 
 
-def ems_tick(time_scale_ticks: float, cols: Union[str, List[str]],
+def ems_tick(decay_ticks: float, cols: Union[str, List[str]],
                    op_control: OperationControl = None) -> UpdateByOperation:
     """Creates an EMS (exponential moving sum) UpdateByOperation for the supplied column names, using ticks as
     the decay unit.
 
     The formula used is
-        a = e^(-1 / time_scale_ticks)
+        a = e^(-1 / decay_ticks)
         ems_next = a * ems_last + value
 
     Args:
-        time_scale_ticks (int): the decay rate in ticks
+        decay_ticks (float): the decay rate in ticks
         cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
             i.e. "new_col = col"; when empty, update_by perform the ems operation on all columns.
         op_control (OperationControl): defines how special cases should behave, when None, the default OperationControl
             settings as specified in :meth:`~OperationControl.__init__` will be used
 
     Returns:
         an UpdateByOperation
 
     Raises:
         DHError
     """
     try:
         cols = to_sequence(cols)
         if op_control is None:
-            return UpdateByOperation(j_updateby_op=_JUpdateByOperation.Ems(time_scale_ticks, *cols))
+            return UpdateByOperation(j_updateby_op=_JUpdateByOperation.Ems(decay_ticks, *cols))
         else:
             return UpdateByOperation(
-                j_updateby_op=_JUpdateByOperation.Ems(op_control.j_op_control, time_scale_ticks, *cols))
+                j_updateby_op=_JUpdateByOperation.Ems(op_control.j_op_control, decay_ticks, *cols))
     except Exception as e:
         raise DHError(e, "failed to create a tick-decay EMS UpdateByOperation.") from e
 
 
-def ems_time(ts_col: str, time_scale: Union[int, str], cols: Union[str, List[str]],
+def ems_time(ts_col: str, decay_time: Union[int, str], cols: Union[str, List[str]],
                    op_control: OperationControl = None) -> UpdateByOperation:
     """Creates an EMS (exponential moving sum) UpdateByOperation for the supplied column names, using time as the
     decay unit.
 
     The formula used is
-        a = e^(-dt / time_scale)
+        a = e^(-dt / decay_time)
         eems_next = a * ems_last + value
 
      Args:
         ts_col (str): the column in the source table to use for timestamps
-        time_scale (Union[int, str]): the decay rate, can be expressed as an integer in nanoseconds or a time
-            interval string, e.g. "00:00:00.001"
+        decay_time (Union[int, str]): the decay rate, can be expressed as an integer in nanoseconds or a time
+            interval string, e.g. "PT00:00:00.001"
         cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
             i.e. "new_col = col"; when empty, update_by perform the ems operation on all columns.
         op_control (OperationControl): defines how special cases should behave,  when None, the default OperationControl
             settings as specified in :meth:`~OperationControl.__init__` will be used
 
     Returns:
         an UpdateByOperation
 
     Raises:
         DHError
      """
     try:
-        time_scale = _JDateTimeUtils.expressionToNanos(time_scale) if isinstance(time_scale, str) else time_scale
+        decay_time = _JDateTimeUtils.parseDurationNanos(decay_time) if isinstance(decay_time, str) else decay_time
         cols = to_sequence(cols)
         if op_control is None:
-            return UpdateByOperation(j_updateby_op=_JUpdateByOperation.Ems(ts_col, time_scale, *cols))
+            return UpdateByOperation(j_updateby_op=_JUpdateByOperation.Ems(ts_col, decay_time, *cols))
         else:
             return UpdateByOperation(
-                j_updateby_op=_JUpdateByOperation.Ems(op_control.j_op_control, ts_col, time_scale, *cols))
+                j_updateby_op=_JUpdateByOperation.Ems(op_control.j_op_control, ts_col, decay_time, *cols))
     except Exception as e:
         raise DHError(e, "failed to create a time-decay EMS UpdateByOperation.") from e
 
 
-def emmin_tick(time_scale_ticks: float, cols: Union[str, List[str]],
+def emmin_tick(decay_ticks: float, cols: Union[str, List[str]],
                    op_control: OperationControl = None) -> UpdateByOperation:
     """Creates an EM Min (exponential moving minimum) UpdateByOperation for the supplied column names, using ticks as
     the decay unit.
 
     The formula used is
-        a = e^(-1 / time_scale_ticks)
+        a = e^(-1 / decay_ticks)
         em_val_next = min(a * em_val_last, value)
 
     Args:
-        time_scale_ticks (int): the decay rate in ticks
+        decay_ticks (float): the decay rate in ticks
         cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
             i.e. "new_col = col"; when empty, update_by perform the operation on all columns.
         op_control (OperationControl): defines how special cases should behave, when None, the default OperationControl
             settings as specified in :meth:`~OperationControl.__init__` will be used
 
     Returns:
         an UpdateByOperation
 
     Raises:
         DHError
     """
     try:
         cols = to_sequence(cols)
         if op_control is None:
-            return UpdateByOperation(j_updateby_op=_JUpdateByOperation.EmMin(time_scale_ticks, *cols))
+            return UpdateByOperation(j_updateby_op=_JUpdateByOperation.EmMin(decay_ticks, *cols))
         else:
             return UpdateByOperation(
-                j_updateby_op=_JUpdateByOperation.EmMin(op_control.j_op_control, time_scale_ticks, *cols))
+                j_updateby_op=_JUpdateByOperation.EmMin(op_control.j_op_control, decay_ticks, *cols))
     except Exception as e:
         raise DHError(e, "failed to create a tick-decay EM Min UpdateByOperation.") from e
 
 
-def emmin_time(ts_col: str, time_scale: Union[int, str], cols: Union[str, List[str]],
+def emmin_time(ts_col: str, decay_time: Union[int, str], cols: Union[str, List[str]],
                    op_control: OperationControl = None) -> UpdateByOperation:
     """Creates an EM Min (exponential moving minimum) UpdateByOperation for the supplied column names, using time as the
     decay unit.
 
     The formula used is
-        a = e^(-dt / time_scale)
+        a = e^(-dt / decay_time)
         em_val_next = min(a * em_val_last, value)
 
      Args:
         ts_col (str): the column in the source table to use for timestamps
-        time_scale (Union[int, str]): the decay rate, can be expressed as an integer in nanoseconds or a time
-            interval string, e.g. "00:00:00.001"
+        decay_time (Union[int, str]): the decay rate, can be expressed as an integer in nanoseconds or a time
+            interval string, e.g. "PT00:00:00.001"
         cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
             i.e. "new_col = col"; when empty, update_by perform the operation on all columns.
         op_control (OperationControl): defines how special cases should behave,  when None, the default OperationControl
             settings as specified in :meth:`~OperationControl.__init__` will be used
 
     Returns:
         an UpdateByOperation
 
     Raises:
         DHError
      """
     try:
-        time_scale = _JDateTimeUtils.expressionToNanos(time_scale) if isinstance(time_scale, str) else time_scale
+        decay_time = _JDateTimeUtils.parseDurationNanos(decay_time) if isinstance(decay_time, str) else decay_time
         cols = to_sequence(cols)
         if op_control is None:
-            return UpdateByOperation(j_updateby_op=_JUpdateByOperation.EmMin(ts_col, time_scale, *cols))
+            return UpdateByOperation(j_updateby_op=_JUpdateByOperation.EmMin(ts_col, decay_time, *cols))
         else:
             return UpdateByOperation(
-                j_updateby_op=_JUpdateByOperation.EmMin(op_control.j_op_control, ts_col, time_scale, *cols))
+                j_updateby_op=_JUpdateByOperation.EmMin(op_control.j_op_control, ts_col, decay_time, *cols))
     except Exception as e:
         raise DHError(e, "failed to create a time-decay EM Min UpdateByOperation.") from e
 
 
-def emmax_tick(time_scale_ticks: float, cols: Union[str, List[str]],
+def emmax_tick(decay_ticks: float, cols: Union[str, List[str]],
                      op_control: OperationControl = None) -> UpdateByOperation:
     """Creates an EM Max (exponential moving maximum) UpdateByOperation for the supplied column names, using ticks as
     the decay unit.
 
     The formula used is
-        a = e^(-1 / time_scale_ticks)
+        a = e^(-1 / decay_ticks)
         em_val_next = max(a * em_val_last, value)
 
     Args:
-        time_scale_ticks (int): the decay rate in ticks
+        decay_ticks (float): the decay rate in ticks
         cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
             i.e. "new_col = col"; when empty, update_by perform the operation on all columns.
         op_control (OperationControl): defines how special cases should behave, when None, the default OperationControl
             settings as specified in :meth:`~OperationControl.__init__` will be used
 
     Returns:
         an UpdateByOperation
 
     Raises:
         DHError
     """
     try:
         cols = to_sequence(cols)
         if op_control is None:
-            return UpdateByOperation(j_updateby_op=_JUpdateByOperation.EmMax(time_scale_ticks, *cols))
+            return UpdateByOperation(j_updateby_op=_JUpdateByOperation.EmMax(decay_ticks, *cols))
         else:
             return UpdateByOperation(
-                j_updateby_op=_JUpdateByOperation.EmMax(op_control.j_op_control, time_scale_ticks, *cols))
+                j_updateby_op=_JUpdateByOperation.EmMax(op_control.j_op_control, decay_ticks, *cols))
     except Exception as e:
         raise DHError(e, "failed to create a tick-decay EM Max UpdateByOperation.") from e
 
 
-def emmax_time(ts_col: str, time_scale: Union[int, str], cols: Union[str, List[str]],
+def emmax_time(ts_col: str, decay_time: Union[int, str], cols: Union[str, List[str]],
                      op_control: OperationControl = None) -> UpdateByOperation:
     """Creates an EM Max (exponential moving maximum) UpdateByOperation for the supplied column names, using time as the
     decay unit.
 
     The formula used is
-        a = e^(-dt / time_scale)
+        a = e^(-dt / decay_time)
         em_val_next = max(a * em_val_last, value)
 
      Args:
         ts_col (str): the column in the source table to use for timestamps
 
-        time_scale (Union[int, str]): the decay rate, can be expressed as an integer in nanoseconds or a time
-            interval string, e.g. "00:00:00.001"
+        decay_time (Union[int, str]): the decay rate, can be expressed as an integer in nanoseconds or a time
+            interval string, e.g. "PT00:00:00.001"
         cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
             i.e. "new_col = col"; when empty, update_by perform the operation on all columns.
         op_control (OperationControl): defines how special cases should behave,  when None, the default OperationControl
             settings as specified in :meth:`~OperationControl.__init__` will be used
 
     Returns:
         an UpdateByOperation
 
     Raises:
         DHError
      """
     try:
-        time_scale = _JDateTimeUtils.expressionToNanos(time_scale) if isinstance(time_scale, str) else time_scale
+        decay_time = _JDateTimeUtils.parseDurationNanos(decay_time) if isinstance(decay_time, str) else decay_time
         cols = to_sequence(cols)
         if op_control is None:
-            return UpdateByOperation(j_updateby_op=_JUpdateByOperation.EmMax(ts_col, time_scale, *cols))
+            return UpdateByOperation(j_updateby_op=_JUpdateByOperation.EmMax(ts_col, decay_time, *cols))
         else:
             return UpdateByOperation(
-                j_updateby_op=_JUpdateByOperation.EmMax(op_control.j_op_control, ts_col, time_scale, *cols))
+                j_updateby_op=_JUpdateByOperation.EmMax(op_control.j_op_control, ts_col, decay_time, *cols))
     except Exception as e:
         raise DHError(e, "failed to create a time-decay EM Max UpdateByOperation.") from e
 
+def emstd_tick(decay_ticks: float, cols: Union[str, List[str]],
+             op_control: OperationControl = None) -> UpdateByOperation:
+    """Creates an EM Std (exponential moving standard deviation) UpdateByOperation for the supplied column names, using
+    ticks as the decay unit.
+
+    The formula used is
+        a = e^(-1 / decay_ticks)
+        variance = a * (prevVariance + (1  a) * (x  prevEma)^2)
+        ema = a * prevEma + x
+        std = sqrt(variance)
+
+    Args:
+        decay_ticks (float): the decay rate in ticks
+        cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
+            i.e. "new_col = col"; when empty, update_by perform the ems operation on all columns.
+        op_control (OperationControl): defines how special cases should behave, when None, the default OperationControl
+            settings as specified in :meth:`~OperationControl.__init__` will be used
+
+    Returns:
+        an UpdateByOperation
+
+    Raises:
+        DHError
+    """
+    try:
+        cols = to_sequence(cols)
+        if op_control is None:
+            return UpdateByOperation(j_updateby_op=_JUpdateByOperation.EmStd(decay_ticks, *cols))
+        else:
+            return UpdateByOperation(
+                j_updateby_op=_JUpdateByOperation.EmStd(op_control.j_op_control, decay_ticks, *cols))
+    except Exception as e:
+        raise DHError(e, "failed to create a tick-decay EM Std UpdateByOperation.") from e
+
+
+def emstd_time(ts_col: str, decay_time: Union[int, str], cols: Union[str, List[str]],
+             op_control: OperationControl = None) -> UpdateByOperation:
+    """Creates an EM Std (exponential moving standard deviation) UpdateByOperation for the supplied column names, using
+    time as the decay unit.
+
+    The formula used is
+        a = e^(-dt / timeDecay)
+        variance = a * (prevVariance + (1  a) * (x  prevEma)^2)
+        ema = a * prevEma + x
+        std = sqrt(variance)
+
+     Args:
+        ts_col (str): the column in the source table to use for timestamps
+        decay_time (Union[int, str]): the decay rate, can be expressed as an integer in nanoseconds or a time
+            interval string, e.g. "PT00:00:00.001"
+        cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
+            i.e. "new_col = col"; when empty, update_by perform the ems operation on all columns.
+        op_control (OperationControl): defines how special cases should behave,  when None, the default OperationControl
+            settings as specified in :meth:`~OperationControl.__init__` will be used
+
+    Returns:
+        an UpdateByOperation
+
+    Raises:
+        DHError
+     """
+    try:
+        decay_time = _JDateTimeUtils.parseDurationNanos(decay_time) if isinstance(decay_time, str) else decay_time
+        cols = to_sequence(cols)
+        if op_control is None:
+            return UpdateByOperation(j_updateby_op=_JUpdateByOperation.EmStd(ts_col, decay_time, *cols))
+        else:
+            return UpdateByOperation(
+                    j_updateby_op=_JUpdateByOperation.EmStd(op_control.j_op_control, ts_col, decay_time, *cols))
+    except Exception as e:
+        raise DHError(e, "failed to create a time-decay EM Std UpdateByOperation.") from e
+
 
 def cum_sum(cols: Union[str, List[str]]) -> UpdateByOperation:
     """Creates a cumulative sum UpdateByOperation for the supplied column names.
 
     Args:
 
         cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
@@ -566,44 +638,44 @@
     function accepts nanoseconds or time strings as the reverse and forward window parameters. Negative values are
     allowed and can be used to generate completely forward or completely reverse windows. A row containing a null in
     the timestamp column belongs to no window and will not be considered in the windows of other rows; its output will
     be null.
      
     Here are some examples of window values:
         rev_time = 0, fwd_time = 0 - contains rows that exactly match the current row timestamp
-        rev_time = "00:10:00", fwd_time = "0" - contains rows from 10m before through the current row timestamp (
+        rev_time = "PT00:10:00", fwd_time = "0" - contains rows from 10m before through the current row timestamp (
             inclusive)
         rev_time = 0, fwd_time = 600_000_000_000 - contains rows from the current row through 10m following the
             current row timestamp (inclusive)
-        rev_time = "00:10:00", fwd_time = "00:10:00" - contains rows from 10m before through 10m following
+        rev_time = "PT00:10:00", fwd_time = "PT00:10:00" - contains rows from 10m before through 10m following
             the current row timestamp (inclusive)
-        rev_time = "00:10:00", fwd_time = "-00:05:00" - contains rows from 10m before through 5m before the
+        rev_time = "PT00:10:00", fwd_time = "-PT00:05:00" - contains rows from 10m before through 5m before the
             current row timestamp (inclusive), this is a purely backwards looking window
-        rev_time = "-00:05:00", fwd_time = "00:10:00"} - contains rows from 5m following through 10m
+        rev_time = "-PT00:05:00", fwd_time = "PT00:10:00"} - contains rows from 5m following through 10m
             following the current row timestamp (inclusive), this is a purely forwards looking window
     
     Args:
         ts_col (str): the timestamp column for determining the window
         cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
             i.e. "new_col = col"; when empty, update_by perform the rolling sum operation on all columns.
         rev_time (int): the look-behind window size, can be expressed as an integer in nanoseconds or a time
-            interval string, e.g. "00:00:00.001"
+            interval string, e.g. "PT00:00:00.001"
         fwd_time (int): the look-ahead window size, can be expressed as an integer in nanoseconds or a time
-            interval string, e.g. "00:00:00.001", default is 0
+            interval string, e.g. "PT00:00:00.001", default is 0
 
     Returns:
         an UpdateByOperation
 
     Raises:
         DHError
     """
     try:
         cols = to_sequence(cols)
-        rev_time = _JDateTimeUtils.expressionToNanos(rev_time) if isinstance(rev_time, str) else rev_time
-        fwd_time = _JDateTimeUtils.expressionToNanos(fwd_time) if isinstance(fwd_time, str) else fwd_time
+        rev_time = _JDateTimeUtils.parseDurationNanos(rev_time) if isinstance(rev_time, str) else rev_time
+        fwd_time = _JDateTimeUtils.parseDurationNanos(fwd_time) if isinstance(fwd_time, str) else fwd_time
         return UpdateByOperation(j_updateby_op=_JUpdateByOperation.RollingSum(ts_col, rev_time, fwd_time, *cols))
     except Exception as e:
         raise DHError(e, "failed to create a rolling sum (time) UpdateByOperation.") from e
 
 
 def rolling_group_tick(cols: Union[str, List[str]], rev_ticks: int, fwd_ticks: int = 0) -> UpdateByOperation:
     """Creates a rolling group UpdateByOperation for the supplied column names, using ticks as the windowing unit. Ticks 
@@ -648,44 +720,44 @@
     function accepts nanoseconds or time strings as the reverse and forward window parameters. Negative values are
     allowed and can be used to generate completely forward or completely reverse windows. A row containing a null in
     the timestamp column belongs to no window and will not be considered in the windows of other rows; its output will
     be null.
      
     Here are some examples of window values:
         rev_time = 0, fwd_time = 0 - contains rows that exactly match the current row timestamp
-        rev_time = "00:10:00", fwd_time = "0" - contains rows from 10m before through the current row timestamp (
+        rev_time = "PT00:10:00", fwd_time = "0" - contains rows from 10m before through the current row timestamp (
             inclusive)
         rev_time = 0, fwd_time = 600_000_000_000 - contains rows from the current row through 10m following the
             current row timestamp (inclusive)
-        rev_time = "00:10:00", fwd_time = "00:10:00" - contains rows from 10m before through 10m following
+        rev_time = "PT00:10:00", fwd_time = "PT00:10:00" - contains rows from 10m before through 10m following
             the current row timestamp (inclusive)
-        rev_time = "00:10:00", fwd_time = "-00:05:00" - contains rows from 10m before through 5m before the
+        rev_time = "PT00:10:00", fwd_time = "-PT00:05:00" - contains rows from 10m before through 5m before the
             current row timestamp (inclusive), this is a purely backwards looking window
-        rev_time = "-00:05:00", fwd_time = "00:10:00"} - contains rows from 5m following through 10m
+        rev_time = "-PT00:05:00", fwd_time = "PT00:10:00"} - contains rows from 5m following through 10m
             following the current row timestamp (inclusive), this is a purely forwards looking window
     
     Args:
         ts_col (str): the timestamp column for determining the window
         cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
             i.e. "new_col = col"; when empty, update_by perform the rolling group operation on all columns.
         rev_time (int): the look-behind window size, can be expressed as an integer in nanoseconds or a time
-            interval string, e.g. "00:00:00.001"
+            interval string, e.g. "PT00:00:00.001"
         fwd_time (int): the look-ahead window size, can be expressed as an integer in nanoseconds or a time
-            interval string, e.g. "00:00:00.001", default is 0
+            interval string, e.g. "PT00:00:00.001", default is 0
 
     Returns:
         an UpdateByOperation
 
     Raises:
         DHError
     """
     try:
         cols = to_sequence(cols)
-        rev_time = _JDateTimeUtils.expressionToNanos(rev_time) if isinstance(rev_time, str) else rev_time
-        fwd_time = _JDateTimeUtils.expressionToNanos(fwd_time) if isinstance(fwd_time, str) else fwd_time
+        rev_time = _JDateTimeUtils.parseDurationNanos(rev_time) if isinstance(rev_time, str) else rev_time
+        fwd_time = _JDateTimeUtils.parseDurationNanos(fwd_time) if isinstance(fwd_time, str) else fwd_time
         return UpdateByOperation(j_updateby_op=_JUpdateByOperation.RollingGroup(ts_col, rev_time, fwd_time, *cols))
     except Exception as e:
         raise DHError(e, "failed to create a rolling group (time) UpdateByOperation.") from e
 
 
 def rolling_avg_tick(cols: Union[str, List[str]], rev_ticks: int, fwd_ticks: int = 0) -> UpdateByOperation:
     """Creates a rolling average UpdateByOperation for the supplied column names, using ticks as the windowing unit. Ticks
@@ -730,44 +802,44 @@
     function accepts nanoseconds or time strings as the reverse and forward window parameters. Negative values are
     allowed and can be used to generate completely forward or completely reverse windows. A row containing a null in
     the timestamp column belongs to no window and will not be considered in the windows of other rows; its output will
     be null.
      
     Here are some examples of window values:
         rev_time = 0, fwd_time = 0 - contains rows that exactly match the current row timestamp
-        rev_time = "00:10:00", fwd_time = "0" - contains rows from 10m before through the current row timestamp (
+        rev_time = "PT00:10:00", fwd_time = "0" - contains rows from 10m before through the current row timestamp (
             inclusive)
         rev_time = 0, fwd_time = 600_000_000_000 - contains rows from the current row through 10m following the
             current row timestamp (inclusive)
-        rev_time = "00:10:00", fwd_time = "00:10:00" - contains rows from 10m before through 10m following
+        rev_time = "PT00:10:00", fwd_time = "PT00:10:00" - contains rows from 10m before through 10m following
             the current row timestamp (inclusive)
-        rev_time = "00:10:00", fwd_time = "-00:05:00" - contains rows from 10m before through 5m before the
+        rev_time = "PT00:10:00", fwd_time = "-PT00:05:00" - contains rows from 10m before through 5m before the
             current row timestamp (inclusive), this is a purely backwards looking window
-        rev_time = "-00:05:00", fwd_time = "00:10:00"} - contains rows from 5m following through 10m
+        rev_time = "-PT00:05:00", fwd_time = "PT00:10:00"} - contains rows from 5m following through 10m
             following the current row timestamp (inclusive), this is a purely forwards looking window
     
     Args:
         ts_col (str): the timestamp column for determining the window
         cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
             i.e. "new_col = col"; when empty, update_by perform the rolling average operation on all columns.
         rev_time (int): the look-behind window size, can be expressed as an integer in nanoseconds or a time
-            interval string, e.g. "00:00:00.001"
+            interval string, e.g. "PT00:00:00.001"
         fwd_time (int): the look-ahead window size, can be expressed as an integer in nanoseconds or a time
-            interval string, e.g. "00:00:00.001", default is 0
+            interval string, e.g. "PT00:00:00.001", default is 0
 
     Returns:
         an UpdateByOperation
 
     Raises:
         DHError
     """
     try:
         cols = to_sequence(cols)
-        rev_time = _JDateTimeUtils.expressionToNanos(rev_time) if isinstance(rev_time, str) else rev_time
-        fwd_time = _JDateTimeUtils.expressionToNanos(fwd_time) if isinstance(fwd_time, str) else fwd_time
+        rev_time = _JDateTimeUtils.parseDurationNanos(rev_time) if isinstance(rev_time, str) else rev_time
+        fwd_time = _JDateTimeUtils.parseDurationNanos(fwd_time) if isinstance(fwd_time, str) else fwd_time
         return UpdateByOperation(j_updateby_op=_JUpdateByOperation.RollingAvg(ts_col, rev_time, fwd_time, *cols))
     except Exception as e:
         raise DHError(e, "failed to create a rolling average (time) UpdateByOperation.") from e
 
 
 def rolling_min_tick(cols: Union[str, List[str]], rev_ticks: int, fwd_ticks: int = 0) -> UpdateByOperation:
     """Creates a rolling minimum UpdateByOperation for the supplied column names, using ticks as the windowing unit. Ticks
@@ -812,44 +884,44 @@
     function accepts nanoseconds or time strings as the reverse and forward window parameters. Negative values are
     allowed and can be used to generate completely forward or completely reverse windows. A row containing a null in
     the timestamp column belongs to no window and will not be considered in the windows of other rows; its output will
     be null.
      
     Here are some examples of window values:
         rev_time = 0, fwd_time = 0 - contains rows that exactly match the current row timestamp
-        rev_time = "00:10:00", fwd_time = "0" - contains rows from 10m before through the current row timestamp (
+        rev_time = "PT00:10:00", fwd_time = "0" - contains rows from 10m before through the current row timestamp (
             inclusive)
         rev_time = 0, fwd_time = 600_000_000_000 - contains rows from the current row through 10m following the
             current row timestamp (inclusive)
-        rev_time = "00:10:00", fwd_time = "00:10:00" - contains rows from 10m before through 10m following
+        rev_time = "PT00:10:00", fwd_time = "PT00:10:00" - contains rows from 10m before through 10m following
             the current row timestamp (inclusive)
-        rev_time = "00:10:00", fwd_time = "-00:05:00" - contains rows from 10m before through 5m before the
+        rev_time = "PT00:10:00", fwd_time = "-PT00:05:00" - contains rows from 10m before through 5m before the
             current row timestamp (inclusive), this is a purely backwards looking window
-        rev_time = "-00:05:00", fwd_time = "00:10:00"} - contains rows from 5m following through 10m
+        rev_time = "-PT00:05:00", fwd_time = "PT00:10:00"} - contains rows from 5m following through 10m
             following the current row timestamp (inclusive), this is a purely forwards looking window
     
     Args:
         ts_col (str): the timestamp column for determining the window
         cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
             i.e. "new_col = col"; when empty, update_by perform the rolling minimum operation on all columns.
         rev_time (int): the look-behind window size, can be expressed as an integer in nanoseconds or a time
-            interval string, e.g. "00:00:00.001"
+            interval string, e.g. "PT00:00:00.001"
         fwd_time (int): the look-ahead window size, can be expressed as an integer in nanoseconds or a time
-            interval string, e.g. "00:00:00.001", default is 0
+            interval string, e.g. "PT00:00:00.001", default is 0
 
     Returns:
         an UpdateByOperation
 
     Raises:
         DHError
     """
     try:
         cols = to_sequence(cols)
-        rev_time = _JDateTimeUtils.expressionToNanos(rev_time) if isinstance(rev_time, str) else rev_time
-        fwd_time = _JDateTimeUtils.expressionToNanos(fwd_time) if isinstance(fwd_time, str) else fwd_time
+        rev_time = _JDateTimeUtils.parseDurationNanos(rev_time) if isinstance(rev_time, str) else rev_time
+        fwd_time = _JDateTimeUtils.parseDurationNanos(fwd_time) if isinstance(fwd_time, str) else fwd_time
         return UpdateByOperation(j_updateby_op=_JUpdateByOperation.RollingMin(ts_col, rev_time, fwd_time, *cols))
     except Exception as e:
         raise DHError(e, "failed to create a rolling minimum (time) UpdateByOperation.") from e
 
 
 def rolling_max_tick(cols: Union[str, List[str]], rev_ticks: int, fwd_ticks: int = 0) -> UpdateByOperation:
     """Creates a rolling maximum UpdateByOperation for the supplied column names, using ticks as the windowing unit. Ticks
@@ -894,44 +966,44 @@
     function accepts nanoseconds or time strings as the reverse and forward window parameters. Negative values are
     allowed and can be used to generate completely forward or completely reverse windows. A row containing a null in
     the timestamp column belongs to no window and will not be considered in the windows of other rows; its output will
     be null.
      
     Here are some examples of window values:
         rev_time = 0, fwd_time = 0 - contains rows that exactly match the current row timestamp
-        rev_time = "00:10:00", fwd_time = "0" - contains rows from 10m before through the current row timestamp (
+        rev_time = "PT00:10:00", fwd_time = "0" - contains rows from 10m before through the current row timestamp (
             inclusive)
         rev_time = 0, fwd_time = 600_000_000_000 - contains rows from the current row through 10m following the
             current row timestamp (inclusive)
-        rev_time = "00:10:00", fwd_time = "00:10:00" - contains rows from 10m before through 10m following
+        rev_time = "PT00:10:00", fwd_time = "PT00:10:00" - contains rows from 10m before through 10m following
             the current row timestamp (inclusive)
-        rev_time = "00:10:00", fwd_time = "-00:05:00" - contains rows from 10m before through 5m before the
+        rev_time = "PT00:10:00", fwd_time = "-PT00:05:00" - contains rows from 10m before through 5m before the
             current row timestamp (inclusive), this is a purely backwards looking window
-        rev_time = "-00:05:00", fwd_time = "00:10:00"} - contains rows from 5m following through 10m
+        rev_time = "-PT00:05:00", fwd_time = "PT00:10:00"} - contains rows from 5m following through 10m
             following the current row timestamp (inclusive), this is a purely forwards looking window
     
     Args:
         ts_col (str): the timestamp column for determining the window
         cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
             i.e. "new_col = col"; when empty, update_by perform the rolling maximum operation on all columns.
         rev_time (int): the look-behind window size, can be expressed as an integer in nanoseconds or a time
-            interval string, e.g. "00:00:00.001"
+            interval string, e.g. "PT00:00:00.001"
         fwd_time (int): the look-ahead window size, can be expressed as an integer in nanoseconds or a time
-            interval string, e.g. "00:00:00.001", default is 0
+            interval string, e.g. "PT00:00:00.001", default is 0
 
     Returns:
         an UpdateByOperation
 
     Raises:
         DHError
     """
     try:
         cols = to_sequence(cols)
-        rev_time = _JDateTimeUtils.expressionToNanos(rev_time) if isinstance(rev_time, str) else rev_time
-        fwd_time = _JDateTimeUtils.expressionToNanos(fwd_time) if isinstance(fwd_time, str) else fwd_time
+        rev_time = _JDateTimeUtils.parseDurationNanos(rev_time) if isinstance(rev_time, str) else rev_time
+        fwd_time = _JDateTimeUtils.parseDurationNanos(fwd_time) if isinstance(fwd_time, str) else fwd_time
         return UpdateByOperation(j_updateby_op=_JUpdateByOperation.RollingMax(ts_col, rev_time, fwd_time, *cols))
     except Exception as e:
         raise DHError(e, "failed to create a rolling maximum (time) UpdateByOperation.") from e
 
 
 def rolling_prod_tick(cols: Union[str, List[str]], rev_ticks: int, fwd_ticks: int = 0) -> UpdateByOperation:
     """Creates a rolling product UpdateByOperation for the supplied column names, using ticks as the windowing unit. Ticks
@@ -976,44 +1048,44 @@
     function accepts nanoseconds or time strings as the reverse and forward window parameters. Negative values are
     allowed and can be used to generate completely forward or completely reverse windows. A row containing a null in
     the timestamp column belongs to no window and will not be considered in the windows of other rows; its output will
     be null.
      
     Here are some examples of window values:
         rev_time = 0, fwd_time = 0 - contains rows that exactly match the current row timestamp
-        rev_time = "00:10:00", fwd_time = "0" - contains rows from 10m before through the current row timestamp (
+        rev_time = "PT00:10:00", fwd_time = "0" - contains rows from 10m before through the current row timestamp (
             inclusive)
         rev_time = 0, fwd_time = 600_000_000_000 - contains rows from the current row through 10m following the
             current row timestamp (inclusive)
-        rev_time = "00:10:00", fwd_time = "00:10:00" - contains rows from 10m before through 10m following
+        rev_time = "PT00:10:00", fwd_time = "PT00:10:00" - contains rows from 10m before through 10m following
             the current row timestamp (inclusive)
-        rev_time = "00:10:00", fwd_time = "-00:05:00" - contains rows from 10m before through 5m before the
+        rev_time = "PT00:10:00", fwd_time = "-PT00:05:00" - contains rows from 10m before through 5m before the
             current row timestamp (inclusive), this is a purely backwards looking window
-        rev_time = "-00:05:00", fwd_time = "00:10:00"} - contains rows from 5m following through 10m
+        rev_time = "-PT00:05:00", fwd_time = "PT00:10:00"} - contains rows from 5m following through 10m
             following the current row timestamp (inclusive), this is a purely forwards looking window
     
     Args:
         ts_col (str): the timestamp column for determining the window
         cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
             i.e. "new_col = col"; when empty, update_by perform the rolling product operation on all columns.
         rev_time (int): the look-behind window size, can be expressed as an integer in nanoseconds or a time
-            interval string, e.g. "00:00:00.001"
+            interval string, e.g. "PT00:00:00.001"
         fwd_time (int): the look-ahead window size, can be expressed as an integer in nanoseconds or a time
-            interval string, e.g. "00:00:00.001", default is 0
+            interval string, e.g. "PT00:00:00.001", default is 0
 
     Returns:
         an UpdateByOperation
 
     Raises:
         DHError
     """
     try:
         cols = to_sequence(cols)
-        rev_time = _JDateTimeUtils.expressionToNanos(rev_time) if isinstance(rev_time, str) else rev_time
-        fwd_time = _JDateTimeUtils.expressionToNanos(fwd_time) if isinstance(fwd_time, str) else fwd_time
+        rev_time = _JDateTimeUtils.parseDurationNanos(rev_time) if isinstance(rev_time, str) else rev_time
+        fwd_time = _JDateTimeUtils.parseDurationNanos(fwd_time) if isinstance(fwd_time, str) else fwd_time
         return UpdateByOperation(j_updateby_op=_JUpdateByOperation.RollingProduct(ts_col, rev_time, fwd_time, *cols))
     except Exception as e:
         raise DHError(e, "failed to create a rolling product (time) UpdateByOperation.") from e
 
 
 def rolling_count_tick(cols: Union[str, List[str]], rev_ticks: int, fwd_ticks: int = 0) -> UpdateByOperation:
     """Creates a rolling count UpdateByOperation for the supplied column names, using ticks as the windowing unit. Ticks
@@ -1058,44 +1130,44 @@
     function accepts nanoseconds or time strings as the reverse and forward window parameters. Negative values are
     allowed and can be used to generate completely forward or completely reverse windows. A row containing a null in
     the timestamp column belongs to no window and will not be considered in the windows of other rows; its output will
     be null.
 
     Here are some examples of window values:
         rev_time = 0, fwd_time = 0 - contains rows that exactly match the current row timestamp
-        rev_time = "00:10:00", fwd_time = "0" - contains rows from 10m before through the current row timestamp (
+        rev_time = "PT00:10:00", fwd_time = "0" - contains rows from 10m before through the current row timestamp (
             inclusive)
         rev_time = 0, fwd_time = 600_000_000_000 - contains rows from the current row through 10m following the
             current row timestamp (inclusive)
-        rev_time = "00:10:00", fwd_time = "00:10:00" - contains rows from 10m before through 10m following
+        rev_time = "PT00:10:00", fwd_time = "PT00:10:00" - contains rows from 10m before through 10m following
             the current row timestamp (inclusive)
-        rev_time = "00:10:00", fwd_time = "-00:05:00" - contains rows from 10m before through 5m before the
+        rev_time = "PT00:10:00", fwd_time = "-PT00:05:00" - contains rows from 10m before through 5m before the
             current row timestamp (inclusive), this is a purely backwards looking window
-        rev_time = "-00:05:00", fwd_time = "00:10:00"} - contains rows from 5m following through 10m
+        rev_time = "-PT00:05:00", fwd_time = "PT00:10:00"} - contains rows from 5m following through 10m
             following the current row timestamp (inclusive), this is a purely forwards looking window
 
     Args:
         ts_col (str): the timestamp column for determining the window
         cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
             i.e. "new_col = col"; when empty, update_by perform the rolling count operation on all columns.
         rev_time (int): the look-behind window size, can be expressed as an integer in nanoseconds or a time
-            interval string, e.g. "00:00:00.001"
+            interval string, e.g. "PT00:00:00.001"
         fwd_time (int): the look-ahead window size, can be expressed as an integer in nanoseconds or a time
-            interval string, e.g. "00:00:00.001", default is 0
+            interval string, e.g. "PT00:00:00.001", default is 0
 
     Returns:
         an UpdateByOperation
 
     Raises:
         DHError
     """
     try:
         cols = to_sequence(cols)
-        rev_time = _JDateTimeUtils.expressionToNanos(rev_time) if isinstance(rev_time, str) else rev_time
-        fwd_time = _JDateTimeUtils.expressionToNanos(fwd_time) if isinstance(fwd_time, str) else fwd_time
+        rev_time = _JDateTimeUtils.parseDurationNanos(rev_time) if isinstance(rev_time, str) else rev_time
+        fwd_time = _JDateTimeUtils.parseDurationNanos(fwd_time) if isinstance(fwd_time, str) else fwd_time
         return UpdateByOperation(j_updateby_op=_JUpdateByOperation.RollingCount(ts_col, rev_time, fwd_time, *cols))
     except Exception as e:
         raise DHError(e, "failed to create a rolling count (time) UpdateByOperation.") from e
 
 
 def rolling_std_tick(cols: Union[str, List[str]], rev_ticks: int, fwd_ticks: int = 0) -> UpdateByOperation:
     """Creates a rolling standard deviation UpdateByOperation for the supplied column names, using ticks as the windowing unit. Ticks
@@ -1140,44 +1212,44 @@
     function accepts nanoseconds or time strings as the reverse and forward window parameters. Negative values are
     allowed and can be used to generate completely forward or completely reverse windows. A row containing a null in
     the timestamp column belongs to no window and will not be considered in the windows of other rows; its output will
     be null.
 
     Here are some examples of window values:
         rev_time = 0, fwd_time = 0 - contains rows that exactly match the current row timestamp
-        rev_time = "00:10:00", fwd_time = "0" - contains rows from 10m before through the current row timestamp (
+        rev_time = "PT00:10:00", fwd_time = "0" - contains rows from 10m before through the current row timestamp (
             inclusive)
         rev_time = 0, fwd_time = 600_000_000_000 - contains rows from the current row through 10m following the
             current row timestamp (inclusive)
-        rev_time = "00:10:00", fwd_time = "00:10:00" - contains rows from 10m before through 10m following
+        rev_time = "PT00:10:00", fwd_time = "PT00:10:00" - contains rows from 10m before through 10m following
             the current row timestamp (inclusive)
-        rev_time = "00:10:00", fwd_time = "-00:05:00" - contains rows from 10m before through 5m before the
+        rev_time = "PT00:10:00", fwd_time = "-PT00:05:00" - contains rows from 10m before through 5m before the
             current row timestamp (inclusive), this is a purely backwards looking window
-        rev_time = "-00:05:00", fwd_time = "00:10:00"} - contains rows from 5m following through 10m
+        rev_time = "-PT00:05:00", fwd_time = "PT00:10:00"} - contains rows from 5m following through 10m
             following the current row timestamp (inclusive), this is a purely forwards looking window
 
     Args:
         ts_col (str): the timestamp column for determining the window
         cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
             i.e. "new_col = col"; when empty, update_by perform the rolling standard deviation operation on all columns.
         rev_time (int): the look-behind window size, can be expressed as an integer in nanoseconds or a time
-            interval string, e.g. "00:00:00.001"
+            interval string, e.g. "PT00:00:00.001"
         fwd_time (int): the look-ahead window size, can be expressed as an integer in nanoseconds or a time
-            interval string, e.g. "00:00:00.001", default is 0
+            interval string, e.g. "PT00:00:00.001", default is 0
 
     Returns:
         an UpdateByOperation
 
     Raises:
         DHError
     """
     try:
         cols = to_sequence(cols)
-        rev_time = _JDateTimeUtils.expressionToNanos(rev_time) if isinstance(rev_time, str) else rev_time
-        fwd_time = _JDateTimeUtils.expressionToNanos(fwd_time) if isinstance(fwd_time, str) else fwd_time
+        rev_time = _JDateTimeUtils.parseDurationNanos(rev_time) if isinstance(rev_time, str) else rev_time
+        fwd_time = _JDateTimeUtils.parseDurationNanos(fwd_time) if isinstance(fwd_time, str) else fwd_time
         return UpdateByOperation(j_updateby_op=_JUpdateByOperation.RollingStd(ts_col, rev_time, fwd_time, *cols))
     except Exception as e:
         raise DHError(e, "failed to create a rolling standard deviation (time) UpdateByOperation.") from e
 
 
 def rolling_wavg_tick(weight_col: str, cols: Union[str, List[str]], rev_ticks: int, fwd_ticks: int = 0) -> UpdateByOperation:
     """Creates a rolling weighted average UpdateByOperation for the supplied column names, using ticks as the windowing unit. Ticks
@@ -1225,41 +1297,41 @@
     function accepts nanoseconds or time strings as the reverse and forward window parameters. Negative values are
     allowed and can be used to generate completely forward or completely reverse windows. A row containing a null in
     the timestamp column belongs to no window and will not be considered in the windows of other rows; its output will
     be null.
 
     Here are some examples of window values:
         rev_time = 0, fwd_time = 0 - contains rows that exactly match the current row timestamp
-        rev_time = "00:10:00", fwd_time = "0" - contains rows from 10m before through the current row timestamp (
+        rev_time = "PT00:10:00", fwd_time = "0" - contains rows from 10m before through the current row timestamp (
             inclusive)
         rev_time = 0, fwd_time = 600_000_000_000 - contains rows from the current row through 10m following the
             current row timestamp (inclusive)
-        rev_time = "00:10:00", fwd_time = "00:10:00" - contains rows from 10m before through 10m following
+        rev_time = "PT00:10:00", fwd_time = "PT00:10:00" - contains rows from 10m before through 10m following
             the current row timestamp (inclusive)
-        rev_time = "00:10:00", fwd_time = "-00:05:00" - contains rows from 10m before through 5m before the
+        rev_time = "PT00:10:00", fwd_time = "-PT00:05:00" - contains rows from 10m before through 5m before the
             current row timestamp (inclusive), this is a purely backwards looking window
-        rev_time = "-00:05:00", fwd_time = "00:10:00"} - contains rows from 5m following through 10m
+        rev_time = "-PT00:05:00", fwd_time = "PT00:10:00"} - contains rows from 5m following through 10m
             following the current row timestamp (inclusive), this is a purely forwards looking window
 
     Args:
         ts_col (str): the timestamp column for determining the window
         cols (Union[str, List[str]]): the column(s) to be operated on, can include expressions to rename the output,
             i.e. "new_col = col"; when empty, update_by perform the rolling weighted average operation on all columns.
         weight_col (str):  the column containing the weight values
         rev_time (int): the look-behind window size, can be expressed as an integer in nanoseconds or a time
-            interval string, e.g. "00:00:00.001"
+            interval string, e.g. "PT00:00:00.001"
         fwd_time (int): the look-ahead window size, can be expressed as an integer in nanoseconds or a time
-            interval string, e.g. "00:00:00.001", default is 0
+            interval string, e.g. "PT00:00:00.001", default is 0
 
     Returns:
         an UpdateByOperation
 
     Raises:
         DHError
     """
     try:
         cols = to_sequence(cols)
-        rev_time = _JDateTimeUtils.expressionToNanos(rev_time) if isinstance(rev_time, str) else rev_time
-        fwd_time = _JDateTimeUtils.expressionToNanos(fwd_time) if isinstance(fwd_time, str) else fwd_time
+        rev_time = _JDateTimeUtils.parseDurationNanos(rev_time) if isinstance(rev_time, str) else rev_time
+        fwd_time = _JDateTimeUtils.parseDurationNanos(fwd_time) if isinstance(fwd_time, str) else fwd_time
         return UpdateByOperation(j_updateby_op=_JUpdateByOperation.RollingWAvg(ts_col, rev_time, fwd_time, weight_col, *cols))
     except Exception as e:
         raise DHError(e, "failed to create a rolling weighted average (time) UpdateByOperation.") from e
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## deephaven/config/__init__.py

```diff
@@ -1,24 +1,15 @@
 #
 # Copyright (c) 2016-2022 Deephaven Data Labs and Patent Pending
 #
 
 """ This module provides access to the Deephaven server configuration. """
 import jpy
 
-from deephaven import DHError
-from deephaven.time import TimeZone
+from deephaven.dtypes import TimeZone
 
-_JDHConfig = jpy.get_type("io.deephaven.configuration.Configuration")
-_JDateTimeZone = jpy.get_type("org.joda.time.DateTimeZone")
+_JDateTimeUtils = jpy.get_type("io.deephaven.time.DateTimeUtils")
 
 
 def get_server_timezone() -> TimeZone:
     """ Returns the server's time zone. """
-    try:
-        j_timezone = _JDateTimeZone.forTimeZone(_JDHConfig.getInstance().getServerTimezone())
-        for tz in TimeZone:
-            if j_timezone == tz.value.getTimeZone():
-                return tz
-        raise NotImplementedError("can't find the time zone in the TimeZone Enum.")
-    except Exception as e:
-        raise DHError(e, message=f"failed to find a recognized time zone") from e
+    return _JDateTimeUtils.timeZone()
```

## deephaven/experimental/outer_joins.py

```diff
@@ -9,15 +9,15 @@
 from typing import Union, Sequence
 
 from deephaven import DHError
 from deephaven.jcompat import to_sequence
 from deephaven.table import Table
 import jpy
 
-from deephaven.ugp import auto_locking_ctx
+from deephaven.update_graph import auto_locking_ctx
 
 _JOuterJoinTools = jpy.get_type("io.deephaven.engine.util.OuterJoinTools")
 
 
 def full_outer_join(l_table: Table, r_table: Table, on: Union[str, Sequence[str]] = None,
                     joins: Union[str, Sequence[str]] = None) -> Table:
     """The full_outer_join function creates a new table containing rows that have matching values in both tables.
```

## deephaven/plot/axisformat.py

```diff
@@ -58,8 +58,8 @@
 
         Args:
              tz (TimeZone): the timezone to use for formatting, default is None meaning to use the default time zone.
         """
         if not tz:
             self.j_axis_format = _JNanosAxisFormat()
         else:
-            self.j_axis_format = _JNanosAxisFormat(tz.value)
+            self.j_axis_format = _JNanosAxisFormat(tz)
```

## deephaven/plot/figure.py

```diff
@@ -15,15 +15,15 @@
 from typing import Any, Dict, Union, Sequence, List, Callable, _GenericAlias
 
 import numpy
 import jpy
 
 from deephaven import DHError, dtypes
 from deephaven._wrapper import JObjectWrapper
-from deephaven.dtypes import DateTime, PyObject
+from deephaven.dtypes import Instant, PyObject
 from deephaven.plot import LineStyle, PlotStyle, Color, Font, AxisFormat, Shape, AxisTransform, \
     SelectableDataSet
 from deephaven.table import Table
 from deephaven.calendar import BusinessCalendar
 from deephaven.jcompat import j_function
 
 _JPlottingConvenience = jpy.get_type("io.deephaven.plot.PlottingConvenience")
@@ -1004,28 +1004,28 @@
             raise DHError(f"unsupported parameter combination: {non_null_args}")
 
     def plot_cat(
         self,
         series_name: str,
         t: Union[Table, SelectableDataSet] = None,
         category: Union[str, List[str], List[int], List[float]] = None,
-        y: Union[str, List[int], List[float], List[DateTime]] = None,
-        y_low: Union[str, List[int], List[float], List[DateTime]] = None,
-        y_high: Union[str, List[int], List[float], List[DateTime]] = None,
+        y: Union[str, List[int], List[float], List[Instant]] = None,
+        y_low: Union[str, List[int], List[float], List[Instant]] = None,
+        y_high: Union[str, List[int], List[float], List[Instant]] = None,
         by: List[str] = None,
     ) -> Figure:
         """Creates a plot with a discrete, categorical axis. Categorical data must not have duplicates.
 
         Args:
             series_name (str): name of the data series
             t (Union[Table, SelectableDataSet]): table or selectable data set (e.g. OneClick filterable table)
             category (Union[str, List[str], List[int], List[float]]): discrete data or column name
-            y (Union[str, List[int], List[float], List[DateTime]]): y-values or column name
-            y_low (Union[str, List[int], List[float], List[DateTime]]): lower y error bar
-            y_high (Union[str, List[int], List[float], List[DateTime]]): upper y error bar
+            y (Union[str, List[int], List[float], List[Instant]]): y-values or column name
+            y_low (Union[str, List[int], List[float], List[Instant]]): lower y error bar
+            y_high (Union[str, List[int], List[float], List[Instant]]): upper y error bar
             by (List[str]): columns that hold grouping data
 
         Returns:
             a new Figure
 
         Raises:
             DHError
@@ -1041,21 +1041,21 @@
             non_null_args.add("t")
             t = _convert_j("t", t, [Table, SelectableDataSet])
         if category is not None:
             non_null_args.add("category")
             category = _convert_j("category", category, [str, List[str], List[int], List[float]])
         if y is not None:
             non_null_args.add("y")
-            y = _convert_j("y", y, [str, List[int], List[float], List[DateTime]])
+            y = _convert_j("y", y, [str, List[int], List[float], List[Instant]])
         if y_low is not None:
             non_null_args.add("y_low")
-            y_low = _convert_j("y_low", y_low, [str, List[int], List[float], List[DateTime]])
+            y_low = _convert_j("y_low", y_low, [str, List[int], List[float], List[Instant]])
         if y_high is not None:
             non_null_args.add("y_high")
-            y_high = _convert_j("y_high", y_high, [str, List[int], List[float], List[DateTime]])
+            y_high = _convert_j("y_high", y_high, [str, List[int], List[float], List[Instant]])
         if by is not None:
             non_null_args.add("by")
             by = _no_convert_j("by", by, [List[str]])
 
         if non_null_args == {"series_name", "category", "y"}:
             return Figure(j_figure=self.j_figure.catPlot(series_name, category, y))
         elif non_null_args == {"series_name", "t", "category", "y"}:
@@ -1111,31 +1111,31 @@
         else:
             raise DHError(f"unsupported parameter combination: {non_null_args}")
 
     def plot_ohlc(
         self,
         series_name: str,
         t: Union[Table, SelectableDataSet] = None,
-        x: Union[str, List[DateTime]] = None,
-        open: Union[str, List[int], List[float], List[DateTime]] = None,
-        high: Union[str, List[int], List[float], List[DateTime]] = None,
-        low: Union[str, List[int], List[float], List[DateTime]] = None,
-        close: Union[str, List[int], List[float], List[DateTime]] = None,
+        x: Union[str, List[Instant]] = None,
+        open: Union[str, List[int], List[float], List[Instant]] = None,
+        high: Union[str, List[int], List[float], List[Instant]] = None,
+        low: Union[str, List[int], List[float], List[Instant]] = None,
+        close: Union[str, List[int], List[float], List[Instant]] = None,
         by: List[str] = None,
     ) -> Figure:
         """Creates an open-high-low-close plot.
 
         Args:
             series_name (str): name of the data series
             t (Union[Table, SelectableDataSet]): table or selectable data set (e.g. OneClick filterable table)
-            x (Union[str, List[DateTime]]): x-values or column name
-            open (Union[str, List[int], List[float], List[DateTime]]): bar open y-values.
-            high (Union[str, List[int], List[float], List[DateTime]]): bar high y-values.
-            low (Union[str, List[int], List[float], List[DateTime]]): bar low y-values.
-            close (Union[str, List[int], List[float], List[DateTime]]): bar close y-values.
+            x (Union[str, List[Instant]]): x-values or column name
+            open (Union[str, List[int], List[float], List[Instant]]): bar open y-values.
+            high (Union[str, List[int], List[float], List[Instant]]): bar high y-values.
+            low (Union[str, List[int], List[float], List[Instant]]): bar low y-values.
+            close (Union[str, List[int], List[float], List[Instant]]): bar close y-values.
             by (List[str]): columns that hold grouping data
 
         Returns:
             a new Figure
 
         Raises:
             DHError
@@ -1148,27 +1148,27 @@
             non_null_args.add("series_name")
             series_name = _convert_j("series_name", series_name, [str])
         if t is not None:
             non_null_args.add("t")
             t = _convert_j("t", t, [Table, SelectableDataSet])
         if x is not None:
             non_null_args.add("x")
-            x = _convert_j("x", x, [str, List[DateTime]])
+            x = _convert_j("x", x, [str, List[Instant]])
         if open is not None:
             non_null_args.add("open")
-            open = _convert_j("open", open, [str, List[int], List[float], List[DateTime]])
+            open = _convert_j("open", open, [str, List[int], List[float], List[Instant]])
         if high is not None:
             non_null_args.add("high")
-            high = _convert_j("high", high, [str, List[int], List[float], List[DateTime]])
+            high = _convert_j("high", high, [str, List[int], List[float], List[Instant]])
         if low is not None:
             non_null_args.add("low")
-            low = _convert_j("low", low, [str, List[int], List[float], List[DateTime]])
+            low = _convert_j("low", low, [str, List[int], List[float], List[Instant]])
         if close is not None:
             non_null_args.add("close")
-            close = _convert_j("close", close, [str, List[int], List[float], List[DateTime]])
+            close = _convert_j("close", close, [str, List[int], List[float], List[Instant]])
         if by is not None:
             non_null_args.add("by")
             by = _no_convert_j("by", by, [List[str]])
 
         if non_null_args == {"series_name", "x", "open", "high", "low", "close"}:
             return Figure(j_figure=self.j_figure.ohlcPlot(series_name, x, open, high, low, close))
         elif non_null_args == {"series_name", "t", "x", "open", "high", "low", "close"}:
@@ -1179,23 +1179,23 @@
             raise DHError(f"unsupported parameter combination: {non_null_args}")
 
     def plot_pie(
         self,
         series_name: str,
         t: Union[Table, SelectableDataSet] = None,
         category: Union[str, List[str], List[int], List[float]] = None,
-        y: Union[str, List[int], List[float], List[DateTime]] = None,
+        y: Union[str, List[int], List[float], List[Instant]] = None,
     ) -> Figure:
         """Creates a pie plot. Categorical data must not have duplicates.
 
         Args:
             series_name (str): name of the data series
             t (Union[Table, SelectableDataSet]): table or selectable data set (e.g. OneClick filterable table)
             category (Union[str, List[str], List[int], List[float]]): discrete data or column name
-            y (Union[str, List[int], List[float], List[DateTime]]): y-values or column name
+            y (Union[str, List[int], List[float], List[Instant]]): y-values or column name
 
         Returns:
             a new Figure
 
         Raises:
             DHError
         """
@@ -1210,15 +1210,15 @@
             non_null_args.add("t")
             t = _convert_j("t", t, [Table, SelectableDataSet])
         if category is not None:
             non_null_args.add("category")
             category = _convert_j("category", category, [str, List[str], List[int], List[float]])
         if y is not None:
             non_null_args.add("y")
-            y = _convert_j("y", y, [str, List[int], List[float], List[DateTime]])
+            y = _convert_j("y", y, [str, List[int], List[float], List[Instant]])
 
         if non_null_args == {"series_name", "category", "y"}:
             return Figure(j_figure=self.j_figure.piePlot(series_name, category, y))
         elif non_null_args == {"series_name", "t", "category", "y"}:
             return Figure(j_figure=self.j_figure.piePlot(series_name, t, category, y))
         else:
             raise DHError(f"unsupported parameter combination: {non_null_args}")
@@ -1292,36 +1292,36 @@
         else:
             raise DHError(f"unsupported parameter combination: {non_null_args}")
 
     def plot_xy(
         self,
         series_name: str,
         t: Union[Table, SelectableDataSet] = None,
-        x: Union[str, List[int], List[float], List[DateTime]] = None,
-        x_low: Union[str, List[int], List[float], List[DateTime]] = None,
-        x_high: Union[str, List[int], List[float], List[DateTime]] = None,
-        y: Union[str, List[int], List[float], List[DateTime]] = None,
-        y_low: Union[str, List[int], List[float], List[DateTime]] = None,
-        y_high: Union[str, List[int], List[float], List[DateTime]] = None,
+        x: Union[str, List[int], List[float], List[Instant]] = None,
+        x_low: Union[str, List[int], List[float], List[Instant]] = None,
+        x_high: Union[str, List[int], List[float], List[Instant]] = None,
+        y: Union[str, List[int], List[float], List[Instant]] = None,
+        y_low: Union[str, List[int], List[float], List[Instant]] = None,
+        y_high: Union[str, List[int], List[float], List[Instant]] = None,
         function: Callable = None,
         by: List[str] = None,
         x_time_axis: bool = None,
         y_time_axis: bool = None,
     ) -> Figure:
         """Creates an XY plot.
 
         Args:
             series_name (str): name of the data series
             t (Union[Table, SelectableDataSet]): table or selectable data set (e.g. OneClick filterable table)
-            x (Union[str, List[int], List[float], List[DateTime]]): x-values or column name
-            x_low (Union[str, List[int], List[float], List[DateTime]]): lower x error bar
-            x_high (Union[str, List[int], List[float], List[DateTime]]): upper x error bar
-            y (Union[str, List[int], List[float], List[DateTime]]): y-values or column name
-            y_low (Union[str, List[int], List[float], List[DateTime]]): lower y error bar
-            y_high (Union[str, List[int], List[float], List[DateTime]]): upper y error bar
+            x (Union[str, List[int], List[float], List[Instant]]): x-values or column name
+            x_low (Union[str, List[int], List[float], List[Instant]]): lower x error bar
+            x_high (Union[str, List[int], List[float], List[Instant]]): upper x error bar
+            y (Union[str, List[int], List[float], List[Instant]]): y-values or column name
+            y_low (Union[str, List[int], List[float], List[Instant]]): lower y error bar
+            y_high (Union[str, List[int], List[float], List[Instant]]): upper y error bar
             function (Callable): function
             by (List[str]): columns that hold grouping data
             x_time_axis (bool): whether to treat the x-values as times
             y_time_axis (bool): whether to treat the y-values as times
 
         Returns:
             a new Figure
@@ -1337,30 +1337,30 @@
             non_null_args.add("series_name")
             series_name = _convert_j("series_name", series_name, [str])
         if t is not None:
             non_null_args.add("t")
             t = _convert_j("t", t, [Table, SelectableDataSet])
         if x is not None:
             non_null_args.add("x")
-            x = _convert_j("x", x, [str, List[int], List[float], List[DateTime]])
+            x = _convert_j("x", x, [str, List[int], List[float], List[Instant]])
         if x_low is not None:
             non_null_args.add("x_low")
-            x_low = _convert_j("x_low", x_low, [str, List[int], List[float], List[DateTime]])
+            x_low = _convert_j("x_low", x_low, [str, List[int], List[float], List[Instant]])
         if x_high is not None:
             non_null_args.add("x_high")
-            x_high = _convert_j("x_high", x_high, [str, List[int], List[float], List[DateTime]])
+            x_high = _convert_j("x_high", x_high, [str, List[int], List[float], List[Instant]])
         if y is not None:
             non_null_args.add("y")
-            y = _convert_j("y", y, [str, List[int], List[float], List[DateTime]])
+            y = _convert_j("y", y, [str, List[int], List[float], List[Instant]])
         if y_low is not None:
             non_null_args.add("y_low")
-            y_low = _convert_j("y_low", y_low, [str, List[int], List[float], List[DateTime]])
+            y_low = _convert_j("y_low", y_low, [str, List[int], List[float], List[Instant]])
         if y_high is not None:
             non_null_args.add("y_high")
-            y_high = _convert_j("y_high", y_high, [str, List[int], List[float], List[DateTime]])
+            y_high = _convert_j("y_high", y_high, [str, List[int], List[float], List[Instant]])
         if function is not None:
             non_null_args.add("function")
             function = _convert_j("function", function, [Callable])
         if by is not None:
             non_null_args.add("by")
             by = _no_convert_j("by", by, [List[str]])
         if x_time_axis is not None:
@@ -1401,25 +1401,25 @@
         else:
             raise DHError(f"unsupported parameter combination: {non_null_args}")
 
     def plot_xy_hist(
         self,
         series_name: str,
         t: Union[Table, SelectableDataSet] = None,
-        x: Union[str, List[int], List[float], List[DateTime]] = None,
+        x: Union[str, List[int], List[float], List[Instant]] = None,
         xmin: float = None,
         xmax: float = None,
         nbins: int = None,
     ) -> Figure:
         """Creates an XY histogram.
 
         Args:
             series_name (str): name of the data series
             t (Union[Table, SelectableDataSet]): table or selectable data set (e.g. OneClick filterable table)
-            x (Union[str, List[int], List[float], List[DateTime]]): x-values or column name
+            x (Union[str, List[int], List[float], List[Instant]]): x-values or column name
             xmin (float): minimum x value to display
             xmax (float): maximum x value to display
             nbins (int): number of bins
 
         Returns:
             a new Figure
 
@@ -1434,15 +1434,15 @@
             non_null_args.add("series_name")
             series_name = _convert_j("series_name", series_name, [str])
         if t is not None:
             non_null_args.add("t")
             t = _convert_j("t", t, [Table, SelectableDataSet])
         if x is not None:
             non_null_args.add("x")
-            x = _convert_j("x", x, [str, List[int], List[float], List[DateTime]])
+            x = _convert_j("x", x, [str, List[int], List[float], List[Instant]])
         if xmin is not None:
             non_null_args.add("xmin")
             xmin = _convert_j("xmin", xmin, [float])
         if xmax is not None:
             non_null_args.add("xmax")
             xmax = _convert_j("xmax", xmax, [float])
         if nbins is not None:
```

## Comparing `deephaven_core-0.24.3.dist-info/METADATA` & `deephaven_core-0.25.0.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: deephaven-core
-Version: 0.24.3
+Version: 0.25.0
 Summary: Deephaven Engine Python Package
 Home-page: https://deephaven.io/
 Author: Deephaven Data Labs
 Author-email: python@deephaven.io
 License: Deephaven Community License
 Keywords: Deephaven Development
 Platform: UNKNOWN
```

## Comparing `deephaven_core-0.24.3.dist-info/RECORD` & `deephaven_core-0.25.0.dist-info/RECORD`

 * *Files 8% similar despite different names*

```diff
@@ -1,54 +1,55 @@
-deephaven/__init__.py,sha256=k_WXfAH5HuJAyHReBk0yYHbQk_9dI-BwfDpPRD0XNuA,1120
+deephaven/__init__.py,sha256=On2P3slwYuf8B4nyk2xJwdkdn5VsU1kvaL0KfM9higQ,1120
 deephaven/_gc.py,sha256=Ej3zzkhgOEdT6lqj7LIzDH8eLgybW68716zpHjeNSgQ,970
 deephaven/_jpy.py,sha256=jczrjWcy2bMRCGpWS7UUFO2AWAG5cURUr4oTxq23EGw,676
 deephaven/_wrapper.py,sha256=R2TFX5myaVvl00Kj8LthRd9frMNnUO4TpK99vCpH-8M,4551
-deephaven/agg.py,sha256=44hXG8f5EvEXK4Wxpxme8f83UfC2gAlQHqKLJU625Fg,10813
+deephaven/agg.py,sha256=8NuUXkeX0yI-MrQnhMncxaJn4KFRsaTkqZDkaXxdhdk,13848
 deephaven/appmode.py,sha256=6r8L679hn7zRFfXCDXuYPUpgsLPMLqg8Zp0xzPy0QU4,1987
-deephaven/arrow.py,sha256=G2FKr5ua9jt6XMqe97Ri9oCp-bhJdzqEbDwvrY7GrHU,4776
-deephaven/calendar.py,sha256=grvrTgPrTAYJz8DjxbgrGc5JHGHrHRkQtpPo8YEa-hk,14670
-deephaven/column.py,sha256=TylYMvChr7xYsBfhtwfwTWeglvu99fybGHamEuuYIDI,7276
+deephaven/arrow.py,sha256=HkXWKe4U0BP2RirWcgSrfa8Ly7CZHueUn7ad-oNJd2M,4758
+deephaven/calendar.py,sha256=V20YmgqJTmZvHBoOnPK1Qo7HhCSo8SqS-e6bjCLsqsY,14651
+deephaven/column.py,sha256=WG375e5XUA3j6wTeU5UMufVC7zrAzDMv2s1es8unyBo,7275
 deephaven/constants.py,sha256=rhvNKqZoG0iwmjVdY0GPvycj1-c1pcSiXT0umzPahVo,3123
-deephaven/csv.py,sha256=5OQDIjc0_1DDvugaJHBN_T40PcmHh6Ik8tMOJHL1AOk,4515
+deephaven/csv.py,sha256=O0CF5duVionLF7VqxpCcCpF-aDDA4NrHB1Pfauv87bw,4937
 deephaven/dherror.py,sha256=QTHto3BTLGzThFsB2yaUvLrFG9b9slwM3DAdimmQm6A,2890
-deephaven/dtypes.py,sha256=bvCW85w4uN-YYfc5WlNUEDj5LK7lnnHjDFEK5G0tPRg,9105
-deephaven/execution_context.py,sha256=JvcJxKd3Zid-6uBEIUr9yVJY8tIT2H7v76vVrkz49fY,3517
+deephaven/dtypes.py,sha256=gIUVTS9DA0W5SzGhGq4YJlo4vtqTP5MCfrixpy53YkU,9696
+deephaven/execution_context.py,sha256=GqXuwrjGTAi66KuP6a3bnu81y8oR4q0h0obnT07XqzY,3749
 deephaven/filters.py,sha256=vjVN0iGlbOoY94kaXnftQeth2V9wNrAWEDwJRSvlW2U,5157
 deephaven/html.py,sha256=zJM2Qa7mwa-bmsRq91mJgdQ63nZ8zYbT6TgBmJPZx7g,650
-deephaven/jcompat.py,sha256=uriYEPqPhrhAO4V_L6qL85tCqdxO3i3B_YHD9oqydmI,4696
+deephaven/jcompat.py,sha256=EYkGrhWXiIC20RWX0BDpcUDAaM31_7pqU91iNfSPMLc,4708
 deephaven/liveness_scope.py,sha256=LOTkbIjoMtfdPBNpZkEn-1pQS0MTeRZAwD6DDKDjXhc,2503
-deephaven/numpy.py,sha256=f_ouyNUO-x7-IN4TIgcT_cejRT6URVrHwV8CdZ9IrRA,6109
-deephaven/pandas.py,sha256=n3ll2RlfCuFfG45bf42u0vPgoXe-9K13Mlt6ic1x4Ro,8745
+deephaven/numpy.py,sha256=NB71mv6v7WU5TMNPCQ90YT11M7IfbHNGVDuG8_FRwJo,6214
+deephaven/pandas.py,sha256=Xi_38mLhfrmo6sY15GSD-aTXUxH7hVmjIQEj9jN38_o,9027
 deephaven/parquet.py,sha256=2swJSd98amMBqK8G1xEuMYSLli_sH2mYssx4TPuXwGw,9173
 deephaven/perfmon.py,sha256=8O98Rk0NviHG2XeInuYDxJTB2YKe7tRCif-GCoX1sjQ,8081
 deephaven/query_library.py,sha256=8z_SsylV4ZbANjiWCVqDgEOnjXHyKj9xlHfz3FMPT5Q,2690
-deephaven/replay.py,sha256=hVK8J79hnf57-7bKhf5vT85y71fKXWpc_QCa9W8Shyc,2471
-deephaven/table.py,sha256=aeG46YGLXteqteprHtajObfggOaxcAaYSHOz_aHy8oY,154120
-deephaven/table_factory.py,sha256=ZGGO9EVO32tYqogc-LAAtIxrhzkmJ579NOFVgX_U2vU,11181
-deephaven/table_listener.py,sha256=66mXyKWwFKAHJ6ovEIDJUCk5tztzLyJMfnZlcuM3Km8,15976
-deephaven/time.py,sha256=HzafT8Kl5oKRdBFNtvxcYUxkO9LkfZ8Kp0-sCpzr3v4,21914
-deephaven/ugp.py,sha256=isoY2u4-whYCfj3nc7iumOPwAhJgDZSOINTeSd5RzB8,4712
-deephaven/updateby.py,sha256=hSxslNqgQsmXotl24xyB8htlH1dOaSSp_RrtIit6KRg,67886
+deephaven/replay.py,sha256=CZoANuVYynVBAgIAjJcfONL0ITNfloQvTpMAz-n9wf0,2711
+deephaven/table.py,sha256=qq5aj0LuwqQc_suf4e2K8jSFik12oNsg542lnULj4Kc,154823
+deephaven/table_factory.py,sha256=r0bAQWenKdLI5ihuqG1D7j5pIaN4x8KTeX_rOxHbOtc,11192
+deephaven/table_listener.py,sha256=yKU1s0cjSSUi-ZW7i5Jecf9S5uKw6UQZiWujJMOu3uo,16259
+deephaven/time.py,sha256=juy97G2YVBjTn-2d0FfyPJU05uRG4RHOTNfEYeTVbZ8,57543
+deephaven/update_graph.py,sha256=dQzVA9-8tNLS-4G2MArPUTjDoSl24bd3E6UcxmwEzBs,7982
+deephaven/updateby.py,sha256=cCza3tuszSYE9nWTgIgTMl3CuQV_O-Azu6dI-HxjNqM,71286
 deephaven/uri.py,sha256=lUD5LT_cIcKAkOVJd9bBmjVhxzB9OVJJowqIvBMPWDk,1155
-deephaven/config/__init__.py,sha256=IEgAvTwLORH5zKDkbDe0S-n6bdjt-T2wtUFUhQTvMFY,842
+deephaven/config/__init__.py,sha256=iY3ud0KoEyeRpFMGVLL9wyS3GMFEGmPFL4i1kDHGZtQ,385
 deephaven/dbc/__init__.py,sha256=-kINZXfFQ_rjPKtE5yXXmV6SoxhFEkaPwPhhBdhoFiI,3200
 deephaven/dbc/adbc.py,sha256=bnl0nS-fgpaS1ZV8OLYph7WaqRs8uFv9GiE861sW9TA,1794
 deephaven/dbc/odbc.py,sha256=Sm54IOlEucnZk8N7XlOizjrKIJ1NHxmMbEsrJwv_V8U,1668
 deephaven/experimental/__init__.py,sha256=dJEBTCuRgxLJx45AEBpju6YkE9ShLAiNPJkDiTtEDeY,1316
 deephaven/experimental/ema.py,sha256=Ncdf1C4tbrI_Z8o1f9UZ0Lw_GFsH2HZDxR8XA8Y3sfI,4815
-deephaven/experimental/outer_joins.py,sha256=firPRUSkCoir1k4oGuvv4RMT64uhIXvSBu9EFKsN9cw,4097
+deephaven/experimental/outer_joins.py,sha256=Na3MOL4wixf16WseYkEA6skGpDd2p6jWnbwCBB4FSRs,4106
+deephaven/experimental/sql.py,sha256=YvtsYvAorCJIr3NThuCzOPVAMLhogW41RfDLBDEoGss,1570
 deephaven/learn/__init__.py,sha256=sSSrxxj6QmBCNavIwsIVIoCCThNpS5LSZpJRd1DXnCU,7350
 deephaven/learn/gather.py,sha256=jqCHN1yITCV3ximqXJDbQ-GhDxfOpCVNgDrse6vpIK8,3120
 deephaven/pandasplugin/__init__.py,sha256=PiwF6ImhyQMzzWPoP4An6ae8jK--U4StBRWu50uUCaI,347
 deephaven/pandasplugin/pandas_as_table.py,sha256=O-L93Iz-6fjoS5aQzQ4mE3hBFYrUldk0kfl9dNyBhGM,550
 deephaven/plot/__init__.py,sha256=G2pf1xnMUf1hDAd3cFXzRBqgNNvjoCmGUyQdqAiK9Bs,574
-deephaven/plot/axisformat.py,sha256=AxUWIq4NNDJaSyfYbp-JZJ56CYfEtlL1iEphrdvZPQ4,2285
+deephaven/plot/axisformat.py,sha256=HP8Wpdo8OpfJz14IE0QibaNBBpoI7Gtm8shA5AtOmdk,2279
 deephaven/plot/axistransform.py,sha256=8LaLfpqeQ5asjzaVY6ufhOv7GgkDorM96e2DRwoxmF0,1479
 deephaven/plot/color.py,sha256=pS_n-wi7yDW8P0o7ve4SNhRITTS6jGEAMr3U3glI6GA,16541
-deephaven/plot/figure.py,sha256=4jcIg7BGmXj263dJhcQIxPEYhaWIoXyfpxYxprkrhVA,110233
+deephaven/plot/figure.py,sha256=D7k2zFHHwk0dnt-99YOoXdE0fIyFRJY8_BwWcGwg-Kc,110184
 deephaven/plot/font.py,sha256=DE8itMivEpbZGZvlXZjWLp6qFnvkRe8cKtNSMsKA3BQ,1659
 deephaven/plot/linestyle.py,sha256=y34EQVbaRTZ-dMdhnhlLmjMQ1DKUH2mWprG0O8t2QhM,3514
 deephaven/plot/plotstyle.py,sha256=pFGqJ7jkg4pTEUfDbhCCtURJKkF94IPjYZzLnnm855E,1285
 deephaven/plot/selectable_dataset.py,sha256=EzErxDkqnavztdEghUfWXZageMicU4STF-OIsYXWBzk,2493
 deephaven/plot/shape.py,sha256=gBlrOAcRHDAF8975y1Jk2VsGRzzTB-HnbuZoVGgZoj0,1213
 deephaven/server/__init__.py,sha256=-990laf-1fgsNkYqDUCRk2h586vUDgHzd9rlKKsjHac,248
 deephaven/server/plugin/__init__.py,sha256=4nzWFJwnvyaBbQCobarAkmy2Vfhm9pnyzhTN6TexFvI,584
@@ -62,12 +63,12 @@
 deephaven/stream/kafka/producer.py,sha256=obCfOKBYzXNg0VViy-h7qXcN-xvKcobrTvYWBP_qFOw,10818
 deephaven_internal/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 deephaven_internal/java_threads.py,sha256=xuCtrDHJNzPkbKoHBTAj7j-lAWNlaQJqleQ-LdP4aT8,1576
 deephaven_internal/stream.py,sha256=OTQuxFnJmok29-28XPd4Nmt6RNk_RnD5ppQkNddlKZY,3426
 deephaven_internal/auto_completer/__init__.py,sha256=2zPuxVzSsw5ZGpOFrvU3UY59s7qC13vCNTOWZoNKqiA,952
 deephaven_internal/auto_completer/_completer.py,sha256=5gFt4Uu2dJQD_5VM8gE67DYhbKUzL6G2lI_piRh0jss,8476
 deephaven_internal/jvm/__init__.py,sha256=K0q4isvz9_4Ulq7hcH7dmxo5mLrdmeMIr99rKu5k1fs,2585
-deephaven_core-0.24.3.dist-info/METADATA,sha256=s44ZgqfVOXns5e41HTQEVv4c1l-3lZqeQpZsUFDajQo,2647
-deephaven_core-0.24.3.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-deephaven_core-0.24.3.dist-info/entry_points.txt,sha256=SdGFMIrwRwAlv0x_hKsuvQl176FayFeWUM64ujCbGnI,87
-deephaven_core-0.24.3.dist-info/top_level.txt,sha256=EshtqOS_YqliZnhyqyyQOCTJARwRdEW_XueIFQ9gJB4,29
-deephaven_core-0.24.3.dist-info/RECORD,,
+deephaven_core-0.25.0.dist-info/METADATA,sha256=uYGhFpFfeSpomB7jwPtg1w6VuzrSj8MVBZ0vf8D3OcI,2647
+deephaven_core-0.25.0.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+deephaven_core-0.25.0.dist-info/entry_points.txt,sha256=SdGFMIrwRwAlv0x_hKsuvQl176FayFeWUM64ujCbGnI,87
+deephaven_core-0.25.0.dist-info/top_level.txt,sha256=EshtqOS_YqliZnhyqyyQOCTJARwRdEW_XueIFQ9gJB4,29
+deephaven_core-0.25.0.dist-info/RECORD,,
```

